#!/usr/bin/env python3
"""
proceed/ â†’ BigQuery é€£æºã‚¹ã‚¯ãƒªãƒ—ãƒˆ
CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’BigQueryãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ­ãƒ¼ãƒ‰ï¼ˆæœˆæ¬¡APPENDãƒ¢ãƒ¼ãƒ‰ï¼‰
"""

import os
import sys
import time
from typing import List, Dict, Optional
from google.cloud import bigquery
from google.cloud import storage
from google.cloud.exceptions import GoogleCloudError

# å›ºå®šå€¤è¨­å®š
PROJECT_ID = "data-platform-prod-475201"
DATASET_ID = "corporate_data"
LANDING_BUCKET = "data-platform-landing-prod"

# ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©ã¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ—ã®ãƒãƒƒãƒ”ãƒ³ã‚°
TABLE_CONFIG = {
    "sales_target_and_achievements": {
        "partition_field": "sales_accounting_period",
        "clustering_fields": ["branch_code"]
    },
    "billing_balance": {
        "partition_field": "sales_month", 
        "clustering_fields": ["branch_code"]
    },
    "ledger_income": {
        "partition_field": "slip_date",  # DATE(slip_date)ã§ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³
        "clustering_fields": ["classification_type"]
    },
    "department_summary": {
        "partition_field": "sales_accounting_period",
        "clustering_fields": ["code"]
    },
    "internal_interest": {
        "partition_field": "year_month",
        "clustering_fields": ["branch"]
    },
    "profit_plan_term": {
        "partition_field": "period",
        "clustering_fields": ["item"]
    },
    "ledger_loss": {
        "partition_field": "slip_date",  # DATE(slip_date)ã§ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³
        "clustering_fields": ["classification_type"]
    }
}

def create_bigquery_client():
    """BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ä½œæˆ"""
    client = bigquery.Client(project=PROJECT_ID)
    return client

def check_table_exists(client: bigquery.Client, table_name: str) -> bool:
    """ãƒ†ãƒ¼ãƒ–ãƒ«ã®å­˜åœ¨ç¢ºèª"""
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
    try:
        client.get_table(table_id)
        return True
    except Exception:
        return False

def load_csv_to_bigquery(
    client: bigquery.Client,
    table_name: str,
    gcs_uri: str,
    yyyymm: str
) -> bool:
    """
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’BigQueryã«ãƒ­ãƒ¼ãƒ‰
    
    Args:
        client: BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
        gcs_uri: GCSä¸Šã®CSVãƒ•ã‚¡ã‚¤ãƒ«URI
        yyyymm: å¯¾è±¡å¹´æœˆ
    
    Returns:
        æˆåŠŸæ™‚True
    """
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
    
    try:
        # ã‚¸ãƒ§ãƒ–è¨­å®š
        job_config = bigquery.LoadJobConfig(
            source_format=bigquery.SourceFormat.CSV,
            skip_leading_rows=1,  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            autodetect=False,  # ã‚¹ã‚­ãƒ¼ãƒã¯å®šç¾©æ¸ˆã¿
            write_disposition=bigquery.WriteDisposition.WRITE_APPEND,  # è¿½åŠ ãƒ¢ãƒ¼ãƒ‰
            schema_update_options=[
                bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION,
            ],
            allow_quoted_newlines=True,
            allow_jagged_rows=False,
            ignore_unknown_values=False,
            max_bad_records=0,  # ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’è¨±å®¹ã—ãªã„
        )
        
        # ãƒ­ãƒ¼ãƒ‰ã‚¸ãƒ§ãƒ–ã®å®Ÿè¡Œ
        load_job = client.load_table_from_uri(
            gcs_uri,
            table_id,
            job_config=job_config
        )
        
        print(f"   â³ ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {table_name} (Job ID: {load_job.job_id})")
        
        # ã‚¸ãƒ§ãƒ–ã®å®Œäº†ã‚’å¾…æ©Ÿï¼ˆæœ€å¤§5åˆ†ï¼‰
        load_job.result(timeout=300)
        
        # ãƒ­ãƒ¼ãƒ‰çµæœã®ç¢ºèª
        destination_table = client.get_table(table_id)
        print(f"   âœ… ãƒ­ãƒ¼ãƒ‰å®Œäº†: {load_job.output_rows} è¡Œã‚’è¿½åŠ ")
        print(f"      ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {destination_table.num_rows:,} è¡Œ")
        
        return True
        
    except GoogleCloudError as e:
        print(f"   âŒ ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’è¡¨ç¤º
        if hasattr(e, 'errors') and e.errors:
            for error in e.errors:
                print(f"      è©³ç´°: {error}")
        return False
    except Exception as e:
        print(f"   âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def delete_partition_data(
    client: bigquery.Client,
    table_name: str,
    yyyymm: str
) -> bool:
    """
    æŒ‡å®šæœˆã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ï¼ˆé‡è¤‡é˜²æ­¢ï¼‰
    
    Args:
        client: BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
        yyyymm: å¯¾è±¡å¹´æœˆï¼ˆä¾‹: 202509ï¼‰
    
    Returns:
        æˆåŠŸæ™‚True
    """
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
    partition_field = TABLE_CONFIG[table_name]["partition_field"]
    
    # yyyymmã‹ã‚‰å¹´æœˆã‚’æŠ½å‡º
    year = yyyymm[:4]
    month = yyyymm[4:6]
    
    # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ¡ä»¶ã«å¿œã˜ãŸDELETEæ–‡ã‚’ä½œæˆ
    if table_name in ["ledger_income", "ledger_loss"]:
        # DATETIMEå‹ã®å ´åˆ
        delete_query = f"""
        DELETE FROM `{table_id}`
        WHERE DATE({partition_field}) = '{year}-{month}-01'
        """
    else:
        # DATEå‹ã®å ´åˆ
        delete_query = f"""
        DELETE FROM `{table_id}`
        WHERE {partition_field} = '{year}-{month}-01'
        """
    
    try:
        print(f"   ğŸ—‘ï¸  æ—¢å­˜ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ä¸­: {year}-{month}")
        query_job = client.query(delete_query)
        query_job.result()  # å®Œäº†ã‚’å¾…æ©Ÿ
        
        if query_job.num_dml_affected_rows:
            print(f"      å‰Šé™¤: {query_job.num_dml_affected_rows} è¡Œ")
        else:
            print(f"      å‰Šé™¤å¯¾è±¡ãªã—")
        
        return True
        
    except Exception as e:
        print(f"   âš ï¸  å‰Šé™¤å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—: {e}")
        return True  # å‰Šé™¤å¤±æ•—ã—ã¦ã‚‚ãƒ­ãƒ¼ãƒ‰ã¯ç¶šè¡Œ

def process_all_tables(yyyymm: str, replace_existing: bool = False):
    """
    å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ã®BigQueryãƒ­ãƒ¼ãƒ‰å‡¦ç†
    
    Args:
        yyyymm: å¯¾è±¡å¹´æœˆï¼ˆä¾‹: 202509ï¼‰
        replace_existing: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰è¿½åŠ 
    """
    print("=" * 60)
    print(f"proceed/ â†’ BigQuery ãƒ­ãƒ¼ãƒ‰å‡¦ç†")
    print(f"å¯¾è±¡å¹´æœˆ: {yyyymm}")
    print(f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {PROJECT_ID}")
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {DATASET_ID}")
    print(f"ãƒ¢ãƒ¼ãƒ‰: {'REPLACE' if replace_existing else 'APPEND'}")
    print("=" * 60)
    
    # BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ
    client = create_bigquery_client()
    
    success_count = 0
    error_count = 0
    
    # å„ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‡¦ç†
    for table_name in TABLE_CONFIG.keys():
        print(f"\nğŸ“Š å‡¦ç†ä¸­: {table_name}")
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«å­˜åœ¨ç¢ºèª
        if not check_table_exists(client, table_name):
            print(f"   âŒ ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {table_name}")
            error_count += 1
            continue
        
        # GCS URI
        gcs_uri = f"gs://{LANDING_BUCKET}/proceed/{yyyymm}/{table_name}.csv"
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        storage_client = storage.Client()
        bucket = storage_client.bucket(LANDING_BUCKET)
        blob_name = f"proceed/{yyyymm}/{table_name}.csv"
        blob = bucket.blob(blob_name)
        
        if not blob.exists():
            print(f"   âš ï¸  CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {gcs_uri}")
            error_count += 1
            continue
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®å‰Šé™¤ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if replace_existing:
            delete_partition_data(client, table_name, yyyymm)
        
        # BigQueryã¸ãƒ­ãƒ¼ãƒ‰
        if load_csv_to_bigquery(client, table_name, gcs_uri, yyyymm):
            success_count += 1
        else:
            error_count += 1
    
    print("\n" + "=" * 60)
    print(f"å‡¦ç†å®Œäº†: æˆåŠŸ {success_count} / ã‚¨ãƒ©ãƒ¼ {error_count}")
    print("=" * 60)
    
    # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
    if success_count > 0:
        print("\nğŸ“ˆ ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆ:")
        for table_name in TABLE_CONFIG.keys():
            try:
                table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
                table = client.get_table(table_id)
                print(f"   {table_name}: {table.num_rows:,} è¡Œ")
            except:
                pass

def verify_load(table_name: str, yyyymm: str):
    """
    ãƒ­ãƒ¼ãƒ‰çµæœã‚’ç¢ºèª
    
    Args:
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
        yyyymm: å¯¾è±¡å¹´æœˆ
    """
    client = create_bigquery_client()
    
    year = yyyymm[:4]
    month = yyyymm[4:6]
    partition_field = TABLE_CONFIG[table_name]["partition_field"]
    
    # ä»¶æ•°ç¢ºèªã‚¯ã‚¨ãƒª
    if table_name in ["ledger_income", "ledger_loss"]:
        query = f"""
        SELECT COUNT(*) as row_count
        FROM `{PROJECT_ID}.{DATASET_ID}.{table_name}`
        WHERE DATE({partition_field}) = '{year}-{month}-01'
        """
    else:
        query = f"""
        SELECT COUNT(*) as row_count
        FROM `{PROJECT_ID}.{DATASET_ID}.{table_name}`
        WHERE {partition_field} = '{year}-{month}-01'
        """
    
    result = client.query(query).result()
    for row in result:
        print(f"ãƒ†ãƒ¼ãƒ–ãƒ«: {table_name}")
        print(f"å¯¾è±¡æœˆ: {year}-{month}")
        print(f"ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {row.row_count:,}")

if __name__ == "__main__":
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python load_to_bigquery.py YYYYMM [--replace]")
        print("  ä¾‹: python load_to_bigquery.py 202509")
        print("  ä¾‹: python load_to_bigquery.py 202509 --replace")
        sys.exit(1)
    
    yyyymm = sys.argv[1]
    replace_mode = "--replace" in sys.argv
    
    # å®Ÿè¡Œ
    process_all_tables(yyyymm, replace_existing=replace_mode)