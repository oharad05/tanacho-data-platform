#!/usr/bin/env python3
"""
proceed/ â†’ BigQuery é€£æºã‚¹ã‚¯ãƒªãƒ—ãƒˆ
CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’BigQueryãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ­ãƒ¼ãƒ‰ï¼ˆæœˆæ¬¡APPENDãƒ¢ãƒ¼ãƒ‰ï¼‰
ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚‚è‡ªå‹•è¨­å®š
"""

import os
import sys
import time
import pandas as pd
from typing import List, Dict, Optional
from google.cloud import bigquery
from google.cloud import storage
from google.cloud.exceptions import GoogleCloudError

# å›ºå®šå€¤è¨­å®š
PROJECT_ID = "data-platform-prod-475201"
DATASET_ID = "corporate_data"
LANDING_BUCKET = "data-platform-landing-prod"
MAPPING_FILE = "config/mapping/excel_mapping.csv"  # Note: ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ç¾åœ¨ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™
COLUMNS_PATH = "config/columns"

# ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©ã¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ—ã®ãƒãƒƒãƒ”ãƒ³ã‚°
TABLE_CONFIG = {
    "sales_target_and_achievements": {
        "partition_field": "sales_accounting_period",
        "clustering_fields": ["branch_code"]
    },
    "billing_balance": {
        "partition_field": "sales_month", 
        "clustering_fields": ["branch_code"]
    },
    "ledger_income": {
        "partition_field": "slip_date",  # DATE(slip_date)ã§ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³
        "clustering_fields": ["classification_type"]
    },
    "department_summary": {
        "partition_field": "sales_accounting_period",
        "clustering_fields": ["code"]
    },
    "internal_interest": {
        "partition_field": "year_month",
        "clustering_fields": ["branch"]
    },
    "profit_plan_term": {
        "partition_field": "period",
        "clustering_fields": ["item"]
    },
    "ledger_loss": {
        "partition_field": "slip_date",  # DATE(slip_date)ã§ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³
        "clustering_fields": ["classification_type"]
    },
    "stocks": {
        "partition_field": "year_month",
        "clustering_fields": ["branch"]
    },
    "ms_allocation_ratio": {
        "partition_field": "year_month",
        "clustering_fields": ["branch"]
    },
    "ms_department_category": {
        "partition_field": None,  # ãƒã‚¹ã‚¿ãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«ãªã®ã§ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ä¸è¦
        "clustering_fields": ["department_category_code"]
    }
}

def create_bigquery_client():
    """BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ä½œæˆ"""
    client = bigquery.Client(project=PROJECT_ID)
    return client

def load_table_name_mapping() -> Dict[str, str]:
    """
    ãƒ†ãƒ¼ãƒ–ãƒ«åãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ—¥æœ¬èªâ†’è‹±èªï¼‰ã‚’èª­ã¿è¾¼ã¿

    Returns:
        {è‹±èªãƒ†ãƒ¼ãƒ–ãƒ«å: æ—¥æœ¬èªå}ã®è¾æ›¸
    """
    if not os.path.exists(MAPPING_FILE):
        print(f"âš ï¸  ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {MAPPING_FILE}")
        return {}

    df = pd.read_csv(MAPPING_FILE)
    mapping = {}
    for _, row in df.iterrows():
        en_name = row['en_name']
        jp_name = row['jp_name'].replace('.xlsx', '')  # æ‹¡å¼µå­ã‚’é™¤å»
        mapping[en_name] = jp_name

    return mapping

def load_column_descriptions(table_name: str) -> Dict[str, str]:
    """
    ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚’èª­ã¿è¾¼ã¿

    Args:
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«åï¼ˆè‹±èªï¼‰

    Returns:
        {è‹±èªã‚«ãƒ©ãƒ å: èª¬æ˜}ã®è¾æ›¸
    """
    column_file = f"{COLUMNS_PATH}/{table_name}.csv"
    if not os.path.exists(column_file):
        print(f"âš ï¸  ã‚«ãƒ©ãƒ å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {column_file}")
        return {}

    df = pd.read_csv(column_file)
    descriptions = {}
    for _, row in df.iterrows():
        en_name = row['en_name']
        description = row['description']
        descriptions[en_name] = description

    return descriptions

def update_table_and_column_descriptions(
    client: bigquery.Client,
    table_name: str
) -> bool:
    """
    ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚’æ›´æ–°

    Args:
        client: BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å

    Returns:
        æˆåŠŸæ™‚True
    """
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"

    try:
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—
        table = client.get_table(table_id)

        # ãƒ†ãƒ¼ãƒ–ãƒ«åãƒãƒƒãƒ”ãƒ³ã‚°ã‚’èª­ã¿è¾¼ã¿
        table_mapping = load_table_name_mapping()
        if table_name in table_mapping:
            table.description = table_mapping[table_name]
            print(f"   ğŸ“ ãƒ†ãƒ¼ãƒ–ãƒ«èª¬æ˜ã‚’è¨­å®š: {table_mapping[table_name]}")

        # ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚’èª­ã¿è¾¼ã¿
        column_descriptions = load_column_descriptions(table_name)

        # æ—¢å­˜ã®ã‚¹ã‚­ãƒ¼ãƒã‚’å–å¾—ã—ã€èª¬æ˜ã‚’è¿½åŠ 
        new_schema = []
        for field in table.schema:
            description = column_descriptions.get(field.name, field.description)
            new_field = bigquery.SchemaField(
                name=field.name,
                field_type=field.field_type,
                mode=field.mode,
                description=description,
                fields=field.fields
            )
            new_schema.append(new_field)

        table.schema = new_schema

        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ›´æ–°
        table = client.update_table(table, ["description", "schema"])
        print(f"   âœ… {len(column_descriptions)}å€‹ã®ã‚«ãƒ©ãƒ èª¬æ˜ã‚’è¨­å®š")

        return True

    except Exception as e:
        print(f"   âš ï¸  èª¬æ˜ã®æ›´æ–°ã«å¤±æ•—: {e}")
        return False

def check_table_exists(client: bigquery.Client, table_name: str) -> bool:
    """ãƒ†ãƒ¼ãƒ–ãƒ«ã®å­˜åœ¨ç¢ºèª"""
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
    try:
        client.get_table(table_id)
        return True
    except Exception:
        return False

def load_csv_to_bigquery(
    client: bigquery.Client,
    table_name: str,
    gcs_uri: str,
    yyyymm: str
) -> bool:
    """
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’BigQueryã«ãƒ­ãƒ¼ãƒ‰
    
    Args:
        client: BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
        gcs_uri: GCSä¸Šã®CSVãƒ•ã‚¡ã‚¤ãƒ«URI
        yyyymm: å¯¾è±¡å¹´æœˆ
    
    Returns:
        æˆåŠŸæ™‚True
    """
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
    
    try:
        # ã‚¸ãƒ§ãƒ–è¨­å®š
        job_config = bigquery.LoadJobConfig(
            source_format=bigquery.SourceFormat.CSV,
            skip_leading_rows=1,  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            autodetect=False,  # ã‚¹ã‚­ãƒ¼ãƒã¯å®šç¾©æ¸ˆã¿
            write_disposition=bigquery.WriteDisposition.WRITE_APPEND,  # è¿½åŠ ãƒ¢ãƒ¼ãƒ‰
            schema_update_options=[
                bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION,
            ],
            allow_quoted_newlines=True,
            allow_jagged_rows=False,
            ignore_unknown_values=False,
            max_bad_records=0,  # ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’è¨±å®¹ã—ãªã„
        )
        
        # ãƒ­ãƒ¼ãƒ‰ã‚¸ãƒ§ãƒ–ã®å®Ÿè¡Œ
        load_job = client.load_table_from_uri(
            gcs_uri,
            table_id,
            job_config=job_config
        )
        
        print(f"   â³ ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {table_name} (Job ID: {load_job.job_id})")
        
        # ã‚¸ãƒ§ãƒ–ã®å®Œäº†ã‚’å¾…æ©Ÿï¼ˆæœ€å¤§5åˆ†ï¼‰
        load_job.result(timeout=300)
        
        # ãƒ­ãƒ¼ãƒ‰çµæœã®ç¢ºèª
        destination_table = client.get_table(table_id)
        print(f"   âœ… ãƒ­ãƒ¼ãƒ‰å®Œäº†: {load_job.output_rows} è¡Œã‚’è¿½åŠ ")
        print(f"      ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {destination_table.num_rows:,} è¡Œ")
        
        return True
        
    except GoogleCloudError as e:
        print(f"   âŒ ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’è¡¨ç¤º
        if hasattr(e, 'errors') and e.errors:
            for error in e.errors:
                print(f"      è©³ç´°: {error}")
        return False
    except Exception as e:
        print(f"   âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def delete_partition_data(
    client: bigquery.Client,
    table_name: str,
    yyyymm: str
) -> bool:
    """
    æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ï¼ˆé‡è¤‡é˜²æ­¢ï¼‰

    CSVã«ã¯è¤‡æ•°æœˆã®ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆãŒã‚ã‚‹ãŸã‚ã€
    ãƒ†ãƒ¼ãƒ–ãƒ«ã”ã¨ã«é©åˆ‡ãªå‰Šé™¤æ–¹æ³•ã‚’ä½¿ç”¨

    Args:
        client: BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
        yyyymm: å¯¾è±¡å¹´æœˆï¼ˆä¾‹: 202509ï¼‰

    Returns:
        æˆåŠŸæ™‚True
    """
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
    partition_field = TABLE_CONFIG[table_name]["partition_field"]

    # yyyymmã‹ã‚‰å¹´æœˆã‚’æŠ½å‡º
    year = yyyymm[:4]
    month = yyyymm[4:6]

    # ãƒ†ãƒ¼ãƒ–ãƒ«å…¨ä½“ã‚’å‰Šé™¤ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆCSVã«è¤‡æ•°æœˆã®ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã‚‹ãŸã‚ï¼‰
    if table_name in ["billing_balance", "profit_plan_term", "ledger_loss"]:
        delete_query = f"DELETE FROM `{table_id}` WHERE TRUE"
        print(f"   ğŸ—‘ï¸  å…¨ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ä¸­ï¼ˆCSVã«è¤‡æ•°æœˆã®ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã‚‹ãŸã‚ï¼‰")
    elif table_name in ["ledger_income"]:
        # DATETIMEå‹ã§ã€æŒ‡å®šæœˆã®ã™ã¹ã¦ã®æ—¥ä»˜ã‚’å‰Šé™¤
        delete_query = f"""
        DELETE FROM `{table_id}`
        WHERE DATE_TRUNC(DATE({partition_field}), MONTH) = '{year}-{month}-01'
        """
        print(f"   ğŸ—‘ï¸  æ—¢å­˜ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ä¸­: {year}-{month}ã®ã™ã¹ã¦ã®æ—¥ä»˜")
    else:
        # ãã®ä»–ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã¯æŒ‡å®šæœˆã®ã¿å‰Šé™¤
        delete_query = f"""
        DELETE FROM `{table_id}`
        WHERE DATE_TRUNC({partition_field}, MONTH) = '{year}-{month}-01'
        """
        print(f"   ğŸ—‘ï¸  æ—¢å­˜ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ä¸­: {year}-{month}")

    try:
        query_job = client.query(delete_query)
        query_job.result()  # å®Œäº†ã‚’å¾…æ©Ÿ

        if query_job.num_dml_affected_rows:
            print(f"      å‰Šé™¤: {query_job.num_dml_affected_rows} è¡Œ")
        else:
            print(f"      å‰Šé™¤å¯¾è±¡ãªã—")

        return True

    except Exception as e:
        print(f"   âš ï¸  å‰Šé™¤å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—: {e}")
        return True  # å‰Šé™¤å¤±æ•—ã—ã¦ã‚‚ãƒ­ãƒ¼ãƒ‰ã¯ç¶šè¡Œ

def process_all_tables(yyyymm: str, replace_existing: bool = True):
    """
    å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ã®BigQueryãƒ­ãƒ¼ãƒ‰å‡¦ç†
    
    Args:
        yyyymm: å¯¾è±¡å¹´æœˆï¼ˆä¾‹: 202509ï¼‰
        replace_existing: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰è¿½åŠ 
    """
    print("=" * 60)
    print(f"proceed/ â†’ BigQuery ãƒ­ãƒ¼ãƒ‰å‡¦ç†")
    print(f"å¯¾è±¡å¹´æœˆ: {yyyymm}")
    print(f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {PROJECT_ID}")
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {DATASET_ID}")
    print(f"ãƒ¢ãƒ¼ãƒ‰: {'REPLACE' if replace_existing else 'APPEND'}")
    print("=" * 60)
    
    # BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ
    client = create_bigquery_client()
    
    success_count = 0
    error_count = 0
    
    # å„ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‡¦ç†
    for table_name in TABLE_CONFIG.keys():
        print(f"\nğŸ“Š å‡¦ç†ä¸­: {table_name}")
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«å­˜åœ¨ç¢ºèª
        if not check_table_exists(client, table_name):
            print(f"   âŒ ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {table_name}")
            error_count += 1
            continue
        
        # GCS URI
        gcs_uri = f"gs://{LANDING_BUCKET}/proceed/{yyyymm}/{table_name}.csv"
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        storage_client = storage.Client()
        bucket = storage_client.bucket(LANDING_BUCKET)
        blob_name = f"proceed/{yyyymm}/{table_name}.csv"
        blob = bucket.blob(blob_name)
        
        if not blob.exists():
            print(f"   âš ï¸  CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {gcs_uri}")
            error_count += 1
            continue
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®å‰Šé™¤ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if replace_existing:
            delete_partition_data(client, table_name, yyyymm)
        
        # BigQueryã¸ãƒ­ãƒ¼ãƒ‰
        if load_csv_to_bigquery(client, table_name, gcs_uri, yyyymm):
            # ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚’æ›´æ–°
            update_table_and_column_descriptions(client, table_name)
            success_count += 1
        else:
            error_count += 1
    
    print("\n" + "=" * 60)
    print(f"å‡¦ç†å®Œäº†: æˆåŠŸ {success_count} / ã‚¨ãƒ©ãƒ¼ {error_count}")
    print("=" * 60)
    
    # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
    if success_count > 0:
        print("\nğŸ“ˆ ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆ:")
        for table_name in TABLE_CONFIG.keys():
            try:
                table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
                table = client.get_table(table_id)
                print(f"   {table_name}: {table.num_rows:,} è¡Œ")
            except:
                pass

def verify_load(table_name: str, yyyymm: str):
    """
    ãƒ­ãƒ¼ãƒ‰çµæœã‚’ç¢ºèª
    
    Args:
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
        yyyymm: å¯¾è±¡å¹´æœˆ
    """
    client = create_bigquery_client()
    
    year = yyyymm[:4]
    month = yyyymm[4:6]
    partition_field = TABLE_CONFIG[table_name]["partition_field"]
    
    # ä»¶æ•°ç¢ºèªã‚¯ã‚¨ãƒª
    if table_name in ["ledger_income", "ledger_loss"]:
        query = f"""
        SELECT COUNT(*) as row_count
        FROM `{PROJECT_ID}.{DATASET_ID}.{table_name}`
        WHERE DATE({partition_field}) = '{year}-{month}-01'
        """
    else:
        query = f"""
        SELECT COUNT(*) as row_count
        FROM `{PROJECT_ID}.{DATASET_ID}.{table_name}`
        WHERE {partition_field} = '{year}-{month}-01'
        """
    
    result = client.query(query).result()
    for row in result:
        print(f"ãƒ†ãƒ¼ãƒ–ãƒ«: {table_name}")
        print(f"å¯¾è±¡æœˆ: {year}-{month}")
        print(f"ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {row.row_count:,}")

if __name__ == "__main__":
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python load_to_bigquery.py YYYYMM [--replace]")
        print("  ä¾‹: python load_to_bigquery.py 202509")
        print("  ä¾‹: python load_to_bigquery.py 202509 --replace")
        sys.exit(1)
    
    yyyymm = sys.argv[1]
    replace_mode = "--replace" in sys.argv
    
    # å®Ÿè¡Œ
    process_all_tables(yyyymm, replace_existing=replace_mode)