#!/usr/bin/env python3
"""
proceed/ â†’ BigQuery é€£æºã‚¹ã‚¯ãƒªãƒ—ãƒˆ
CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’BigQueryãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ­ãƒ¼ãƒ‰

ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‹•ä½œ:
- 2024/9ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã‚’å…¨ã¦å‰Šé™¤
- GCSã®proceed/ãƒ•ã‚©ãƒ«ãƒ€ã«ã‚ã‚‹å…¨å¹´æœˆã®CSVã‚’ãƒ­ãƒ¼ãƒ‰

ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚‚è‡ªå‹•è¨­å®š

ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¿ã‚¤ãƒ—:
- å˜æœˆå‹: å„CSVãŒãã®æœˆã®ãƒ‡ãƒ¼ã‚¿ã®ã¿å«ã‚€ â†’ å…¨CSVã‚’ãã®ã¾ã¾ãƒ­ãƒ¼ãƒ‰
- ç´¯ç©å‹: å„CSVãŒå…¨æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ â†’ å…¨CSVã‚’çµåˆã—ã¦ã‚­ãƒ¼æ¯ã«æœ€æ–°ãƒ•ã‚©ãƒ«ãƒ€ã‚’å„ªå…ˆ
"""

import os
import sys
import time
import io
import pandas as pd
from typing import List, Dict, Optional
from google.cloud import bigquery
from google.cloud import storage
from google.cloud.exceptions import GoogleCloudError

# å›ºå®šå€¤è¨­å®š
PROJECT_ID = "data-platform-prod-475201"
DATASET_ID = "corporate_data"
LANDING_BUCKET = "data-platform-landing-prod"
MAPPING_FILE = "google-drive/config/mapping/excel_mapping.csv"  # Note: ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ç¾åœ¨ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™
COLUMNS_PATH = "google-drive/config/columns"

# ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€£æºç”¨è¨­å®š
SPREADSHEET_PROCEED_PATH = "spreadsheet/proceed"
SPREADSHEET_COLUMNS_PATH = "spreadsheet/config/columns"
SPREADSHEET_TABLE_PREFIX = "ss_"

# ã‚¼ãƒ­æ—¥ä»˜å¤‰æ›è¨­å®šï¼ˆ0000/00/00ã‚’nullã«å¤‰æ›ï¼‰
ZERO_DATE_CONFIG = {
    "construction_progress_days_amount": [
        "contract_date",
        "construction_start_date",
        "construction_end_date",
        "completion_date",
    ],
}

# ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«è¨­å®šï¼ˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãªã—ï¼‰
SPREADSHEET_TABLE_CONFIG = {
    "gs_sales_profit": {
        "description": "GSå£²ä¸Šåˆ©ç›Š",
    },
    "inventory_advance_tokyo": {
        "description": "æ±äº¬åœ¨åº«å‰æ‰•",
    },
    "inventory_advance_nagasaki": {
        "description": "é•·å´åœ¨åº«å‰æ‰•",
    },
    "inventory_advance_fukuoka": {
        "description": "ç¦å²¡åœ¨åº«å‰æ‰•",
    },
}

# æœŸé¦–ï¼ˆãƒ‡ãƒ¼ã‚¿é–‹å§‹æ—¥ï¼‰
FISCAL_START_YYYYMM = "202409"  # 2024å¹´9æœˆ
FISCAL_START_DATE = "2024-09-01"

# ç´¯ç©å‹ãƒ†ãƒ¼ãƒ–ãƒ«ã®å®šç¾©ï¼ˆå„CSVãŒå…¨æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰
# ã‚­ãƒ¼æ¯ã«æœ€æ–°ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆmax(source_folder)ï¼‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆã—ã¦ãƒ­ãƒ¼ãƒ‰
CUMULATIVE_TABLE_CONFIG = {
    "billing_balance": {
        # ã‚½ãƒ¼ã‚¹: 3_è«‹æ±‚æ®‹é«˜ä¸€è¦§è¡¨ï¼ˆæœˆé–“ï¼‰.xlsx
        "unique_keys": ["sales_month", "branch_code", "branch_name"],
    },
    "profit_plan_term": {
        # ã‚½ãƒ¼ã‚¹: 12_æç›Š5æœŸç›®æ¨™.xlsxï¼ˆæ±äº¬æ”¯åº—ç›®æ¨™103æœŸã‚·ãƒ¼ãƒˆï¼‰
        "unique_keys": ["period", "item"],
    },
    "profit_plan_term_nagasaki": {
        # ã‚½ãƒ¼ã‚¹: 12_æç›Š5æœŸç›®æ¨™.xlsxï¼ˆé•·å´æ”¯åº—ç›®æ¨™103æœŸã‚·ãƒ¼ãƒˆï¼‰
        "unique_keys": ["period", "item"],
    },
    "profit_plan_term_fukuoka": {
        # ã‚½ãƒ¼ã‚¹: 12_æç›Š5æœŸç›®æ¨™.xlsxï¼ˆç¦å²¡æ”¯åº—ç›®æ¨™103æœŸã‚·ãƒ¼ãƒˆï¼‰
        "unique_keys": ["period", "item"],
    },
    "ms_allocation_ratio": {
        # ã‚½ãƒ¼ã‚¹: 10_æ¡ˆåˆ†æ¯”ç‡ãƒã‚¹ã‚¿.xlsx
        "unique_keys": ["year_month", "branch", "department", "category"],
    },
    "construction_progress_days_amount": {
        # ã‚½ãƒ¼ã‚¹: å·¥äº‹é€²æ—æ—¥æ•°é‡‘é¡.xlsx
        # property_periodï¼ˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ—ï¼‰ã‚‚å«ã‚ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼ã¨ã™ã‚‹
        "unique_keys": ["property_period", "branch_code", "staff_code", "property_number", "customer_code", "contract_date"],
    },
    "stocks": {
        # ã‚½ãƒ¼ã‚¹: 9_åœ¨åº«.xlsx
        # å¹´æœˆãƒ»æ”¯åº—ãƒ»éƒ¨ç½²ãƒ»ã‚«ãƒ†ã‚´ãƒªã§ãƒ¦ãƒ‹ãƒ¼ã‚¯
        "unique_keys": ["year_month", "branch", "department", "category"],
    },
    "construction_progress_days_final_date": {
        # ã‚½ãƒ¼ã‚¹: å·¥äº‹é€²æ—æ—¥æ•°æœ€çµ‚æ—¥.xlsx
        # æœ€çµ‚è«‹æ±‚å£²ä¸Šæ—¥ãƒ»ç‰©ä»¶ç•ªå·ãƒ»ç‰©ä»¶ãƒ‡ãƒ¼ã‚¿åŒºåˆ†ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯
        "unique_keys": ["final_billing_sales_date", "property_number", "property_data_classification"],
    },
}

# ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©ã¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ—ã®ãƒãƒƒãƒ”ãƒ³ã‚°
TABLE_CONFIG = {
    "sales_target_and_achievements": {
        "partition_field": "sales_accounting_period",
        "clustering_fields": ["branch_code"]
    },
    "billing_balance": {
        "partition_field": "sales_month", 
        "clustering_fields": ["branch_code"]
    },
    "ledger_income": {
        "partition_field": "slip_date",  # DATE(slip_date)ã§ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³
        "clustering_fields": ["classification_type"]
    },
    "department_summary": {
        "partition_field": "sales_accounting_period",
        "clustering_fields": ["code"]
    },
    "internal_interest": {
        "partition_field": "year_month",
        "clustering_fields": ["branch"]
    },
    "profit_plan_term": {
        "partition_field": "period",
        "clustering_fields": ["item"]
    },
    "profit_plan_term_nagasaki": {
        "partition_field": "period",
        "clustering_fields": ["item"]
    },
    "profit_plan_term_fukuoka": {
        "partition_field": "period",
        "clustering_fields": ["item"]
    },
    "ledger_loss": {
        "partition_field": "accounting_month",  # accounting_monthã§å‰Šé™¤åˆ¤å®š
        "clustering_fields": ["classification_type"]
    },
    "stocks": {
        "partition_field": "year_month",
        "clustering_fields": ["branch"]
    },
    "ms_allocation_ratio": {
        "partition_field": "year_month",
        "clustering_fields": ["branch"]
    },
    "customer_sales_target_and_achievements": {
        "partition_field": "sales_accounting_period",
        "clustering_fields": ["branch_code"]
    },
    "construction_progress_days_amount": {
        "partition_field": "property_period",
        "clustering_fields": ["branch_code"]
    },
    "construction_progress_days_final_date": {
        "partition_field": "final_billing_sales_date",
        "clustering_fields": []
    },
}

def create_bigquery_client():
    """BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ä½œæˆ"""
    client = bigquery.Client(project=PROJECT_ID)
    return client

def get_available_months_from_gcs() -> list:
    """GCSã®google-drive/proceed/ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªå¹´æœˆãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆ2024/9ä»¥é™ï¼‰"""
    storage_client = storage.Client()
    bucket = storage_client.bucket(LANDING_BUCKET)
    blobs = bucket.list_blobs(prefix="google-drive/proceed/")

    months = set()
    for blob in blobs:
        # google-drive/proceed/202409/xxx.csv ã®ã‚ˆã†ãªå½¢å¼ã‹ã‚‰yyyymmã‚’æŠ½å‡º
        parts = blob.name.split("/")
        if len(parts) >= 3 and parts[2].isdigit() and len(parts[2]) == 6:
            yyyymm = parts[2]
            # 2024/9ä»¥é™ã®ã¿å¯¾è±¡
            if yyyymm >= FISCAL_START_YYYYMM:
                months.add(yyyymm)

    return sorted(list(months))

def delete_all_data_since_fiscal_start(
    client: bigquery.Client,
    table_name: str
) -> bool:
    """
    2024å¹´9æœˆä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã‚’å…¨ã¦å‰Šé™¤ï¼ˆå†ªç­‰æ€§ä¿è¨¼ï¼‰

    Args:
        client: BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å

    Returns:
        æˆåŠŸæ™‚True
    """
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
    partition_field = TABLE_CONFIG[table_name]["partition_field"]

    delete_query = f"""
    DELETE FROM `{table_id}`
    WHERE {partition_field} >= '{FISCAL_START_DATE}'
    """

    try:
        print(f"   ğŸ—‘ï¸  æ—¢å­˜ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ä¸­: {FISCAL_START_DATE}ä»¥é™")
        query_job = client.query(delete_query)
        query_job.result()

        if query_job.num_dml_affected_rows:
            print(f"      å‰Šé™¤: {query_job.num_dml_affected_rows} è¡Œ")
        else:
            print(f"      å‰Šé™¤å¯¾è±¡ãªã—")

        return True

    except Exception as e:
        print(f"   âš ï¸  å‰Šé™¤å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—: {e}")
        return True

def load_table_name_mapping() -> Dict[str, str]:
    """
    ãƒ†ãƒ¼ãƒ–ãƒ«åãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ—¥æœ¬èªâ†’è‹±èªï¼‰ã‚’èª­ã¿è¾¼ã¿

    Returns:
        {è‹±èªãƒ†ãƒ¼ãƒ–ãƒ«å: æ—¥æœ¬èªå}ã®è¾æ›¸
    """
    if not os.path.exists(MAPPING_FILE):
        print(f"âš ï¸  ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {MAPPING_FILE}")
        return {}

    df = pd.read_csv(MAPPING_FILE)
    mapping = {}
    for _, row in df.iterrows():
        en_name = row['en_name']
        jp_name = row['jp_name'].replace('.xlsx', '')  # æ‹¡å¼µå­ã‚’é™¤å»
        mapping[en_name] = jp_name

    return mapping

def load_column_descriptions(table_name: str) -> Dict[str, str]:
    """
    ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚’èª­ã¿è¾¼ã¿

    Args:
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«åï¼ˆè‹±èªï¼‰

    Returns:
        {è‹±èªã‚«ãƒ©ãƒ å: èª¬æ˜}ã®è¾æ›¸
    """
    column_file = f"{COLUMNS_PATH}/{table_name}.csv"
    if not os.path.exists(column_file):
        print(f"âš ï¸  ã‚«ãƒ©ãƒ å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {column_file}")
        return {}

    df = pd.read_csv(column_file)
    descriptions = {}
    for _, row in df.iterrows():
        en_name = row['en_name']
        description = row['description']
        descriptions[en_name] = description

    return descriptions

def update_table_and_column_descriptions(
    client: bigquery.Client,
    table_name: str
) -> bool:
    """
    ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚’æ›´æ–°

    Args:
        client: BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å

    Returns:
        æˆåŠŸæ™‚True
    """
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"

    try:
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—
        table = client.get_table(table_id)

        # ãƒ†ãƒ¼ãƒ–ãƒ«åãƒãƒƒãƒ”ãƒ³ã‚°ã‚’èª­ã¿è¾¼ã¿
        table_mapping = load_table_name_mapping()
        if table_name in table_mapping:
            table.description = table_mapping[table_name]
            print(f"   ğŸ“ ãƒ†ãƒ¼ãƒ–ãƒ«èª¬æ˜ã‚’è¨­å®š: {table_mapping[table_name]}")

        # ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚’èª­ã¿è¾¼ã¿
        column_descriptions = load_column_descriptions(table_name)

        # æ—¢å­˜ã®ã‚¹ã‚­ãƒ¼ãƒã‚’å–å¾—ã—ã€èª¬æ˜ã‚’è¿½åŠ 
        new_schema = []
        for field in table.schema:
            description = column_descriptions.get(field.name, field.description)
            new_field = bigquery.SchemaField(
                name=field.name,
                field_type=field.field_type,
                mode=field.mode,
                description=description,
                fields=field.fields
            )
            new_schema.append(new_field)

        table.schema = new_schema

        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ›´æ–°
        table = client.update_table(table, ["description", "schema"])
        print(f"   âœ… {len(column_descriptions)}å€‹ã®ã‚«ãƒ©ãƒ èª¬æ˜ã‚’è¨­å®š")

        return True

    except Exception as e:
        print(f"   âš ï¸  èª¬æ˜ã®æ›´æ–°ã«å¤±æ•—: {e}")
        return False

def check_table_exists(client: bigquery.Client, table_name: str) -> bool:
    """ãƒ†ãƒ¼ãƒ–ãƒ«ã®å­˜åœ¨ç¢ºèª"""
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
    try:
        client.get_table(table_id)
        return True
    except Exception:
        return False

def load_csv_to_bigquery(
    client: bigquery.Client,
    table_name: str,
    gcs_uri: str,
    yyyymm: str
) -> bool:
    """
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’BigQueryã«ãƒ­ãƒ¼ãƒ‰
    
    Args:
        client: BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
        gcs_uri: GCSä¸Šã®CSVãƒ•ã‚¡ã‚¤ãƒ«URI
        yyyymm: å¯¾è±¡å¹´æœˆ
    
    Returns:
        æˆåŠŸæ™‚True
    """
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
    
    try:
        # ã‚¸ãƒ§ãƒ–è¨­å®š
        job_config = bigquery.LoadJobConfig(
            source_format=bigquery.SourceFormat.CSV,
            skip_leading_rows=1,  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            autodetect=False,  # ã‚¹ã‚­ãƒ¼ãƒã¯å®šç¾©æ¸ˆã¿
            write_disposition=bigquery.WriteDisposition.WRITE_APPEND,  # è¿½åŠ ãƒ¢ãƒ¼ãƒ‰
            schema_update_options=[
                bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION,
            ],
            allow_quoted_newlines=True,
            allow_jagged_rows=False,
            ignore_unknown_values=False,
            max_bad_records=0,  # ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’è¨±å®¹ã—ãªã„
        )
        
        # ãƒ­ãƒ¼ãƒ‰ã‚¸ãƒ§ãƒ–ã®å®Ÿè¡Œ
        load_job = client.load_table_from_uri(
            gcs_uri,
            table_id,
            job_config=job_config
        )
        
        print(f"   â³ ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {table_name} (Job ID: {load_job.job_id})")
        
        # ã‚¸ãƒ§ãƒ–ã®å®Œäº†ã‚’å¾…æ©Ÿï¼ˆæœ€å¤§5åˆ†ï¼‰
        load_job.result(timeout=300)
        
        # ãƒ­ãƒ¼ãƒ‰çµæœã®ç¢ºèª
        destination_table = client.get_table(table_id)
        print(f"   âœ… ãƒ­ãƒ¼ãƒ‰å®Œäº†: {load_job.output_rows} è¡Œã‚’è¿½åŠ ")
        print(f"      ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {destination_table.num_rows:,} è¡Œ")
        
        return True
        
    except GoogleCloudError as e:
        print(f"   âŒ ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’è¡¨ç¤º
        if hasattr(e, 'errors') and e.errors:
            for error in e.errors:
                print(f"      è©³ç´°: {error}")
        return False
    except Exception as e:
        print(f"   âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def delete_partition_data_by_csv(
    client: bigquery.Client,
    table_name: str,
    gcs_uri: str
) -> bool:
    """
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿æœˆã«åŸºã¥ã„ã¦æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ï¼ˆé‡è¤‡é˜²æ­¢ï¼‰

    ãƒ•ã‚©ãƒ«ãƒ€åã§ã¯ãªãã€CSVã«å«ã¾ã‚Œã‚‹å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿æœˆã‚’èª­ã¿å–ã£ã¦å‰Šé™¤ã™ã‚‹ã€‚
    ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ•ã‚©ãƒ«ãƒ€åã¨ãƒ‡ãƒ¼ã‚¿æœˆãŒãšã‚Œã¦ã„ã‚‹å ´åˆã§ã‚‚æ­£ã—ãå‹•ä½œã™ã‚‹ã€‚

    Args:
        client: BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
        gcs_uri: GCSä¸Šã®CSVãƒ•ã‚¡ã‚¤ãƒ«URI

    Returns:
        æˆåŠŸæ™‚True
    """
    import io

    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
    partition_field = TABLE_CONFIG[table_name]["partition_field"]

    try:
        # GCSã‹ã‚‰CSVã‚’èª­ã¿è¾¼ã‚“ã§ãƒ‡ãƒ¼ã‚¿æœˆã‚’å–å¾—
        storage_client = storage.Client()

        # gs://bucket/path ã®å½¢å¼ã‹ã‚‰ãƒã‚±ãƒƒãƒˆåã¨ãƒ‘ã‚¹ã‚’æŠ½å‡º
        gcs_path = gcs_uri.replace("gs://", "")
        bucket_name = gcs_path.split("/")[0]
        blob_path = "/".join(gcs_path.split("/")[1:])

        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(blob_path)
        csv_content = blob.download_as_bytes()

        df = pd.read_csv(io.BytesIO(csv_content))

        if partition_field not in df.columns:
            print(f"   âš ï¸  ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ '{partition_field}' ãŒCSVã«å­˜åœ¨ã—ã¾ã›ã‚“")
            return False

        # CSVã«å«ã¾ã‚Œã‚‹ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªæœˆã‚’å–å¾—
        unique_months = df[partition_field].dropna().unique()

        if len(unique_months) == 0:
            print(f"   âš ï¸  CSVã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return True

        print(f"   ğŸ—‘ï¸  CSVã«å«ã¾ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿æœˆ: {list(unique_months)}")

        total_deleted = 0

        for month_value in unique_months:
            # æ—¥ä»˜å‹ã‹ã©ã†ã‹ã§å‡¦ç†ã‚’åˆ†ã‘ã‚‹
            if table_name in ["ledger_income", "ledger_loss"]:
                # DATETIMEå‹ã®å ´åˆã€æœˆã®ç¯„å›²ã§å‰Šé™¤
                # month_valueã¯ "2024-09-01" ã®ã‚ˆã†ãªå½¢å¼ã‚’æƒ³å®š
                delete_query = f"""
                DELETE FROM `{table_id}`
                WHERE DATE_TRUNC(DATE({partition_field}), MONTH) = DATE('{month_value}')
                """
            else:
                # DATEå‹ã®å ´åˆ
                delete_query = f"""
                DELETE FROM `{table_id}`
                WHERE {partition_field} = DATE('{month_value}')
                """

            query_job = client.query(delete_query)
            query_job.result()

            if query_job.num_dml_affected_rows:
                total_deleted += query_job.num_dml_affected_rows

        if total_deleted > 0:
            print(f"      å‰Šé™¤: {total_deleted} è¡Œ")
        else:
            print(f"      å‰Šé™¤å¯¾è±¡ãªã—")

        return True

    except Exception as e:
        print(f"   âš ï¸  å‰Šé™¤å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return True  # å‰Šé™¤å¤±æ•—ã—ã¦ã‚‚ãƒ­ãƒ¼ãƒ‰ã¯ç¶šè¡Œ


def process_cumulative_table(
    client: bigquery.Client,
    storage_client: storage.Client,
    table_name: str,
    target_months: List[str]
) -> bool:
    """
    ç´¯ç©å‹ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å‡¦ç†

    å…¨æœˆã®CSVã‚’èª­ã¿è¾¼ã¿ã€source_folderã‚«ãƒ©ãƒ ã‚’è¿½åŠ ã—ã¦çµåˆã€‚
    ã‚­ãƒ¼æ¯ã«max(source_folder)ã®ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆã—ã¦é‡è¤‡ã‚’è§£æ¶ˆã€‚

    Args:
        client: BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        storage_client: GCSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
        target_months: å¯¾è±¡å¹´æœˆãƒªã‚¹ãƒˆ

    Returns:
        æˆåŠŸæ™‚True
    """
    print(f"\nğŸ“Š å‡¦ç†ä¸­ï¼ˆç´¯ç©å‹ï¼‰: {table_name}")

    config = CUMULATIVE_TABLE_CONFIG[table_name]
    unique_keys = config["unique_keys"]
    bucket = storage_client.bucket(LANDING_BUCKET)

    # å…¨æœˆã®CSVã‚’èª­ã¿è¾¼ã¿ã€source_folderã‚«ãƒ©ãƒ ã‚’è¿½åŠ 
    all_dfs = []
    for yyyymm in target_months:
        blob = bucket.blob(f"google-drive/proceed/{yyyymm}/{table_name}.csv")
        if blob.exists():
            csv_content = blob.download_as_string().decode("utf-8")
            df = pd.read_csv(io.StringIO(csv_content))
            df["source_folder"] = int(yyyymm)
            all_dfs.append(df)
            print(f"   ğŸ“ {yyyymm}: {len(df)}è¡Œ")

    if not all_dfs:
        print(f"   âš ï¸  CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return False

    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
    combined_df = pd.concat(all_dfs, ignore_index=True)
    print(f"   ğŸ“Š çµåˆå¾Œ: {len(combined_df)}è¡Œ")

    # ã‚­ãƒ¼æ¯ã«max(source_folder)ã§ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆæœ€æ–°ãƒ•ã‚©ãƒ«ãƒ€ã‚’å„ªå…ˆï¼‰
    idx = combined_df.groupby(unique_keys)["source_folder"].transform("max") == combined_df["source_folder"]
    deduped_df = combined_df[idx].drop_duplicates(subset=unique_keys, keep="last").reset_index(drop=True)
    print(f"   âœ¨ é‡è¤‡é™¤å»å¾Œ: {len(deduped_df)}è¡Œ")

    # ã‚¼ãƒ­æ—¥ä»˜ã‚’nullã«å¤‰æ›ï¼ˆBigQueryã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å‰ã«å¤‰æ›ï¼‰
    if table_name in ZERO_DATE_CONFIG:
        zero_date_patterns = ['0000/00/00', '0000-00-00', '0000/0/0', '0000-0-0']
        for col in ZERO_DATE_CONFIG[table_name]:
            if col in deduped_df.columns:
                for pattern in zero_date_patterns:
                    mask = deduped_df[col].astype(str).str.strip() == pattern
                    if mask.any():
                        count = mask.sum()
                        deduped_df.loc[mask, col] = None
                        print(f"   ğŸ”„ {col}: {count}ä»¶ã®ã‚¼ãƒ­æ—¥ä»˜ã‚’nullã«å¤‰æ›")

    # ä¸€æ™‚CSVã«ä¿å­˜
    temp_csv = f"/tmp/{table_name}_cumulative.csv"
    deduped_df.to_csv(temp_csv, index=False)

    # GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    temp_blob = bucket.blob(f"google-drive/temp/{table_name}_cumulative.csv")
    temp_blob.upload_from_filename(temp_csv)
    gcs_uri = f"gs://{LANDING_BUCKET}/google-drive/temp/{table_name}_cumulative.csv"

    # BigQueryã«ãƒ­ãƒ¼ãƒ‰
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"

    try:
        # ç´¯ç©å‹ãƒ†ãƒ¼ãƒ–ãƒ«ã¯å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ï¼ˆCSVã«å…¨å±¥æ­´ãŒå«ã¾ã‚Œã‚‹ãŸã‚ï¼‰
        # æ³¨: å˜æœˆå‹ãƒ†ãƒ¼ãƒ–ãƒ«ã¯2024/9ä»¥é™ã®ã¿å‰Šé™¤
        delete_query = f"""
        DELETE FROM `{table_id}`
        WHERE TRUE
        """
        query_job = client.query(delete_query)
        query_job.result()
        deleted = query_job.num_dml_affected_rows or 0
        print(f"   ğŸ—‘ï¸  æ—¢å­˜ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ï¼ˆå…¨ä»¶ï¼‰: {deleted}è¡Œ")

        # ã‚¹ã‚­ãƒ¼ãƒã‚’å–å¾—ã—ã¦source_folderã‚«ãƒ©ãƒ ã‚’è¿½åŠ ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
        table = client.get_table(table_id)
        existing_schema = list(table.schema)
        has_source_folder = any(f.name == "source_folder" for f in existing_schema)

        if not has_source_folder:
            new_schema = existing_schema + [bigquery.SchemaField("source_folder", "INTEGER")]
            table.schema = new_schema
            client.update_table(table, ["schema"])
            print(f"   â• source_folderã‚«ãƒ©ãƒ ã‚’è¿½åŠ ")

        # ãƒ­ãƒ¼ãƒ‰
        job_config = bigquery.LoadJobConfig(
            source_format=bigquery.SourceFormat.CSV,
            skip_leading_rows=1,
            write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
            allow_quoted_newlines=True,
        )
        load_job = client.load_table_from_uri(gcs_uri, table_id, job_config=job_config)
        load_job.result(timeout=300)
        print(f"   âœ… ãƒ­ãƒ¼ãƒ‰å®Œäº†: {load_job.output_rows}è¡Œ")

        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        temp_blob.delete()
        os.remove(temp_csv)

        # ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚’æ›´æ–°
        update_table_and_column_descriptions(client, table_name)

        return True

    except Exception as e:
        print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False


def get_spreadsheet_files_from_gcs() -> List[str]:
    """GCSã®spreadsheet/proceed/ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªãƒ†ãƒ¼ãƒ–ãƒ«åãƒªã‚¹ãƒˆã‚’å–å¾—"""
    storage_client = storage.Client()
    bucket = storage_client.bucket(LANDING_BUCKET)
    blobs = bucket.list_blobs(prefix=f"{SPREADSHEET_PROCEED_PATH}/")

    tables = []
    for blob in blobs:
        # spreadsheet/proceed/xxx.csv ã®å½¢å¼ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
        if blob.name.endswith(".csv"):
            table_name = blob.name.split("/")[-1].replace(".csv", "")
            if table_name in SPREADSHEET_TABLE_CONFIG:
                tables.append(table_name)

    return sorted(list(set(tables)))


def load_spreadsheet_column_descriptions(table_name: str) -> Dict[str, str]:
    """
    GCSã‹ã‚‰ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚«ãƒ©ãƒ èª¬æ˜ã‚’èª­ã¿è¾¼ã¿

    Args:
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«åï¼ˆãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãªã—ï¼‰

    Returns:
        {è‹±èªã‚«ãƒ©ãƒ å: æ—¥æœ¬èªã‚«ãƒ©ãƒ å}ã®è¾æ›¸
    """
    storage_client = storage.Client()
    bucket = storage_client.bucket(LANDING_BUCKET)
    blob_path = f"{SPREADSHEET_COLUMNS_PATH}/{table_name}.csv"
    blob = bucket.blob(blob_path)

    if not blob.exists():
        print(f"   âš ï¸  ã‚«ãƒ©ãƒ å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: gs://{LANDING_BUCKET}/{blob_path}")
        return {}

    try:
        csv_content = blob.download_as_string().decode("utf-8")
        df = pd.read_csv(io.StringIO(csv_content))
        descriptions = {}
        for _, row in df.iterrows():
            en_name = row['en_name']
            jp_name = row['jp_name']
            descriptions[en_name] = jp_name
        return descriptions
    except Exception as e:
        print(f"   âš ï¸  ã‚«ãƒ©ãƒ å®šç¾©ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {}


def load_spreadsheet_to_bigquery(
    client: bigquery.Client,
    table_name: str,
    gcs_uri: str
) -> bool:
    """
    ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆCSVã‚’BigQueryã«ãƒ­ãƒ¼ãƒ‰ï¼ˆWRITE_TRUNCATEï¼‰

    Args:
        client: BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«åï¼ˆãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãªã—ï¼‰
        gcs_uri: GCSä¸Šã®CSVãƒ•ã‚¡ã‚¤ãƒ«URI

    Returns:
        æˆåŠŸæ™‚True
    """
    bq_table_name = f"{SPREADSHEET_TABLE_PREFIX}{table_name}"
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{bq_table_name}"

    try:
        # ã‚¸ãƒ§ãƒ–è¨­å®šï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿æ´—ã„æ›¿ãˆï¼‰
        job_config = bigquery.LoadJobConfig(
            source_format=bigquery.SourceFormat.CSV,
            skip_leading_rows=1,
            autodetect=True,  # ã‚¹ã‚­ãƒ¼ãƒè‡ªå‹•æ¤œå‡º
            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,  # å…¨ãƒ‡ãƒ¼ã‚¿æ´—ã„æ›¿ãˆ
            allow_quoted_newlines=True,
        )

        # ãƒ­ãƒ¼ãƒ‰ã‚¸ãƒ§ãƒ–ã®å®Ÿè¡Œ
        load_job = client.load_table_from_uri(
            gcs_uri,
            table_id,
            job_config=job_config
        )

        print(f"   â³ ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {bq_table_name} (Job ID: {load_job.job_id})")

        # ã‚¸ãƒ§ãƒ–ã®å®Œäº†ã‚’å¾…æ©Ÿ
        load_job.result(timeout=300)

        # ãƒ­ãƒ¼ãƒ‰çµæœã®ç¢ºèª
        destination_table = client.get_table(table_id)
        print(f"   âœ… ãƒ­ãƒ¼ãƒ‰å®Œäº†: {load_job.output_rows} è¡Œ")

        # ãƒ†ãƒ¼ãƒ–ãƒ«ã®èª¬æ˜ã‚’è¨­å®š
        config = SPREADSHEET_TABLE_CONFIG.get(table_name, {})
        table_description = config.get("description", "")
        if table_description:
            destination_table.description = table_description
            client.update_table(destination_table, ["description"])

        # ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚’è¨­å®š
        column_descriptions = load_spreadsheet_column_descriptions(table_name)
        if column_descriptions:
            new_schema = []
            for field in destination_table.schema:
                description = column_descriptions.get(field.name, field.description)
                new_field = bigquery.SchemaField(
                    name=field.name,
                    field_type=field.field_type,
                    mode=field.mode,
                    description=description,
                    fields=field.fields
                )
                new_schema.append(new_field)
            destination_table.schema = new_schema
            client.update_table(destination_table, ["schema"])
            print(f"   ğŸ“ {len(column_descriptions)}å€‹ã®ã‚«ãƒ©ãƒ èª¬æ˜ã‚’è¨­å®š")

        return True

    except GoogleCloudError as e:
        print(f"   âŒ ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        if hasattr(e, 'errors') and e.errors:
            for error in e.errors:
                print(f"      è©³ç´°: {error}")
        return False
    except Exception as e:
        print(f"   âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False


def process_spreadsheet_tables() -> Dict[str, int]:
    """
    ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã®BigQueryãƒ­ãƒ¼ãƒ‰å‡¦ç†

    Returns:
        {"success": æˆåŠŸæ•°, "error": ã‚¨ãƒ©ãƒ¼æ•°, "skipped": ã‚¹ã‚­ãƒƒãƒ—æ•°}
    """
    print("\n" + "=" * 60)
    print("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ â†’ BigQuery ãƒ­ãƒ¼ãƒ‰å‡¦ç†")
    print(f"GCSãƒ‘ã‚¹: gs://{LANDING_BUCKET}/{SPREADSHEET_PROCEED_PATH}/")
    print(f"BigQueryãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹: {SPREADSHEET_TABLE_PREFIX}")
    print("=" * 60)

    # GCSã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã‚’å–å¾—
    available_tables = get_spreadsheet_files_from_gcs()

    if not available_tables:
        print("\nâš ï¸  ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print(f"   ãƒ‘ã‚¹: gs://{LANDING_BUCKET}/{SPREADSHEET_PROCEED_PATH}/")
        return {"success": 0, "error": 0, "skipped": len(SPREADSHEET_TABLE_CONFIG)}

    print(f"\nåˆ©ç”¨å¯èƒ½ãªãƒ†ãƒ¼ãƒ–ãƒ«: {', '.join(available_tables)}")

    client = create_bigquery_client()
    storage_client = storage.Client()
    bucket = storage_client.bucket(LANDING_BUCKET)

    success_count = 0
    error_count = 0
    skipped_count = 0

    for table_name, config in SPREADSHEET_TABLE_CONFIG.items():
        print(f"\nğŸ“Š å‡¦ç†ä¸­: {SPREADSHEET_TABLE_PREFIX}{table_name}")

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        blob_path = f"{SPREADSHEET_PROCEED_PATH}/{table_name}.csv"
        blob = bucket.blob(blob_path)

        if not blob.exists():
            print(f"   âš ï¸  CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: gs://{LANDING_BUCKET}/{blob_path}")
            skipped_count += 1
            continue

        gcs_uri = f"gs://{LANDING_BUCKET}/{blob_path}"

        if load_spreadsheet_to_bigquery(client, table_name, gcs_uri):
            success_count += 1
        else:
            error_count += 1

    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "-" * 40)
    print(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«å‡¦ç†å®Œäº†:")
    print(f"  æˆåŠŸ: {success_count}")
    print(f"  ã‚¨ãƒ©ãƒ¼: {error_count}")
    print(f"  ã‚¹ã‚­ãƒƒãƒ—: {skipped_count}")

    # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
    if success_count > 0:
        print("\nğŸ“ˆ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆ:")
        for table_name in SPREADSHEET_TABLE_CONFIG.keys():
            try:
                bq_table_name = f"{SPREADSHEET_TABLE_PREFIX}{table_name}"
                table_id = f"{PROJECT_ID}.{DATASET_ID}.{bq_table_name}"
                table = client.get_table(table_id)
                print(f"   {bq_table_name}: {table.num_rows:,} è¡Œ")
            except Exception:
                pass

    return {"success": success_count, "error": error_count, "skipped": skipped_count}


def process_all_tables(yyyymm: str = None):
    """
    å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ã®BigQueryãƒ­ãƒ¼ãƒ‰å‡¦ç†

    ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‹•ä½œ:
    - 2024/9ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã‚’å…¨ã¦å‰Šé™¤
    - GCSã®proceed/ãƒ•ã‚©ãƒ«ãƒ€ã«ã‚ã‚‹å…¨å¹´æœˆã®CSVã‚’ãƒ­ãƒ¼ãƒ‰

    ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥å‡¦ç†:
    - å˜æœˆå‹: å„CSVã‚’ãã®ã¾ã¾ãƒ­ãƒ¼ãƒ‰
    - ç´¯ç©å‹: å…¨CSVã‚’çµåˆã—ã¦ã‚­ãƒ¼æ¯ã«æœ€æ–°ãƒ•ã‚©ãƒ«ãƒ€ã‚’å„ªå…ˆ

    Args:
        yyyymm: å¯¾è±¡å¹´æœˆï¼ˆçœç•¥æ™‚ã¯2024/9ä»¥é™ã®å…¨å¹´æœˆï¼‰
    """
    # å¯¾è±¡å¹´æœˆãƒªã‚¹ãƒˆã‚’å–å¾—
    target_months = get_available_months_from_gcs()

    print("=" * 60)
    print(f"proceed/ â†’ BigQuery ãƒ­ãƒ¼ãƒ‰å‡¦ç†")
    print(f"å¯¾è±¡å¹´æœˆ: {', '.join(target_months)}")
    print(f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {PROJECT_ID}")
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {DATASET_ID}")
    print(f"ãƒ¢ãƒ¼ãƒ‰: REPLACEï¼ˆ2024/9ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã‚’å…¨ã¦å‰Šé™¤ã—ã¦å†ãƒ­ãƒ¼ãƒ‰ï¼‰")
    print("=" * 60)

    # ç´¯ç©å‹ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
    print(f"\nç´¯ç©å‹ãƒ†ãƒ¼ãƒ–ãƒ«: {', '.join(CUMULATIVE_TABLE_CONFIG.keys())}")
    print("â€» ç´¯ç©å‹ãƒ†ãƒ¼ãƒ–ãƒ«ã¯ã‚­ãƒ¼æ¯ã«æœ€æ–°ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆ")

    # BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ
    client = create_bigquery_client()
    storage_client = storage.Client()

    success_count = 0
    error_count = 0

    # å„ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‡¦ç†
    for table_name in TABLE_CONFIG.keys():
        # ãƒ†ãƒ¼ãƒ–ãƒ«å­˜åœ¨ç¢ºèª
        if not check_table_exists(client, table_name):
            print(f"\nğŸ“Š å‡¦ç†ä¸­: {table_name}")
            print(f"   âŒ ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {table_name}")
            error_count += 1
            continue

        # ç´¯ç©å‹ãƒ†ãƒ¼ãƒ–ãƒ«ã¯å°‚ç”¨å‡¦ç†
        if table_name in CUMULATIVE_TABLE_CONFIG:
            if process_cumulative_table(client, storage_client, table_name, target_months):
                success_count += 1
            else:
                error_count += 1
            continue

        # å˜æœˆå‹ãƒ†ãƒ¼ãƒ–ãƒ«ã®å‡¦ç†
        print(f"\nğŸ“Š å‡¦ç†ä¸­ï¼ˆå˜æœˆå‹ï¼‰: {table_name}")

        # 2024/9ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã‚’å…¨ã¦å‰Šé™¤ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ã”ã¨ã«1å›ã ã‘ï¼‰
        delete_all_data_since_fiscal_start(client, table_name)

        # å…¨å¹´æœˆã®CSVã‚’ãƒ­ãƒ¼ãƒ‰
        table_success = True
        bucket = storage_client.bucket(LANDING_BUCKET)

        for month in target_months:
            gcs_uri = f"gs://{LANDING_BUCKET}/google-drive/proceed/{month}/{table_name}.csv"
            blob_name = f"google-drive/proceed/{month}/{table_name}.csv"
            blob = bucket.blob(blob_name)

            if not blob.exists():
                print(f"   âš ï¸  CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {gcs_uri}")
                continue

            # BigQueryã¸ãƒ­ãƒ¼ãƒ‰
            if not load_csv_to_bigquery(client, table_name, gcs_uri, month):
                table_success = False

        if table_success:
            # ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚’æ›´æ–°
            update_table_and_column_descriptions(client, table_name)
            success_count += 1
        else:
            error_count += 1

    print("\n" + "=" * 60)
    print(f"å‡¦ç†å®Œäº†: æˆåŠŸ {success_count} / ã‚¨ãƒ©ãƒ¼ {error_count}")
    print("=" * 60)

    # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
    if success_count > 0:
        print("\nğŸ“ˆ ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆ:")
        for table_name in TABLE_CONFIG.keys():
            try:
                table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
                table = client.get_table(table_id)
                cumulative_marker = "ï¼ˆç´¯ç©å‹ï¼‰" if table_name in CUMULATIVE_TABLE_CONFIG else ""
                print(f"   {table_name}{cumulative_marker}: {table.num_rows:,} è¡Œ")
            except:
                pass

    # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã®å‡¦ç†
    process_spreadsheet_tables()


def verify_load(table_name: str, yyyymm: str):
    """
    ãƒ­ãƒ¼ãƒ‰çµæœã‚’ç¢ºèª
    
    Args:
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
        yyyymm: å¯¾è±¡å¹´æœˆ
    """
    client = create_bigquery_client()
    
    year = yyyymm[:4]
    month = yyyymm[4:6]
    partition_field = TABLE_CONFIG[table_name]["partition_field"]
    
    # ä»¶æ•°ç¢ºèªã‚¯ã‚¨ãƒª
    if table_name in ["ledger_income", "ledger_loss"]:
        query = f"""
        SELECT COUNT(*) as row_count
        FROM `{PROJECT_ID}.{DATASET_ID}.{table_name}`
        WHERE DATE({partition_field}) = '{year}-{month}-01'
        """
    else:
        query = f"""
        SELECT COUNT(*) as row_count
        FROM `{PROJECT_ID}.{DATASET_ID}.{table_name}`
        WHERE {partition_field} = '{year}-{month}-01'
        """
    
    result = client.query(query).result()
    for row in result:
        print(f"ãƒ†ãƒ¼ãƒ–ãƒ«: {table_name}")
        print(f"å¯¾è±¡æœˆ: {year}-{month}")
        print(f"ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {row.row_count:,}")

if __name__ == "__main__":
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2024/9ä»¥é™ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’REPLACEãƒ¢ãƒ¼ãƒ‰ã§ãƒ­ãƒ¼ãƒ‰
    # å¼•æ•°ãªã—ã§å®Ÿè¡Œã™ã‚‹ã¨ã€GCSã®proceed/ã«ã‚ã‚‹å…¨å¹´æœˆãŒå¯¾è±¡

    if len(sys.argv) == 1:
        # å¼•æ•°ãªã—: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‹•ä½œï¼ˆ2024/9ä»¥é™ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’REPLACEï¼‰
        print("å¼•æ•°ãªã—: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆ2024/9ä»¥é™ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’REPLACEï¼‰")
        process_all_tables()
    elif sys.argv[1] == "--help" or sys.argv[1] == "-h":
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python load_to_bigquery.py              # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: å…¨ãƒ‡ãƒ¼ã‚¿ã‚’REPLACEï¼ˆDrive + Spreadsheetï¼‰")
        print("  python load_to_bigquery.py YYYYMM       # ç‰¹å®šæœˆã®ã¿ãƒ­ãƒ¼ãƒ‰ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ ï¼‰")
        print("  python load_to_bigquery.py --spreadsheet # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿ãƒ­ãƒ¼ãƒ‰")
        print("")
        print("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‹•ä½œ:")
        print("  - 2024/9ä»¥é™ã®Driveãƒ‡ãƒ¼ã‚¿ã‚’å…¨ã¦å‰Šé™¤ã—ã¦å†ãƒ­ãƒ¼ãƒ‰")
        print("  - ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å…¨ã¦æ´—ã„æ›¿ãˆ")
        print("")
        print("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«:")
        for table_name, config in SPREADSHEET_TABLE_CONFIG.items():
            print(f"  - {SPREADSHEET_TABLE_PREFIX}{table_name}: {config.get('description', '')}")
        sys.exit(0)
    elif sys.argv[1] == "--spreadsheet":
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿ãƒ­ãƒ¼ãƒ‰
        print("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ¢ãƒ¼ãƒ‰: ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿ãƒ­ãƒ¼ãƒ‰")
        process_spreadsheet_tables()
    else:
        # ç‰¹å®šå¹´æœˆã®ã¿ãƒ­ãƒ¼ãƒ‰ï¼ˆè¿½åŠ ãƒ¢ãƒ¼ãƒ‰ï¼‰
        yyyymm = sys.argv[1]
        print(f"ç‰¹å®šæœˆãƒ¢ãƒ¼ãƒ‰: {yyyymm} ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆè¿½åŠ ï¼‰")
        # æ—§å‹•ä½œã¨ã®äº’æ›æ€§ã®ãŸã‚ã€ç‰¹å®šæœˆæŒ‡å®šæ™‚ã¯è¿½åŠ ãƒ¢ãƒ¼ãƒ‰
        # process_all_tablesã¯å¼•æ•°ãªã—ã§å…¨ãƒ‡ãƒ¼ã‚¿REPLACEã«ãªã£ã¦ã„ã‚‹ã®ã§ã€
        # ç‰¹å®šæœˆã ã‘ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆã¯å€‹åˆ¥å‡¦ç†
        from google.cloud import storage as storage_module

        client = create_bigquery_client()
        storage_client = storage_module.Client()

        print("=" * 60)
        print(f"proceed/ â†’ BigQuery ãƒ­ãƒ¼ãƒ‰å‡¦ç†ï¼ˆç‰¹å®šæœˆï¼‰")
        print(f"å¯¾è±¡å¹´æœˆ: {yyyymm}")
        print(f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {PROJECT_ID}")
        print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {DATASET_ID}")
        print("=" * 60)

        for table_name in TABLE_CONFIG.keys():
            print(f"\nğŸ“Š å‡¦ç†ä¸­: {table_name}")

            if not check_table_exists(client, table_name):
                print(f"   âŒ ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {table_name}")
                continue

            gcs_uri = f"gs://{LANDING_BUCKET}/google-drive/proceed/{yyyymm}/{table_name}.csv"
            bucket = storage_client.bucket(LANDING_BUCKET)
            blob = bucket.blob(f"google-drive/proceed/{yyyymm}/{table_name}.csv")

            if not blob.exists():
                print(f"   âš ï¸  CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {gcs_uri}")
                continue

            # ç´¯ç©å‹ãƒ†ãƒ¼ãƒ–ãƒ«ã®å ´åˆã¯å°‚ç”¨å‡¦ç†ï¼ˆsource_folderã‚«ãƒ©ãƒ è¿½åŠ ãŒå¿…è¦ï¼‰
            if table_name in CUMULATIVE_TABLE_CONFIG:
                print(f"   ï¼ˆç´¯ç©å‹ãƒ†ãƒ¼ãƒ–ãƒ«: å…¨æœŸé–“ã§å†å‡¦ç†ï¼‰")
                target_months = get_available_months_from_gcs()
                process_cumulative_table(client, storage_client, table_name, target_months)
                continue

            # å˜æœˆå‹ãƒ†ãƒ¼ãƒ–ãƒ«ã®å‡¦ç†
            # æ—¢å­˜ã®æŒ‡å®šæœˆãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
            delete_partition_data_by_csv(client, table_name, gcs_uri)

            # BigQueryã¸ãƒ­ãƒ¼ãƒ‰
            load_csv_to_bigquery(client, table_name, gcs_uri, yyyymm)

            # èª¬æ˜ã‚’æ›´æ–°
            update_table_and_column_descriptions(client, table_name)

        print("\nå‡¦ç†å®Œäº†")