#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿æ´—ã„ãŒãˆçµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ç”¨æ–¹æ³•:
    # 1-1: å…¨ãƒ‡ãƒ¼ã‚¿æ´—ã„ãŒãˆ
    python scripts/manual/data_refresh.py --mode=full

    # 1-2: æŒ‡å®šæœˆã®ã¿æ´—ã„ãŒãˆ
    python scripts/manual/data_refresh.py --mode=monthly --month=202509

    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    --dry-run           å®Ÿè¡Œã›ãšã«å‡¦ç†å†…å®¹ã‚’è¡¨ç¤º
    --skip-backup       ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ã‚¹ã‚­ãƒƒãƒ—
    --skip-spreadsheet  ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€£æºã‚’ã‚¹ã‚­ãƒƒãƒ—
    --skip-drive        Driveé€£æºã‚’ã‚¹ã‚­ãƒƒãƒ—
"""

import os
import sys
import argparse
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from google.cloud import bigquery, storage

# å›ºå®šå€¤è¨­å®š
PROJECT_ID = "data-platform-prod-475201"
DATASET_ID = "corporate_data"
BACKUP_DATASET_ID = "corporate_data_bk"
LANDING_BUCKET = "data-platform-landing-prod"
FISCAL_START_YYYYMM = "202409"
FISCAL_START_DATE = "2024-09-01"

# ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©ï¼ˆload_to_bigquery.pyã¨åŒæœŸï¼‰
TABLE_CONFIG = {
    "sales_target_and_achievements": {"partition_field": "sales_accounting_period"},
    "billing_balance": {"partition_field": "sales_month"},
    "ledger_income": {"partition_field": "slip_date"},
    "department_summary": {"partition_field": "sales_accounting_period"},
    "internal_interest": {"partition_field": "year_month"},
    "profit_plan_term": {"partition_field": "period"},
    "profit_plan_term_nagasaki": {"partition_field": "period"},
    "profit_plan_term_fukuoka": {"partition_field": "period"},
    "ledger_loss": {"partition_field": "accounting_month"},
    "stocks": {"partition_field": "year_month"},
    "ms_allocation_ratio": {"partition_field": "year_month"},
    "customer_sales_target_and_achievements": {"partition_field": "sales_accounting_period"},
    "construction_progress_days_amount": {"partition_field": "property_period"},
    "construction_progress_days_final_date": {"partition_field": "final_billing_sales_date"},
}

# ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€£æºãƒ†ãƒ¼ãƒ–ãƒ«
SPREADSHEET_TABLES = [
    "ss_gs_sales_profit",
    "ss_inventory_advance_tokyo",
    "ss_inventory_advance_nagasaki",
    "ss_inventory_advance_fukuoka",
    "management_materials_current_month",
]


@dataclass
class RefreshResult:
    """å‡¦ç†çµæœã‚’æ ¼ç´ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    # Step 1: Drive â†’ GCS
    drive_success: List[str] = field(default_factory=list)
    drive_failed: List[str] = field(default_factory=list)

    # Step 1': ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ
    spreadsheet_success: List[str] = field(default_factory=list)
    spreadsheet_failed: List[str] = field(default_factory=list)

    # Step 2: raw â†’ proceed
    transform_success: List[str] = field(default_factory=list)
    transform_failed: List[str] = field(default_factory=list)

    # Step 5: BigQuery ãƒ­ãƒ¼ãƒ‰
    bq_success: List[str] = field(default_factory=list)
    bq_failed: List[str] = field(default_factory=list)

    # Step 6: é‡è¤‡ãƒã‚§ãƒƒã‚¯
    duplicates: Dict[str, int] = field(default_factory=dict)

    # Step 7: å·®åˆ†
    diff_results: Dict[str, Dict] = field(default_factory=dict)


def get_available_months() -> List[str]:
    """GCSã®proceed/ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªå¹´æœˆãƒªã‚¹ãƒˆã‚’å–å¾—"""
    client = storage.Client()
    bucket = client.bucket(LANDING_BUCKET)
    blobs = bucket.list_blobs(prefix="proceed/")

    months = set()
    for blob in blobs:
        parts = blob.name.split("/")
        if len(parts) >= 2 and parts[1].isdigit() and len(parts[1]) == 6:
            yyyymm = parts[1]
            if yyyymm >= FISCAL_START_YYYYMM:
                months.add(yyyymm)

    return sorted(list(months))


def get_drive_months() -> List[str]:
    """Driveã‹ã‚‰å–å¾—å¯èƒ½ãªå¹´æœˆãƒªã‚¹ãƒˆã‚’å–å¾—"""
    # sync_drive_to_gcs.pyã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
    # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«GCSã®raw/ã‹ã‚‰å–å¾—
    client = storage.Client()
    bucket = client.bucket(LANDING_BUCKET)
    blobs = bucket.list_blobs(prefix="raw/")

    months = set()
    for blob in blobs:
        parts = blob.name.split("/")
        if len(parts) >= 2 and parts[1].isdigit() and len(parts[1]) == 6:
            yyyymm = parts[1]
            if yyyymm >= FISCAL_START_YYYYMM:
                months.add(yyyymm)

    return sorted(list(months))


# ===== Step 1: Drive â†’ GCS =====
def sync_drive_to_gcs_wrapper(yyyymm: str, dry_run: bool = False) -> Tuple[bool, str]:
    """Drive â†’ GCS åŒæœŸã®ãƒ©ãƒƒãƒ‘ãƒ¼"""
    if dry_run:
        print(f"  [DRY-RUN] sync_drive_to_gcs({yyyymm})")
        return True, yyyymm

    try:
        from scripts.manual.sync_drive_to_gcs import sync_drive_to_gcs
        result = sync_drive_to_gcs(yyyymm)
        return True, yyyymm
    except Exception as e:
        print(f"  âŒ DriveåŒæœŸã‚¨ãƒ©ãƒ¼ ({yyyymm}): {e}")
        return False, yyyymm


# ===== Step 1': ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ â†’ GCS/BQ =====
def sync_spreadsheet_wrapper(dry_run: bool = False) -> Tuple[List[str], List[str]]:
    """ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆåŒæœŸã®ãƒ©ãƒƒãƒ‘ãƒ¼"""
    if dry_run:
        print(f"  [DRY-RUN] sync_spreadsheet_to_bq()")
        return SPREADSHEET_TABLES, []

    success = []
    failed = []

    try:
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€£æºã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'spreadsheet'))
        from sync_spreadsheet_to_bq import (
            load_mapping_from_gcs,
            load_columns_mapping_from_gcs,
            fetch_spreadsheet_data,
            transform_data,
            save_to_gcs,
            load_to_bigquery
        )

        mapping_df = load_mapping_from_gcs()

        for _, row in mapping_df.iterrows():
            table_name = row['en_name']
            try:
                columns_mapping = load_columns_mapping_from_gcs(table_name)
                raw_data = fetch_spreadsheet_data(row['sheet_id'], row['sheet_name'])
                df = transform_data(raw_data, columns_mapping)

                if not df.empty:
                    gcs_path = save_to_gcs(df, table_name)
                    load_to_bigquery(gcs_path, table_name, columns_mapping)
                    success.append(table_name)
                else:
                    failed.append(f"{table_name} (empty)")

            except Exception as e:
                print(f"  âŒ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ ({table_name}): {e}")
                failed.append(table_name)

        return success, failed

    except Exception as e:
        print(f"  âŒ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€£æºå…¨ä½“ã‚¨ãƒ©ãƒ¼: {e}")
        return [], ["all"]


# ===== Step 2: raw â†’ proceed =====
def transform_raw_to_proceed_wrapper(yyyymm: str, dry_run: bool = False) -> Tuple[bool, str]:
    """raw â†’ proceed å¤‰æ›ã®ãƒ©ãƒƒãƒ‘ãƒ¼"""
    if dry_run:
        print(f"  [DRY-RUN] transform_raw_to_proceed({yyyymm})")
        return True, yyyymm

    try:
        from scripts.manual.transform_raw_to_proceed import process_gcs_files
        process_gcs_files(yyyymm)
        return True, yyyymm
    except Exception as e:
        print(f"  âŒ å¤‰æ›ã‚¨ãƒ©ãƒ¼ ({yyyymm}): {e}")
        return False, yyyymm


# ===== Step 3: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— =====
def backup_tables(dry_run: bool = False) -> bool:
    """corporate_data â†’ corporate_data_bk ã¸ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
    print("\n" + "=" * 60)
    print("Step 3: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ")
    print("=" * 60)

    if dry_run:
        print(f"  [DRY-RUN] {DATASET_ID} â†’ {BACKUP_DATASET_ID}")
        return True

    client = bigquery.Client(project=PROJECT_ID)

    try:
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã‚’å–å¾—
        tables = list(client.list_tables(f"{PROJECT_ID}.{DATASET_ID}"))

        for table in tables:
            table_name = table.table_id
            source_table = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
            dest_table = f"{PROJECT_ID}.{BACKUP_DATASET_ID}.{table_name}"

            try:
                # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚³ãƒ”ãƒ¼ï¼ˆä¸Šæ›¸ãï¼‰
                job_config = bigquery.CopyJobConfig(
                    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE
                )

                copy_job = client.copy_table(source_table, dest_table, job_config=job_config)
                copy_job.result()

                print(f"  âœ… {table_name}")

            except Exception as e:
                print(f"  âš ï¸ {table_name}: {e}")

        print(f"\nâœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†: {len(tables)} ãƒ†ãƒ¼ãƒ–ãƒ«")
        return True

    except Exception as e:
        print(f"âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
        return False


# ===== Step 4-5: BigQuery ãƒ­ãƒ¼ãƒ‰ =====
def load_to_bigquery_full(dry_run: bool = False) -> Tuple[List[str], List[str]]:
    """å…¨ãƒ‡ãƒ¼ã‚¿ã‚’BigQueryã«ãƒ­ãƒ¼ãƒ‰ï¼ˆ1-1ç”¨ï¼‰"""
    if dry_run:
        print(f"  [DRY-RUN] å…¨ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ â†’ å…¨ãƒ­ãƒ¼ãƒ‰")
        return list(TABLE_CONFIG.keys()), []

    try:
        from scripts.manual.load_to_bigquery import process_all_tables
        process_all_tables()
        return list(TABLE_CONFIG.keys()), []
    except Exception as e:
        print(f"  âŒ BigQueryãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return [], list(TABLE_CONFIG.keys())


def load_to_bigquery_monthly(yyyymm: str, dry_run: bool = False) -> Tuple[List[str], List[str]]:
    """æŒ‡å®šæœˆã®ã¿BigQueryã«ãƒ­ãƒ¼ãƒ‰ï¼ˆ1-2ç”¨ï¼‰"""
    if dry_run:
        print(f"  [DRY-RUN] {yyyymm}ã®ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ â†’ ãƒ­ãƒ¼ãƒ‰")
        return list(TABLE_CONFIG.keys()), []

    success = []
    failed = []

    client = bigquery.Client(project=PROJECT_ID)
    storage_client = storage.Client()
    bucket = storage_client.bucket(LANDING_BUCKET)

    for table_name, config in TABLE_CONFIG.items():
        try:
            partition_field = config["partition_field"]
            table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"

            # å¹´æœˆã‚’æ—¥ä»˜å½¢å¼ã«å¤‰æ›
            year = yyyymm[:4]
            month = yyyymm[4:6]
            target_date = f"{year}-{month}-01"

            # æŒ‡å®šæœˆã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
            if table_name in ["ledger_income", "ledger_loss"]:
                delete_query = f"""
                DELETE FROM `{table_id}`
                WHERE DATE_TRUNC(DATE({partition_field}), MONTH) = DATE('{target_date}')
                """
            else:
                delete_query = f"""
                DELETE FROM `{table_id}`
                WHERE {partition_field} = DATE('{target_date}')
                """

            query_job = client.query(delete_query)
            query_job.result()

            deleted_rows = query_job.num_dml_affected_rows or 0
            print(f"  ğŸ—‘ï¸ {table_name}: {deleted_rows}è¡Œå‰Šé™¤")

            # CSVã‚’ãƒ­ãƒ¼ãƒ‰
            gcs_uri = f"gs://{LANDING_BUCKET}/proceed/{yyyymm}/{table_name}.csv"
            blob = bucket.blob(f"proceed/{yyyymm}/{table_name}.csv")

            if not blob.exists():
                print(f"  âš ï¸ {table_name}: CSVãƒ•ã‚¡ã‚¤ãƒ«ãªã—")
                continue

            job_config = bigquery.LoadJobConfig(
                source_format=bigquery.SourceFormat.CSV,
                skip_leading_rows=1,
                write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
                allow_quoted_newlines=True,
            )

            load_job = client.load_table_from_uri(gcs_uri, table_id, job_config=job_config)
            load_job.result()

            print(f"  âœ… {table_name}: {load_job.output_rows}è¡Œãƒ­ãƒ¼ãƒ‰")
            success.append(table_name)

        except Exception as e:
            print(f"  âŒ {table_name}: {e}")
            failed.append(table_name)

    return success, failed


# ===== Step 6: é‡è¤‡ãƒã‚§ãƒƒã‚¯ =====
def check_duplicates(dry_run: bool = False) -> Dict[str, int]:
    """é‡è¤‡ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯"""
    print("\n" + "=" * 60)
    print("Step 6: é‡è¤‡ãƒã‚§ãƒƒã‚¯")
    print("=" * 60)

    if dry_run:
        print("  [DRY-RUN] é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        return {}

    client = bigquery.Client(project=PROJECT_ID)
    duplicates = {}

    for table_name, config in TABLE_CONFIG.items():
        partition_field = config["partition_field"]
        table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"

        try:
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚¯ã‚¨ãƒªï¼ˆå…¨ã‚«ãƒ©ãƒ ã§é‡è¤‡ã‚’æ¤œå‡ºï¼‰
            query = f"""
            SELECT COUNT(*) as total_rows,
                   COUNT(DISTINCT CONCAT(CAST({partition_field} AS STRING), '_', TO_JSON_STRING(t))) as unique_rows
            FROM `{table_id}` t
            WHERE {partition_field} >= '{FISCAL_START_DATE}'
            """

            result = client.query(query).result()
            for row in result:
                diff = row.total_rows - row.unique_rows
                if diff > 0:
                    duplicates[table_name] = diff
                    print(f"  âš ï¸ {table_name}: {diff}ä»¶ã®é‡è¤‡")
                else:
                    print(f"  âœ… {table_name}: é‡è¤‡ãªã—")

        except Exception as e:
            print(f"  âš ï¸ {table_name}: ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ - {e}")

    return duplicates


# ===== Step 7: å·®åˆ†èª¿æŸ» =====
def compare_with_backup(dry_run: bool = False) -> Dict[str, Dict]:
    """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨æ–°ãƒ‡ãƒ¼ã‚¿ã®å·®åˆ†ã‚’èª¿æŸ»"""
    print("\n" + "=" * 60)
    print("Step 7: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ã®å·®åˆ†èª¿æŸ»")
    print("=" * 60)

    if dry_run:
        print("  [DRY-RUN] å·®åˆ†èª¿æŸ»ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        return {}

    client = bigquery.Client(project=PROJECT_ID)
    diff_results = {}

    for table_name in TABLE_CONFIG.keys():
        try:
            source_table = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
            backup_table = f"{PROJECT_ID}.{BACKUP_DATASET_ID}.{table_name}"

            # è¡Œæ•°æ¯”è¼ƒ
            query = f"""
            SELECT
                (SELECT COUNT(*) FROM `{source_table}`) as new_count,
                (SELECT COUNT(*) FROM `{backup_table}`) as backup_count
            """

            result = client.query(query).result()
            for row in result:
                diff = row.new_count - row.backup_count
                diff_results[table_name] = {
                    "new": row.new_count,
                    "backup": row.backup_count,
                    "diff": diff
                }

                if diff != 0:
                    print(f"  ğŸ“Š {table_name}: {row.backup_count:,} â†’ {row.new_count:,} ({diff:+,})")
                else:
                    print(f"  âœ… {table_name}: {row.new_count:,}è¡Œï¼ˆå¤‰æ›´ãªã—ï¼‰")

        except Exception as e:
            print(f"  âš ï¸ {table_name}: æ¯”è¼ƒã‚¨ãƒ©ãƒ¼ - {e}")

    return diff_results


# ===== Step 8: çµæœã‚µãƒãƒªãƒ¼ =====
def print_summary(result: RefreshResult, mode: str, month: Optional[str] = None):
    """çµæœã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
    print("\n" + "=" * 60)
    print("Step 8: å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼")
    print("=" * 60)

    print(f"\nğŸ“‹ å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: {mode}" + (f" ({month})" if month else ""))
    print(f"ğŸ“… å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # â‘  Drive â†’ GCS
    print(f"\nâ‘  Drive â†’ GCS:")
    print(f"   æˆåŠŸ: {len(result.drive_success)}ä»¶")
    print(f"   å¤±æ•—: {len(result.drive_failed)}ä»¶")
    if result.drive_failed:
        print(f"   å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«: {', '.join(result.drive_failed)}")

    # â‘¡ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ
    print(f"\nâ‘¡ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ â†’ BigQuery:")
    print(f"   æˆåŠŸ: {len(result.spreadsheet_success)}ä»¶")
    print(f"   å¤±æ•—: {len(result.spreadsheet_failed)}ä»¶")
    if result.spreadsheet_failed:
        print(f"   å¤±æ•—ã‚·ãƒ¼ãƒˆ: {', '.join(result.spreadsheet_failed)}")

    # â‘¢ raw â†’ proceed
    print(f"\nâ‘¢ GCS raw/ â†’ proceed/:")
    print(f"   æˆåŠŸ: {len(result.transform_success)}ä»¶")
    print(f"   å¤±æ•—: {len(result.transform_failed)}ä»¶")
    if result.transform_failed:
        print(f"   å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«: {', '.join(result.transform_failed)}")

    # â‘£ BigQuery ãƒ­ãƒ¼ãƒ‰
    print(f"\nâ‘£ BigQuery ãƒ­ãƒ¼ãƒ‰:")
    print(f"   æˆåŠŸ: {len(result.bq_success)}ä»¶")
    print(f"   å¤±æ•—: {len(result.bq_failed)}ä»¶")
    if result.bq_failed:
        print(f"   å¤±æ•—ãƒ†ãƒ¼ãƒ–ãƒ«: {', '.join(result.bq_failed)}")

    # â‘¤ é‡è¤‡ãƒã‚§ãƒƒã‚¯
    print(f"\nâ‘¤ é‡è¤‡ãƒã‚§ãƒƒã‚¯:")
    if result.duplicates:
        for table, count in result.duplicates.items():
            print(f"   âš ï¸ {table}: {count}ä»¶ã®é‡è¤‡")
    else:
        print(f"   âœ… é‡è¤‡ãªã—")

    # â‘¥ å·®åˆ†èª¿æŸ»
    print(f"\nâ‘¥ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ã®å·®åˆ†:")
    changes = [t for t, d in result.diff_results.items() if d.get("diff", 0) != 0]
    if changes:
        for table in changes:
            d = result.diff_results[table]
            print(f"   ğŸ“Š {table}: {d['backup']:,} â†’ {d['new']:,} ({d['diff']:+,})")
    else:
        print(f"   âœ… å·®åˆ†ãªã—")

    print("\n" + "=" * 60)


def main():
    parser = argparse.ArgumentParser(description="ãƒ‡ãƒ¼ã‚¿æ´—ã„ãŒãˆçµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    parser.add_argument("--mode", choices=["full", "monthly"], required=True,
                        help="full: å…¨ãƒ‡ãƒ¼ã‚¿æ´—ã„ãŒãˆ, monthly: æŒ‡å®šæœˆã®ã¿")
    parser.add_argument("--month", type=str, help="å¯¾è±¡å¹´æœˆï¼ˆmonthlyãƒ¢ãƒ¼ãƒ‰æ™‚å¿…é ˆï¼‰ä¾‹: 202509")
    parser.add_argument("--dry-run", action="store_true", help="å®Ÿè¡Œã›ãšã«å‡¦ç†å†…å®¹ã‚’è¡¨ç¤º")
    parser.add_argument("--skip-backup", action="store_true", help="ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ã‚¹ã‚­ãƒƒãƒ—")
    parser.add_argument("--skip-spreadsheet", action="store_true", help="ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€£æºã‚’ã‚¹ã‚­ãƒƒãƒ—")
    parser.add_argument("--skip-drive", action="store_true", help="Driveé€£æºã‚’ã‚¹ã‚­ãƒƒãƒ—")

    args = parser.parse_args()

    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    if args.mode == "monthly" and not args.month:
        parser.error("--mode=monthly ã®å ´åˆã€--month ã¯å¿…é ˆã§ã™")

    if args.month and len(args.month) != 6:
        parser.error("--month ã¯ YYYYMM å½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„ï¼ˆä¾‹: 202509ï¼‰")

    # é–‹å§‹
    print("=" * 60)
    print("ãƒ‡ãƒ¼ã‚¿æ´—ã„ãŒãˆçµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    print(f"ãƒ¢ãƒ¼ãƒ‰: {'å…¨ãƒ‡ãƒ¼ã‚¿æ´—ã„ãŒãˆ (1-1)' if args.mode == 'full' else 'æŒ‡å®šæœˆã®ã¿ (1-2)'}")
    if args.month:
        print(f"å¯¾è±¡å¹´æœˆ: {args.month}")
    print(f"Dry-run: {args.dry_run}")
    print(f"Skip backup: {args.skip_backup}")
    print(f"Skip spreadsheet: {args.skip_spreadsheet}")
    print(f"Skip drive: {args.skip_drive}")
    print("=" * 60)

    result = RefreshResult()

    # ===== Step 1: Drive â†’ GCS =====
    if not args.skip_drive:
        print("\n" + "=" * 60)
        print("Step 1: Drive â†’ GCS raw/")
        print("=" * 60)

        if args.mode == "full":
            # å…¨æœˆã‚’å–å¾—ï¼ˆDriveã‹ã‚‰ï¼‰
            # ã“ã“ã§ã¯202409ã‹ã‚‰ç¾åœ¨æœˆã¾ã§ã‚’ç”Ÿæˆ
            from datetime import datetime
            from dateutil.relativedelta import relativedelta

            start = datetime.strptime(FISCAL_START_YYYYMM, "%Y%m")
            end = datetime.now()
            months = []
            current = start
            while current <= end:
                months.append(current.strftime("%Y%m"))
                current += relativedelta(months=1)
        else:
            months = [args.month]

        for yyyymm in months:
            success, month = sync_drive_to_gcs_wrapper(yyyymm, args.dry_run)
            if success:
                result.drive_success.append(month)
            else:
                result.drive_failed.append(month)
    else:
        print("\n[SKIP] Driveé€£æºã‚’ã‚¹ã‚­ãƒƒãƒ—")

    # ===== Step 1': ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ =====
    if not args.skip_spreadsheet:
        print("\n" + "=" * 60)
        print("Step 1': ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ â†’ GCS/BigQuery")
        print("=" * 60)

        success, failed = sync_spreadsheet_wrapper(args.dry_run)
        result.spreadsheet_success = success
        result.spreadsheet_failed = failed
    else:
        print("\n[SKIP] ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€£æºã‚’ã‚¹ã‚­ãƒƒãƒ—")

    # ===== Step 2: raw â†’ proceed =====
    if not args.skip_drive:
        print("\n" + "=" * 60)
        print("Step 2: GCS raw/ â†’ proceed/")
        print("=" * 60)

        if args.mode == "full":
            months = result.drive_success  # Step 1ã§æˆåŠŸã—ãŸæœˆ
        else:
            months = [args.month]

        for yyyymm in months:
            success, month = transform_raw_to_proceed_wrapper(yyyymm, args.dry_run)
            if success:
                result.transform_success.append(month)
            else:
                result.transform_failed.append(month)

    # ===== Step 3: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— =====
    if not args.skip_backup:
        backup_tables(args.dry_run)
    else:
        print("\n[SKIP] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ã‚¹ã‚­ãƒƒãƒ—")

    # ===== Step 4-5: BigQuery ãƒ­ãƒ¼ãƒ‰ =====
    print("\n" + "=" * 60)
    print("Step 4-5: BigQuery ãƒ­ãƒ¼ãƒ‰")
    print("=" * 60)

    if args.mode == "full":
        success, failed = load_to_bigquery_full(args.dry_run)
    else:
        success, failed = load_to_bigquery_monthly(args.month, args.dry_run)

    result.bq_success = success
    result.bq_failed = failed

    # ===== Step 6: é‡è¤‡ãƒã‚§ãƒƒã‚¯ =====
    result.duplicates = check_duplicates(args.dry_run)

    # ===== Step 7: å·®åˆ†èª¿æŸ» =====
    if not args.skip_backup:
        result.diff_results = compare_with_backup(args.dry_run)

    # ===== Step 8: çµæœã‚µãƒãƒªãƒ¼ =====
    print_summary(result, args.mode, args.month)

    # çµ‚äº†ã‚³ãƒ¼ãƒ‰
    if result.drive_failed or result.spreadsheet_failed or result.transform_failed or result.bq_failed:
        sys.exit(1)
    else:
        print("\nâœ… å…¨å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
        sys.exit(0)


if __name__ == "__main__":
    main()
