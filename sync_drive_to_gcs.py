#!/usr/bin/env python3
"""
Google Drive â†’ GCS é€£æºå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å›ºå®šå€¤:
- DRIVE_FOLDER_ID: 1bHmrsqE1jdUgWbiPFsiPTymR5IRND4t6
- LANDING_BUCKET: data-platform-landing-prod
"""

import os, io, json, base64, datetime as dt, pandas as pd, re, traceback
from typing import Optional, Tuple
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from google.oauth2 import service_account
from google.cloud import storage
from google.auth import default as google_auth_default

# ===== å›ºå®šå€¤è¨­å®š =====
PROJECT_ID = "data-platform-prod-475201"
DRIVE_FOLDER_ID = "1bHmrsqE1jdUgWbiPFsiPTymR5IRND4t6"  # 02_ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
LANDING_BUCKET = "data-platform-landing-prod"
MAPPING_GCS_PATH = "config/mapping_files.csv"
SCOPES = ["https://www.googleapis.com/auth/drive.readonly"]

# ===== Drive API =====
def _build_drive_service():
    creds, _ = google_auth_default(scopes=SCOPES)
    return build("drive", "v3", credentials=creds, cache_discovery=False)

def _find_month_subfolder(drive, parent_id: str, yyyymm: str) -> Optional[str]:
    """æœˆãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¤œç´¢"""
    q = (f"'{parent_id}' in parents and "
         "mimeType='application/vnd.google-apps.folder' and "
         f"name='{yyyymm}' and trashed=false")
    try:
        res = drive.files().list(
            q=q, fields="files(id,name)",
            includeItemsFromAllDrives=True, supportsAllDrives=True, pageSize=50
        ).execute()
        files = res.get("files", [])
        if files:
            print(f"âœ… æœˆãƒ•ã‚©ãƒ«ãƒ€ç™ºè¦‹: {files[0]['name']} (ID: {files[0]['id']})")
            return files[0]["id"]
    except HttpError as e:
        print(f"âŒ ãƒ•ã‚©ãƒ«ãƒ€æ¤œç´¢å¤±æ•—: {e}")
    
    print(f"âš ï¸  æœˆãƒ•ã‚©ãƒ«ãƒ€ {yyyymm} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    return None

def _iter_files(drive, folder_id):
    """ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—"""
    q = f"'{folder_id}' in parents and trashed=false"
    page_token = None
    while True:
        res = drive.files().list(
            q=q,
            fields="nextPageToken, files(id,name,mimeType,size)",
            pageToken=page_token,
            pageSize=1000,
            includeItemsFromAllDrives=True,
            supportsAllDrives=True,
        ).execute()
        for f in res.get("files", []):
            yield f
        page_token = res.get("nextPageToken")
        if not page_token:
            break

def _download_xlsx(drive, file_id, name_hint=None):
    """Driveä¸Šã® .xlsx ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    meta = drive.files().get(fileId=file_id, fields="name", supportsAllDrives=True).execute()
    name = meta.get("name", name_hint or "file")
    out_name = name if name.lower().endswith(".xlsx") else re.sub(r"\.[^.]+$", "", name) + ".xlsx"
    req = drive.files().get_media(fileId=file_id)
    buf = io.BytesIO(req.execute())
    buf.seek(0)
    return out_name, buf, "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"

# ===== ãƒžãƒƒãƒ”ãƒ³ã‚° =====
def _load_mapping_csv():
    """GCSã‹ã‚‰ãƒžãƒƒãƒ”ãƒ³ã‚°CSVã‚’èª­ã¿è¾¼ã¿"""
    client = storage.Client()
    blob = client.bucket(LANDING_BUCKET).blob(MAPPING_GCS_PATH)
    df = pd.read_csv(io.BytesIO(blob.download_as_bytes()))
    cols = {c.strip().lower(): c for c in df.columns}
    jp_col = cols.get("jp_name") or cols.get("original_name")
    en_col = cols.get("en_name") or cols.get("english_slug")
    if not (jp_col and en_col):
        raise ValueError("mapping CSVã« 'jp_name' ã¨ 'en_name' ãŒå¿…è¦ã§ã™ã€‚")
    out = df[[jp_col, en_col]].copy()
    out.columns = ["jp_name", "en_name"]
    return out

def _slug_from_mapping(df_map: pd.DataFrame, original_name: str):
    """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚¹ãƒ©ã‚°ã‚’å–å¾—"""
    row = df_map.loc[df_map["jp_name"] == original_name]
    if row.empty:
        base = os.path.splitext(original_name)[0]
        safe = re.sub(r"[^0-9A-Za-z_]+", "_", base)
        safe = re.sub(r"_+", "_", safe).strip("_")
        print(f"  âš ï¸  ãƒžãƒƒãƒ”ãƒ³ã‚°æœªå®šç¾©: {original_name} â†’ {safe.lower()}")
        return safe.lower()
    slug = str(row.iloc[0]["en_name"]).strip()
    return slug

# ===== GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ =====
def _gcs_upload_raw(bucket, bytes_io: io.BytesIO, yyyymm: str, slug: str, content_type: str) -> str:
    """GCSã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
    path = f"raw/{yyyymm}/{slug}.xlsx"
    blob = bucket.blob(path)
    blob.content_type = content_type
    blob.upload_from_file(bytes_io)
    return f"gs://{LANDING_BUCKET}/{path}"

# ===== ãƒ¡ã‚¤ãƒ³å‡¦ç† =====
def sync_drive_to_gcs(yyyymm: str):
    """
    Google Driveã‹ã‚‰GCSã¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åŒæœŸ
    """
    print("=" * 60)
    print(f"Google Drive â†’ GCS é€£æºé–‹å§‹")
    print(f"å¯¾è±¡å¹´æœˆ: {yyyymm}")
    print(f"ãƒ•ã‚©ãƒ«ãƒ€ID: {DRIVE_FOLDER_ID}")
    print(f"GCSãƒã‚±ãƒƒãƒˆ: {LANDING_BUCKET}")
    print("=" * 60)
    
    # Drive APIæŽ¥ç¶š
    drive = _build_drive_service()
    
    # æœˆãƒ•ã‚©ãƒ«ãƒ€æ¤œç´¢
    month_id = _find_month_subfolder(drive, DRIVE_FOLDER_ID, yyyymm)
    if not month_id:
        return
    
    # ãƒžãƒƒãƒ”ãƒ³ã‚°èª­è¾¼
    try:
        df_map = _load_mapping_csv()
        print(f"âœ… ãƒžãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ ({len(df_map)}ä»¶)")
    except Exception as e:
        print(f"âŒ ãƒžãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        df_map = pd.DataFrame(columns=["jp_name", "en_name"])
    
    # GCSæº–å‚™
    storage_client = storage.Client()
    bucket = storage_client.bucket(LANDING_BUCKET)
    
    processed = 0
    skipped = 0
    
    print("\nðŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­...")
    print("-" * 40)
    
    for f in _iter_files(drive, month_id):
        name = f.get("name", "")
        lname = name.lower()
        
        if not lname.endswith(".xlsx"):
            skipped += 1
            continue
        
        try:
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            out_name, filebytes, ctype = _download_xlsx(drive, f["id"], name_hint=name)
            
            # ãƒžãƒƒãƒ”ãƒ³ã‚°è§£æ±º
            slug = _slug_from_mapping(df_map, name)
            
            # GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            gcs_uri = _gcs_upload_raw(bucket, filebytes, yyyymm, slug, ctype)
            
            print(f"âœ… {name}")
            print(f"   â†’ {gcs_uri}")
            processed += 1
            
        except Exception as e:
            print(f"âŒ {name}: {e}")
    
    print("-" * 40)
    print(f"\nðŸ“Š å‡¦ç†çµæžœ:")
    print(f"  å‡¦ç†æ¸ˆã¿: {processed} ãƒ•ã‚¡ã‚¤ãƒ«")
    print(f"  ã‚¹ã‚­ãƒƒãƒ—: {skipped} ãƒ•ã‚¡ã‚¤ãƒ«")
    print("=" * 60)
    
    return processed

if __name__ == "__main__":
    import sys
    yyyymm = sys.argv[1] if len(sys.argv) > 1 else "202509"
    sync_drive_to_gcs(yyyymm)