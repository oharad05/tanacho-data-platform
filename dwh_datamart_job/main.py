#!/usr/bin/env python3
"""
DWH/DataMartæ›´æ–°ã‚¸ãƒ§ãƒ–
=====================
GCSã‹ã‚‰SQLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€BigQueryã§é †æ¬¡å®Ÿè¡Œã™ã‚‹Cloud Run Job

ä½¿ç”¨æ–¹æ³•:
  - ç’°å¢ƒå¤‰æ•° UPDATE_TYPE ã§æ›´æ–°ã‚¿ã‚¤ãƒ—ã‚’æŒ‡å®š
    - "dwh": DWHãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿æ›´æ–°
    - "datamart": DataMartãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿æ›´æ–°
    - "all": DWH + DataMart ä¸¡æ–¹æ›´æ–°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰

ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½:
  - DataMartæ›´æ–°å¾Œã«ã€Œsecondary_department='ãã®ä»–'ã€ã®value>0ã‚’ãƒã‚§ãƒƒã‚¯
  - corporate_dataãƒ†ãƒ¼ãƒ–ãƒ«ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
  - çµæœã¯Google Cloud Loggingã«å‡ºåŠ›
"""

import os
import sys
import json
import logging
import yaml
from datetime import datetime
from typing import Dict, Any, List, Optional
from google.cloud import bigquery
from google.cloud import storage

# ============================================================
# çµ±ä¸€ãƒ­ã‚°è¨­å®š
# ============================================================

VALIDATION_ENABLED = os.environ.get("VALIDATION_ENABLED", "true").lower() == "true"

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è­˜åˆ¥ç”¨ã®è¨­å®š
PIPELINE_ID = "data-pipeline"
# execution_idã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã€ãªã‘ã‚Œã°ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ç”Ÿæˆ
EXECUTION_ID = os.environ.get("EXECUTION_ID", datetime.utcnow().strftime("%Y%m%d_%H%M%S"))
STEP_NAME = "dwh-datamart-update"

# çµ±ä¸€ãƒ­ã‚°ç”¨ã®logger
pipeline_logger = logging.getLogger("pipeline-logger")
if not pipeline_logger.handlers:
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter('%(message)s'))
    pipeline_logger.addHandler(handler)
    pipeline_logger.setLevel(logging.INFO)


def log_pipeline_event(
    action: str,
    status: str = "INFO",
    message: str = "",
    table_name: Optional[str] = None,
    details: Optional[Dict[str, Any]] = None
) -> None:
    """
    çµ±ä¸€å½¢å¼ã§ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ­ã‚°ã‚’å‡ºåŠ›

    Args:
        action: å®Ÿè¡Œä¸­ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆä¾‹: "dwh_update", "datamart_update", "validation"ï¼‰
        status: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼ˆ"INFO", "OK", "ERROR", "WARNING"ï¼‰
        message: ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        table_name: å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        details: è©³ç´°æƒ…å ±ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    """
    severity = "ERROR" if status == "ERROR" else "WARNING" if status == "WARNING" else "INFO"

    log_entry = {
        "severity": severity,
        "message": message,
        "labels": {
            "pipeline_id": PIPELINE_ID,
            "execution_id": EXECUTION_ID,
            "step": STEP_NAME,
            "action": action,
            "status": status
        },
        "jsonPayload": {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "pipeline_id": PIPELINE_ID,
            "execution_id": EXECUTION_ID,
            "step": STEP_NAME,
            "action": action,
            "status": status,
            "message": message
        }
    }

    if table_name:
        log_entry["labels"]["table_name"] = table_name
        log_entry["jsonPayload"]["table_name"] = table_name

    if details:
        log_entry["jsonPayload"]["details"] = details

    if severity == "ERROR":
        pipeline_logger.error(json.dumps(log_entry, ensure_ascii=False))
    elif severity == "WARNING":
        pipeline_logger.warning(json.dumps(log_entry, ensure_ascii=False))
    else:
        pipeline_logger.info(json.dumps(log_entry, ensure_ascii=False))


# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
validation_logger = pipeline_logger

PROJECT_ID = "data-platform-prod-475201"
GCS_BUCKET = "data-platform-landing-prod"
SQL_PREFIX = "sql/split_dwh_dm"

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨­å®š
SOURCE_DATASET = "corporate_data"
BACKUP_DATASET = "corporate_data_bk"

# corporate_dataã®ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¯¾è±¡ï¼‰
CORPORATE_DATA_TABLES = [
    "billing_balance",
    "construction_progress_days_amount",
    "construction_progress_days_final_date",
    "customer_sales_target_and_achievements",
    "department_summary",
    "internal_interest",
    "ledger_income",
    "ledger_loss",
    "management_materials_current_month",
    "ms_allocation_ratio",
    "profit_plan_term",
    "profit_plan_term_fukuoka",
    "profit_plan_term_nagasaki",
    "sales_target_and_achievements",
    "ss_gs_sales_profit",
    "ss_inventory_advance_fukuoka",
    "ss_inventory_advance_nagasaki",
    "ss_inventory_advance_tokyo",
    "stocks",
]

# DWH SQLãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå®Ÿè¡Œé †åºï¼‰
DWH_SQL_FILES = [
    "dwh_sales_actual.sql",
    "dwh_sales_actual_prev_year.sql",
    "dwh_sales_target.sql",
    "operating_expenses.sql",
    "non_operating_income.sql",
    "non_operating_expenses.sql",
    "non_operating_expenses_nagasaki.sql",
    "non_operating_expenses_fukuoka.sql",
    "miscellaneous_loss.sql",
    "head_office_expenses.sql",
    "dwh_recurring_profit_target.sql",
    "operating_expenses_target.sql",
    "operating_income_target.sql",
]

# DataMart SQLãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå®Ÿè¡Œé †åºï¼‰
DATAMART_SQL_FILES = [
    "aggregated_metrics_all_branches.sql",
    "datamart_management_report_tokyo.sql",
    "datamart_management_report_nagasaki.sql",
    "datamart_management_report_fukuoka.sql",
    "datamart_management_report_all.sql",
    "datamart_management_report_all_for_display.sql",
    "cumulative_management_documents_all_period_all.sql",
    "cumulative_management_documents_all_period_all_for_display.sql",
]

# ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã®GCSãƒ‘ã‚¹
TABLE_UNIQUE_KEYS_GCS_PATH = "config/table_unique_keys.yml"


def load_table_unique_keys() -> Dict[str, Dict]:
    """
    GCSã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼å®šç¾©ã‚’èª­ã¿è¾¼ã‚€

    Returns:
        ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’ã‚­ãƒ¼ã€è¨­å®šã‚’å€¤ã¨ã™ã‚‹è¾æ›¸
    """
    try:
        client = storage.Client(project=PROJECT_ID)
        bucket = client.bucket(GCS_BUCKET)
        blob = bucket.blob(TABLE_UNIQUE_KEYS_GCS_PATH)
        yaml_content = blob.download_as_text()
        config = yaml.safe_load(yaml_content)
        return config.get("tables", {})
    except Exception as e:
        print(f"[WARN] ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼å®šç¾©ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        return {}


def get_sql_from_gcs(bucket_name: str, blob_path: str) -> str:
    """GCSã‹ã‚‰SQLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    client = storage.Client(project=PROJECT_ID)
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(blob_path)
    return blob.download_as_text()


def execute_sql(bq_client: bigquery.Client, sql: str, description: str) -> bool:
    """BigQueryã§SQLã‚’å®Ÿè¡Œ"""
    print(f"  å®Ÿè¡Œä¸­: {description}")
    try:
        query_job = bq_client.query(sql)
        query_job.result()  # å®Œäº†ã‚’å¾…æ©Ÿ
        print(f"  âœ“ å®Œäº†: {description}")
        return True
    except Exception as e:
        print(f"  âœ— ã‚¨ãƒ©ãƒ¼: {description}")
        print(f"    {str(e)}")
        return False


# ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è¨­å®š
TABLE_PARTITION_CONFIG = {
    "sales_target_and_achievements": {
        "partition_field": "sales_accounting_period",
        "clustering_fields": ["branch_code"]
    },
    "billing_balance": {
        "partition_field": "sales_month",
        "clustering_fields": ["branch_code"]
    },
    "ledger_income": {
        "partition_field": "slip_date",
        "clustering_fields": ["classification_type"]
    },
    "ledger_loss": {
        "partition_field": "accounting_month",
        "clustering_fields": ["classification_type"]
    },
    "department_summary": {
        "partition_field": "sales_accounting_period",
        "clustering_fields": ["code"]
    },
    "internal_interest": {
        "partition_field": "year_month",
        "clustering_fields": ["branch"]
    },
    "profit_plan_term": {
        "partition_field": "period",
        "clustering_fields": ["item"]
    },
    "profit_plan_term_nagasaki": {
        "partition_field": "period",
        "clustering_fields": ["item"]
    },
    "profit_plan_term_fukuoka": {
        "partition_field": "period",
        "clustering_fields": ["item"]
    },
    "stocks": {
        "partition_field": "year_month",
        "clustering_fields": ["branch"]
    },
    "ms_allocation_ratio": {
        "partition_field": "year_month",
        "clustering_fields": ["branch"]
    },
    "customer_sales_target_and_achievements": {
        "partition_field": "sales_accounting_period",
        "clustering_fields": ["branch_code"]
    },
    "construction_progress_days_amount": {
        "partition_field": "property_period",
        "clustering_fields": ["branch_code"]
    },
    "construction_progress_days_final_date": {
        "partition_field": "final_billing_sales_date",
        "clustering_fields": []
    },
}


def backup_corporate_data(bq_client: bigquery.Client) -> Dict[str, int]:
    """
    corporate_dataã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’corporate_data_bkã«ã‚³ãƒ”ãƒ¼ã—ã€ä»¶æ•°ã‚’è¿”ã™

    ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³è¨­å®šãŒã‚ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã¯ã€DROP â†’ CREATE TABLE ... PARTITION BY ã§
    ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³è¨­å®šã‚’ä¿æŒã—ã¦ã‚³ãƒ”ãƒ¼ã™ã‚‹ã€‚

    Returns:
        ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’ã‚­ãƒ¼ã€ä»¶æ•°ã‚’å€¤ã¨ã™ã‚‹è¾æ›¸
    """
    print("\n" + "=" * 50)
    print("corporate_data â†’ corporate_data_bk ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—é–‹å§‹")
    print("=" * 50)

    row_counts = {}

    for table_name in CORPORATE_DATA_TABLES:
        source_table = f"{PROJECT_ID}.{SOURCE_DATASET}.{table_name}"
        backup_table = f"{PROJECT_ID}.{BACKUP_DATASET}.{table_name}"

        try:
            # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³è¨­å®šã‚’ç¢ºèª
            partition_config = TABLE_PARTITION_CONFIG.get(table_name)

            if partition_config:
                # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³è¨­å®šãŒã‚ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«: DROP â†’ CREATE TABLE ... PARTITION BY
                partition_field = partition_config["partition_field"]
                clustering_fields = partition_config.get("clustering_fields", [])

                # 1. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‰Šé™¤
                drop_sql = f"DROP TABLE IF EXISTS `{backup_table}`"
                bq_client.query(drop_sql).result()

                # 2. ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ä»˜ãã§ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆ
                clustering_clause = ""
                if clustering_fields:
                    clustering_clause = f"CLUSTER BY {', '.join(clustering_fields)}"

                copy_sql = f"""
                CREATE TABLE `{backup_table}`
                PARTITION BY {partition_field}
                {clustering_clause}
                AS SELECT * FROM `{source_table}`
                """
                bq_client.query(copy_sql).result()
            else:
                # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³è¨­å®šãŒãªã„ãƒ†ãƒ¼ãƒ–ãƒ«: å¾“æ¥ã®æ–¹æ³•
                copy_sql = f"""
                CREATE OR REPLACE TABLE `{backup_table}` AS
                SELECT * FROM `{source_table}`
                """
                bq_client.query(copy_sql).result()

            # ä»¶æ•°ã‚’å–å¾—
            count_sql = f"SELECT COUNT(*) as cnt FROM `{source_table}`"
            result = bq_client.query(count_sql).result()
            count = list(result)[0].cnt
            row_counts[table_name] = count

            print(f"  âœ“ {table_name}: {count:,} ä»¶")

        except Exception as e:
            print(f"  âœ— {table_name}: ã‚¨ãƒ©ãƒ¼ - {str(e)}")
            row_counts[table_name] = -1  # ã‚¨ãƒ©ãƒ¼ã‚’ç¤ºã™

    print(f"\nãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†: {len([v for v in row_counts.values() if v >= 0])}/{len(CORPORATE_DATA_TABLES)} ãƒ†ãƒ¼ãƒ–ãƒ«")
    return row_counts


def compare_row_counts(bq_client: bigquery.Client, backup_counts: Dict[str, int]) -> None:
    """
    ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ™‚ã®ä»¶æ•°ã¨ç¾åœ¨ã®ä»¶æ•°ã‚’æ¯”è¼ƒã—ã€ãƒ­ã‚°ã«å‡ºåŠ›

    Args:
        bq_client: BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        backup_counts: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ™‚ã®ä»¶æ•°
    """
    print("\n" + "=" * 50)
    print("ãƒ†ãƒ¼ãƒ–ãƒ«ä»¶æ•°æ¯”è¼ƒï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— vs ç¾åœ¨ï¼‰")
    print("=" * 50)

    comparison_results = []

    for table_name in CORPORATE_DATA_TABLES:
        backup_count = backup_counts.get(table_name, -1)

        try:
            # ç¾åœ¨ã®ä»¶æ•°ã‚’å–å¾—
            source_table = f"{PROJECT_ID}.{SOURCE_DATASET}.{table_name}"
            count_sql = f"SELECT COUNT(*) as cnt FROM `{source_table}`"
            result = bq_client.query(count_sql).result()
            current_count = list(result)[0].cnt

            diff = current_count - backup_count if backup_count >= 0 else None
            diff_str = f"{diff:+,}" if diff is not None else "N/A"

            comparison_results.append({
                "table": table_name,
                "backup_count": backup_count,
                "current_count": current_count,
                "diff": diff
            })

            # å·®åˆ†ãŒã‚ã‚‹å ´åˆã¯ç›®ç«‹ã¤ã‚ˆã†ã«è¡¨ç¤º
            if diff and diff != 0:
                print(f"  ğŸ“Š {table_name}: {backup_count:,} â†’ {current_count:,} ({diff_str})")
            else:
                print(f"  {table_name}: {backup_count:,} â†’ {current_count:,} ({diff_str})")

        except Exception as e:
            print(f"  âœ— {table_name}: ä»¶æ•°å–å¾—ã‚¨ãƒ©ãƒ¼ - {str(e)}")
            comparison_results.append({
                "table": table_name,
                "backup_count": backup_count,
                "current_count": -1,
                "diff": None,
                "error": str(e)
            })

    # æ§‹é€ åŒ–ãƒ­ã‚°ã¨ã—ã¦å‡ºåŠ›
    log_entry = {
        "severity": "INFO",
        "message": "ãƒ†ãƒ¼ãƒ–ãƒ«ä»¶æ•°æ¯”è¼ƒçµæœ",
        "labels": {
            "service": "dwh-datamart-update",
            "operation": "row_count_comparison"
        },
        "jsonPayload": {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "comparison": comparison_results
        }
    }
    validation_logger.info(json.dumps(log_entry, ensure_ascii=False))


def check_duplicates(bq_client: bigquery.Client) -> Dict[str, Any]:
    """
    corporate_dataãƒ†ãƒ¼ãƒ–ãƒ«ã®é‡è¤‡ã‚’ãƒã‚§ãƒƒã‚¯

    Args:
        bq_client: BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

    Returns:
        é‡è¤‡ãƒã‚§ãƒƒã‚¯çµæœã®è¾æ›¸
    """
    print("\n" + "=" * 50)
    print("corporate_data é‡è¤‡ãƒã‚§ãƒƒã‚¯é–‹å§‹")
    print("=" * 50)

    # ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼å®šç¾©ã‚’èª­ã¿è¾¼ã¿
    table_configs = load_table_unique_keys()

    if not table_configs:
        print("  âš ï¸  ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼å®šç¾©ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return {"status": "SKIPPED", "reason": "no_config"}

    duplicate_results = []
    has_duplicates = False

    for table_name in CORPORATE_DATA_TABLES:
        if table_name not in table_configs:
            print(f"  âš ï¸  {table_name}: ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼æœªå®šç¾©ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
            continue

        config = table_configs[table_name]
        unique_keys = config.get("unique_keys", [])

        if not unique_keys:
            print(f"  âš ï¸  {table_name}: ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼ãŒç©ºï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
            continue

        try:
            # ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼ã‚’çµåˆã—ã¦CONCATã§é‡è¤‡ãƒã‚§ãƒƒã‚¯
            table_id = f"{PROJECT_ID}.{SOURCE_DATASET}.{table_name}"

            # ã‚«ãƒ©ãƒ ã®å­˜åœ¨ç¢ºèª
            table = bq_client.get_table(table_id)
            existing_columns = {field.name for field in table.schema}
            valid_keys = [k for k in unique_keys if k in existing_columns]

            if len(valid_keys) != len(unique_keys):
                missing = set(unique_keys) - set(valid_keys)
                print(f"  âš ï¸  {table_name}: ã‚«ãƒ©ãƒ ä¸è¶³ {missing}ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
                continue

            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
            key_concat = ", '-', ".join([f"CAST({k} AS STRING)" for k in valid_keys])
            query = f"""
            SELECT
                COUNT(*) as total_rows,
                COUNT(DISTINCT CONCAT({key_concat})) as unique_keys,
                COUNT(*) - COUNT(DISTINCT CONCAT({key_concat})) as duplicates
            FROM `{table_id}`
            """

            result = bq_client.query(query).result()
            row = list(result)[0]

            total_rows = row.total_rows
            unique_count = row.unique_keys
            duplicates = row.duplicates

            result_entry = {
                "table": table_name,
                "total_rows": total_rows,
                "unique_keys": unique_count,
                "duplicates": duplicates,
                "unique_key_columns": valid_keys
            }
            duplicate_results.append(result_entry)

            if duplicates > 0:
                has_duplicates = True
                print(f"  âŒ {table_name}: {total_rows:,}è¡Œ / ãƒ¦ãƒ‹ãƒ¼ã‚¯{unique_count:,} / é‡è¤‡{duplicates:,}")
            else:
                print(f"  âœ… {table_name}: {total_rows:,}è¡Œ / é‡è¤‡ãªã—")

        except Exception as e:
            print(f"  âœ— {table_name}: ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ - {str(e)}")
            duplicate_results.append({
                "table": table_name,
                "error": str(e)
            })

    # çµæœã‚µãƒãƒªãƒ¼
    tables_with_duplicates = [r for r in duplicate_results if r.get("duplicates", 0) > 0]
    print(f"\né‡è¤‡ãƒã‚§ãƒƒã‚¯å®Œäº†: {len(duplicate_results)}ãƒ†ãƒ¼ãƒ–ãƒ«ä¸­ {len(tables_with_duplicates)}ãƒ†ãƒ¼ãƒ–ãƒ«ã«é‡è¤‡ã‚ã‚Š")

    # æ§‹é€ åŒ–ãƒ­ã‚°ã¨ã—ã¦å‡ºåŠ›
    result = {
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "service": "dwh-datamart-update",
        "validation_type": "duplicate_check",
        "status": "ERROR" if has_duplicates else "OK",
        "tables_checked": len(duplicate_results),
        "tables_with_duplicates": len(tables_with_duplicates),
        "details": duplicate_results
    }

    log_entry = {
        "severity": "ERROR" if has_duplicates else "INFO",
        "message": f"é‡è¤‡ãƒã‚§ãƒƒã‚¯çµæœ: {len(tables_with_duplicates)}ãƒ†ãƒ¼ãƒ–ãƒ«ã«é‡è¤‡ã‚ã‚Š" if has_duplicates else "é‡è¤‡ãƒã‚§ãƒƒã‚¯çµæœ: é‡è¤‡ãªã—",
        "labels": {
            "service": "dwh-datamart-update",
            "validation_type": "duplicate_check",
            "status": result["status"]
        },
        "jsonPayload": result
    }
    validation_logger.info(json.dumps(log_entry, ensure_ascii=False))

    return result


def update_dwh(bq_client: bigquery.Client) -> bool:
    """DWHãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ›´æ–°"""
    print("\n" + "=" * 50)
    print("DWHæ›´æ–°å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
    print("=" * 50)

    log_pipeline_event(
        action="dwh_update",
        status="INFO",
        message="DWHæ›´æ–°å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™",
        details={"total_files": len(DWH_SQL_FILES)}
    )

    success_count = 0
    failed_files = []
    total = len(DWH_SQL_FILES)

    for i, sql_file in enumerate(DWH_SQL_FILES, 1):
        print(f"\n[{i}/{total}] {sql_file}")
        blob_path = f"{SQL_PREFIX}/{sql_file}"
        table_name = sql_file.replace(".sql", "")

        try:
            sql = get_sql_from_gcs(GCS_BUCKET, blob_path)
            if execute_sql(bq_client, sql, sql_file):
                success_count += 1
                log_pipeline_event(
                    action="dwh_update",
                    status="OK",
                    message=f"âœ“ å®Œäº†: {sql_file}",
                    table_name=table_name
                )
            else:
                failed_files.append(sql_file)
                log_pipeline_event(
                    action="dwh_update",
                    status="ERROR",
                    message=f"âœ— SQLå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {sql_file}",
                    table_name=table_name
                )
        except Exception as e:
            print(f"  âœ— SQLãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {blob_path}")
            print(f"    {str(e)}")
            failed_files.append(sql_file)
            log_pipeline_event(
                action="dwh_update",
                status="ERROR",
                message=f"âœ— SQLãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {sql_file}",
                table_name=table_name,
                details={"error": str(e)}
            )

    print(f"\nDWHæ›´æ–°å®Œäº†: {success_count}/{total} æˆåŠŸ")

    log_pipeline_event(
        action="dwh_update",
        status="OK" if success_count == total else "ERROR",
        message=f"DWHæ›´æ–°å®Œäº†: {success_count}/{total} æˆåŠŸ",
        details={
            "success_count": success_count,
            "total": total,
            "failed_files": failed_files
        }
    )

    return success_count == total


def update_datamart(bq_client: bigquery.Client) -> bool:
    """DataMartãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ›´æ–°"""
    print("\n" + "=" * 50)
    print("DataMartæ›´æ–°å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
    print("=" * 50)

    log_pipeline_event(
        action="datamart_update",
        status="INFO",
        message="DataMartæ›´æ–°å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™",
        details={"total_files": len(DATAMART_SQL_FILES)}
    )

    success_count = 0
    failed_files = []
    total = len(DATAMART_SQL_FILES)

    for i, sql_file in enumerate(DATAMART_SQL_FILES, 1):
        print(f"\n[{i}/{total}] {sql_file}")
        blob_path = f"{SQL_PREFIX}/{sql_file}"
        table_name = sql_file.replace(".sql", "")

        try:
            sql = get_sql_from_gcs(GCS_BUCKET, blob_path)
            if execute_sql(bq_client, sql, sql_file):
                success_count += 1
                log_pipeline_event(
                    action="datamart_update",
                    status="OK",
                    message=f"âœ“ å®Œäº†: {sql_file}",
                    table_name=table_name
                )
            else:
                failed_files.append(sql_file)
                log_pipeline_event(
                    action="datamart_update",
                    status="ERROR",
                    message=f"âœ— SQLå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {sql_file}",
                    table_name=table_name
                )
        except Exception as e:
            print(f"  âœ— SQLãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {blob_path}")
            print(f"    {str(e)}")
            failed_files.append(sql_file)
            log_pipeline_event(
                action="datamart_update",
                status="ERROR",
                message=f"âœ— SQLãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {sql_file}",
                table_name=table_name,
                details={"error": str(e)}
            )

    print(f"\nDataMartæ›´æ–°å®Œäº†: {success_count}/{total} æˆåŠŸ")

    log_pipeline_event(
        action="datamart_update",
        status="OK" if success_count == total else "ERROR",
        message=f"DataMartæ›´æ–°å®Œäº†: {success_count}/{total} æˆåŠŸ",
        details={
            "success_count": success_count,
            "total": total,
            "failed_files": failed_files
        }
    )

    return success_count == total


# ============================================================
# ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–¢æ•°
# ============================================================

def log_validation_result(result: Dict[str, Any]) -> None:
    """
    ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœã‚’Cloud Loggingã«å‡ºåŠ›

    æ§‹é€ åŒ–ãƒ­ã‚°ã¨ã—ã¦Cloud Loggingã§æ¤œç´¢ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯èƒ½ã€‚
    """
    log_entry = {
        "severity": "ERROR" if result.get("status") == "ERROR" else "INFO",
        "message": _format_validation_message(result),
        "labels": {
            "service": "datamart-validation",
            "validation_type": result.get("validation_type", "unknown"),
            "status": result.get("status", "unknown")
        },
        "jsonPayload": result
    }

    if result.get("status") == "ERROR":
        validation_logger.error(json.dumps(log_entry, ensure_ascii=False))
    else:
        validation_logger.info(json.dumps(log_entry, ensure_ascii=False))


def _format_validation_message(result: Dict[str, Any]) -> str:
    """ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ•´å½¢"""
    status = result.get("status", "UNKNOWN")
    validation_type = result.get("validation_type", "validation")

    if status == "OK":
        return f"[VALIDATION {status}] DataMart: {validation_type} passed"
    else:
        count = result.get("sonota_non_zero_count", 0)
        return f"[VALIDATION {status}] DataMart: {validation_type} failed ({count} records with ãã®ä»– > 0)"


def validate_sonota_values(bq_client: bigquery.Client) -> Dict[str, Any]:
    """
    secondary_department='ãã®ä»–' ã® value > 0 ã‚’ãƒã‚§ãƒƒã‚¯

    Args:
        bq_client: BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

    Returns:
        æ¤œè¨¼çµæœã®è¾æ›¸
    """
    errors = []

    # ãƒã‚§ãƒƒã‚¯å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«
    table_id = f"{PROJECT_ID}.corporate_data_dm.management_documents_all_period_all"

    query = f"""
    SELECT
        date,
        main_department,
        main_category,
        secondary_category,
        secondary_department,
        value
    FROM `{table_id}`
    WHERE secondary_department = 'ãã®ä»–'
      AND value > 0
    ORDER BY date DESC, main_department, main_category
    LIMIT 20
    """

    try:
        result = bq_client.query(query).result()
        alerts = []
        for row in result:
            alerts.append({
                "date": str(row.date) if row.date else None,
                "main_department": row.main_department,
                "main_category": row.main_category,
                "secondary_category": row.secondary_category,
                "value": float(row.value) if row.value else 0
            })

        sonota_count = len(alerts)

        if sonota_count > 0:
            errors.append({
                "type": "SONOTA_NON_ZERO",
                "message": f"secondary_department='ãã®ä»–' ã§ value > 0 ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒ {sonota_count} ä»¶ã‚ã‚Šã¾ã™",
                "details": {
                    "count": sonota_count,
                    "sample_records": alerts[:10]  # æœ€å¤§10ä»¶ã®ã‚µãƒ³ãƒ—ãƒ«
                }
            })

        return {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "service": "datamart-validation",
            "validation_type": "sonota_check",
            "table": "management_documents_all_period_all",
            "status": "ERROR" if errors else "OK",
            "sonota_non_zero_count": sonota_count,
            "errors": errors
        }

    except Exception as e:
        return {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "service": "datamart-validation",
            "validation_type": "sonota_check",
            "status": "ERROR",
            "errors": [{
                "type": "QUERY_ERROR",
                "message": f"ãã®ä»–ãƒã‚§ãƒƒã‚¯ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}"
            }]
        }


def run_datamart_validation(bq_client: bigquery.Client) -> bool:
    """DataMartãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ"""
    print("\n" + "=" * 50)
    print("DataMartãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã—ã¾ã™")
    print("=" * 50)

    # ãã®ä»–ãƒã‚§ãƒƒã‚¯
    print("\n[1/1] secondary_department='ãã®ä»–' ãƒã‚§ãƒƒã‚¯")
    result = validate_sonota_values(bq_client)
    log_validation_result(result)

    if result.get("status") == "ERROR":
        for error in result.get("errors", []):
            print(f"  âš ï¸  {error.get('message')}")
            if error.get("details", {}).get("sample_records"):
                print("  ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ã‚³ãƒ¼ãƒ‰:")
                for record in error["details"]["sample_records"][:5]:
                    print(f"    - {record.get('date')}: {record.get('main_department')} / "
                          f"{record.get('main_category')} / {record.get('secondary_category')} = {record.get('value')}")
        return False
    else:
        print("  âœ… ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³OK: ãã®ä»–ãƒã‚§ãƒƒã‚¯ passed")
        return True


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    start_time = datetime.utcnow()
    update_type = os.environ.get("UPDATE_TYPE", "all").lower()
    enable_backup = os.environ.get("ENABLE_BACKUP", "true").lower() == "true"

    print(f"æ›´æ–°ã‚¿ã‚¤ãƒ—: {update_type}")
    print(f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {PROJECT_ID}")
    print(f"SQLã‚½ãƒ¼ã‚¹: gs://{GCS_BUCKET}/{SQL_PREFIX}/")
    print(f"ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³: {'æœ‰åŠ¹' if VALIDATION_ENABLED else 'ç„¡åŠ¹'}")
    print(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {'æœ‰åŠ¹' if enable_backup else 'ç„¡åŠ¹'}")
    print(f"å®Ÿè¡ŒID: {EXECUTION_ID}")

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹ãƒ­ã‚°
    log_pipeline_event(
        action="pipeline_start",
        status="INFO",
        message="DWH/DataMartæ›´æ–°ã‚¸ãƒ§ãƒ–ã‚’é–‹å§‹ã—ã¾ã™",
        details={
            "update_type": update_type,
            "validation_enabled": VALIDATION_ENABLED,
            "backup_enabled": enable_backup
        }
    )

    bq_client = bigquery.Client(project=PROJECT_ID)

    dwh_success = True
    datamart_success = True
    validation_success = True
    backup_counts = {}

    # Step 1: corporate_data â†’ corporate_data_bk ã¸ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    if enable_backup:
        backup_counts = backup_corporate_data(bq_client)

    # Step 2: DWHæ›´æ–°
    if update_type in ("dwh", "all"):
        dwh_success = update_dwh(bq_client)

    # Step 3: DataMartæ›´æ–°
    if update_type in ("datamart", "all"):
        datamart_success = update_datamart(bq_client)

        # DataMartæ›´æ–°å¾Œã«ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
        if datamart_success and VALIDATION_ENABLED:
            validation_success = run_datamart_validation(bq_client)

    # Step 4: ä»¶æ•°æ¯”è¼ƒï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒæœ‰åŠ¹ãªå ´åˆï¼‰
    if enable_backup and backup_counts:
        compare_row_counts(bq_client, backup_counts)

    # Step 5: é‡è¤‡ãƒã‚§ãƒƒã‚¯
    if VALIDATION_ENABLED:
        duplicate_result = check_duplicates(bq_client)
        if duplicate_result.get("status") == "ERROR":
            print("\nâš ï¸  é‡è¤‡ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸï¼ˆè­¦å‘Šã®ã¿ï¼‰")

    # å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—
    end_time = datetime.utcnow()
    duration_seconds = (end_time - start_time).total_seconds()

    print("\n" + "=" * 50)
    if dwh_success and datamart_success:
        if not validation_success:
            print("æ›´æ–°å‡¦ç†ã¯å®Œäº†ã—ã¾ã—ãŸãŒã€ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã§è­¦å‘ŠãŒã‚ã‚Šã¾ã™")
            print("=" * 50)

            # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†ãƒ­ã‚°ï¼ˆè­¦å‘Šã‚ã‚Šï¼‰
            log_pipeline_event(
                action="pipeline_complete",
                status="WARNING",
                message="æ›´æ–°å‡¦ç†ã¯å®Œäº†ã—ã¾ã—ãŸãŒã€ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã§è­¦å‘ŠãŒã‚ã‚Šã¾ã™",
                details={
                    "dwh_success": dwh_success,
                    "datamart_success": datamart_success,
                    "validation_success": validation_success,
                    "duration_seconds": duration_seconds
                }
            )
            sys.exit(0)
        else:
            print("å…¨ã¦ã®æ›´æ–°å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
            print("=" * 50)

            # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†ãƒ­ã‚°ï¼ˆæˆåŠŸï¼‰
            log_pipeline_event(
                action="pipeline_complete",
                status="OK",
                message="å…¨ã¦ã®æ›´æ–°å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ",
                details={
                    "dwh_success": dwh_success,
                    "datamart_success": datamart_success,
                    "validation_success": validation_success,
                    "duration_seconds": duration_seconds
                }
            )
            sys.exit(0)
    else:
        print("ä¸€éƒ¨ã®æ›´æ–°å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        print("=" * 50)

        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†ãƒ­ã‚°ï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰
        log_pipeline_event(
            action="pipeline_complete",
            status="ERROR",
            message="ä¸€éƒ¨ã®æ›´æ–°å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
            details={
                "dwh_success": dwh_success,
                "datamart_success": datamart_success,
                "validation_success": validation_success,
                "duration_seconds": duration_seconds
            }
        )
        sys.exit(1)


if __name__ == "__main__":
    main()
