#!/usr/bin/env python3
"""
gcs-to-bq Cloud Run Service
GCSä¸Šã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’CSVã«å¤‰æ›ã—ã€BigQueryã«ãƒ­ãƒ¼ãƒ‰

ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½:
- ã‚«ãƒ©ãƒ ä¸æ•´åˆãƒã‚§ãƒƒã‚¯
- ãƒ¬ã‚³ãƒ¼ãƒ‰0ä»¶ãƒã‚§ãƒƒã‚¯
- é‡è¤‡ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯

çµæœã¯Google Cloud Loggingã«å‡ºåŠ›ã•ã‚Œã€å¾Œã‹ã‚‰Slackç­‰ã«é€£æºå¯èƒ½ã€‚
"""

import os
import io
import json
import base64
import traceback
import logging
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, Optional, Any, List
from flask import Flask, request, jsonify
from google.cloud import storage
from google.cloud import bigquery
from google.cloud.exceptions import GoogleCloudError

# ============================================================
# ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
# ============================================================

# ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æœ‰åŠ¹åŒ–ãƒ•ãƒ©ã‚°
VALIDATION_ENABLED = os.environ.get("VALIDATION_ENABLED", "true").lower() == "true"

# ãƒ†ãƒ¼ãƒ–ãƒ«ã”ã¨ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼å®šç¾©ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
UNIQUE_KEYS_CONFIG = {
    "sales_target_and_achievements": ["sales_accounting_period", "branch_code", "department_code", "staff_code"],
    "billing_balance": ["sales_month", "branch_code", "branch_name"],
    "ledger_income": ["slip_date", "slip_number", "line_number"],
    "ledger_loss": ["accounting_month", "slip_number", "line_number"],
    "department_summary": ["sales_accounting_period", "code"],
    "internal_interest": ["year_month", "branch", "category"],
    "profit_plan_term": ["period", "item"],
    "profit_plan_term_nagasaki": ["period", "item"],
    "profit_plan_term_fukuoka": ["period", "item"],
    "stocks": ["year_month", "branch", "category"],
    "ms_allocation_ratio": ["year_month", "branch", "department", "category"],
    "customer_sales_target_and_achievements": ["sales_accounting_period", "branch_code", "customer_code"],
    "construction_progress_days_amount": ["property_period", "branch_code", "staff_code", "property_number", "customer_code", "contract_date"],
    "construction_progress_days_final_date": ["final_billing_sales_date", "property_number", "property_data_classification"],
}

# ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°ç”¨ã®logger
validation_logger = logging.getLogger("gcs-to-bq-validation")
if not validation_logger.handlers:
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter('%(message)s'))
    validation_logger.addHandler(handler)
    validation_logger.setLevel(logging.INFO)

# ç’°å¢ƒå¤‰æ•°
PROJECT_ID = os.environ.get("GCP_PROJECT", "data-platform-prod-475201")
DATASET_ID = "corporate_data"
LANDING_BUCKET = os.environ.get("LANDING_BUCKET", "data-platform-landing-prod")
COLUMNS_PATH = "google-drive/config/columns"
MAPPING_FILE = "google-drive/config/mapping/excel_mapping.csv"
MONETARY_SCALE_FILE = "google-drive/config/mapping/monetary_scale_conversion.csv"
ZERO_DATE_FILE = "google-drive/config/mapping/zero_date_to_null.csv"

# ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©
TABLE_CONFIG = {
    "sales_target_and_achievements": {
        "partition_field": "sales_accounting_period",
        "clustering_fields": ["branch_code"]
    },
    "billing_balance": {
        "partition_field": "sales_month",
        "clustering_fields": ["branch_code"]
    },
    "ledger_income": {
        "partition_field": "slip_date",
        "clustering_fields": ["classification_type"]
    },
    "department_summary": {
        "partition_field": "sales_accounting_period",
        "clustering_fields": ["code"]
    },
    "internal_interest": {
        "partition_field": "year_month",
        "clustering_fields": ["branch"]
    },
    "profit_plan_term": {
        "partition_field": "period",
        "clustering_fields": ["item"]
    },
    "ledger_loss": {
        "partition_field": "slip_date",
        "clustering_fields": ["classification_type"]
    },
    "customer_sales_target_and_achievements": {
        "partition_field": "sales_accounting_period",
        "clustering_fields": ["branch_code", "customer_code"]
    },
    "construction_progress_days_amount": {
        "partition_field": "property_period",
        "clustering_fields": ["branch_code", "property_number"]
    },
    "construction_progress_days_final_date": {
        "partition_field": "final_billing_sales_date",
        "clustering_fields": ["property_number"]
    }
}

# ============================================================
# ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–¢æ•°
# ============================================================

def log_validation_result(result: Dict[str, Any]) -> None:
    """
    ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœã‚’Cloud Loggingã«å‡ºåŠ›

    æ§‹é€ åŒ–ãƒ­ã‚°ã¨ã—ã¦Cloud Loggingã§æ¤œç´¢ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯èƒ½ã€‚
    ãƒ­ã‚°ã¯ä»¥ä¸‹ã®ãƒ©ãƒ™ãƒ«ã§ãƒ•ã‚£ãƒ«ã‚¿å¯èƒ½:
    - labels.service: "gcs-to-bq"
    - labels.validation_type: "column_check" / "empty_check" / "duplicate_check"
    - labels.status: "OK" / "ERROR"
    """
    log_entry = {
        "severity": "ERROR" if result.get("status") == "ERROR" else "INFO",
        "message": _format_validation_message(result),
        "labels": {
            "service": "gcs-to-bq",
            "table_name": result.get("table_name", "unknown"),
            "validation_type": result.get("validation_type", "unknown"),
            "status": result.get("status", "unknown")
        },
        "jsonPayload": result
    }

    if result.get("status") == "ERROR":
        validation_logger.error(json.dumps(log_entry, ensure_ascii=False))
    elif result.get("warnings"):
        validation_logger.warning(json.dumps(log_entry, ensure_ascii=False))
    else:
        validation_logger.info(json.dumps(log_entry, ensure_ascii=False))


def _format_validation_message(result: Dict[str, Any]) -> str:
    """ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ•´å½¢"""
    status = result.get("status", "UNKNOWN")
    table_name = result.get("table_name", "unknown")
    validation_type = result.get("validation_type", "validation")

    if status == "OK":
        row_count = result.get("row_count", result.get("total_rows", 0))
        return f"[VALIDATION {status}] {table_name}: {validation_type} passed ({row_count} rows)"
    else:
        error_count = len(result.get("errors", []))
        return f"[VALIDATION {status}] {table_name}: {validation_type} failed ({error_count} errors)"


def validate_columns_and_rows(
    df: pd.DataFrame,
    table_name: str,
    expected_columns: List[str],
    source_file: str = None
) -> Dict[str, Any]:
    """
    ã‚«ãƒ©ãƒ ä¸æ•´åˆã¨ãƒ¬ã‚³ãƒ¼ãƒ‰0ä»¶ã‚’ãƒã‚§ãƒƒã‚¯

    Args:
        df: æ¤œè¨¼å¯¾è±¡ã®DataFrameï¼ˆæ—¥æœ¬èªã‚«ãƒ©ãƒ åï¼‰
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
        expected_columns: æœŸå¾…ã•ã‚Œã‚‹ã‚«ãƒ©ãƒ åãƒªã‚¹ãƒˆï¼ˆæ—¥æœ¬èªï¼‰
        source_file: ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å

    Returns:
        æ¤œè¨¼çµæœã®è¾æ›¸
    """
    errors = []
    warnings = []

    # ã‚«ãƒ©ãƒ åã®æ”¹è¡Œã‚’é™¤å»ã—ã¦ã‹ã‚‰æ¯”è¼ƒ
    actual_columns = [str(col).replace('\n', '') for col in df.columns]

    # 1. ã‚«ãƒ©ãƒ ä¸æ•´åˆãƒã‚§ãƒƒã‚¯
    missing_columns = [col for col in expected_columns if col not in actual_columns]
    extra_columns = [col for col in actual_columns if col not in expected_columns]

    if missing_columns:
        errors.append({
            "type": "MISSING_COLUMNS",
            "message": f"æœŸå¾…ã•ã‚Œã‚‹ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {missing_columns}",
            "details": {"missing": missing_columns}
        })

    if extra_columns:
        warnings.append({
            "type": "EXTRA_COLUMNS",
            "message": f"å®šç¾©å¤–ã®ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã™: {extra_columns}",
            "details": {"extra": extra_columns}
        })

    # 2. ãƒ¬ã‚³ãƒ¼ãƒ‰0ä»¶ãƒã‚§ãƒƒã‚¯
    row_count = len(df)
    if row_count == 0:
        errors.append({
            "type": "EMPTY_DATA",
            "message": "ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™"
        })

    result = {
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "service": "gcs-to-bq",
        "validation_type": "column_and_row_check",
        "table_name": table_name,
        "source_file": source_file,
        "status": "ERROR" if errors else "OK",
        "row_count": row_count,
        "column_count": len(actual_columns),
        "expected_column_count": len(expected_columns),
        "errors": errors,
        "warnings": warnings
    }

    return result


def validate_duplicates_in_bq(
    bq_client: bigquery.Client,
    table_name: str
) -> Dict[str, Any]:
    """
    BigQueryãƒ†ãƒ¼ãƒ–ãƒ«ã®é‡è¤‡ã‚’ãƒã‚§ãƒƒã‚¯

    Args:
        bq_client: BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å

    Returns:
        æ¤œè¨¼çµæœã®è¾æ›¸
    """
    errors = []

    # ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼å®šç¾©ã‚’å–å¾—
    unique_keys = UNIQUE_KEYS_CONFIG.get(table_name)
    if not unique_keys:
        return {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "service": "gcs-to-bq",
            "validation_type": "duplicate_check",
            "table_name": table_name,
            "status": "SKIPPED",
            "message": "ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        }

    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
    key_cols = ", ".join(unique_keys)

    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚¯ã‚¨ãƒª
    query = f"""
    SELECT {key_cols}, COUNT(*) as duplicate_count
    FROM `{table_id}`
    GROUP BY {key_cols}
    HAVING COUNT(*) > 1
    LIMIT 10
    """

    try:
        result = bq_client.query(query).result()
        duplicates = [dict(row) for row in result]
        duplicate_count = len(duplicates)

        if duplicate_count > 0:
            errors.append({
                "type": "DUPLICATE_RECORDS",
                "message": f"é‡è¤‡ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒå­˜åœ¨ã—ã¾ã™ï¼ˆã‚µãƒ³ãƒ—ãƒ«: {duplicate_count}ä»¶ï¼‰",
                "details": {
                    "unique_keys": unique_keys,
                    "sample_duplicates": duplicates
                }
            })

        return {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "service": "gcs-to-bq",
            "validation_type": "duplicate_check",
            "table_name": table_name,
            "status": "ERROR" if errors else "OK",
            "unique_keys": unique_keys,
            "duplicate_sample_count": duplicate_count,
            "errors": errors
        }

    except Exception as e:
        return {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "service": "gcs-to-bq",
            "validation_type": "duplicate_check",
            "table_name": table_name,
            "status": "ERROR",
            "errors": [{
                "type": "QUERY_ERROR",
                "message": f"é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}"
            }]
        }


# ============================================================
# Excel â†’ CSV å¤‰æ›å‡¦ç†
# ============================================================

def load_column_mapping(table_name: str) -> Dict[str, Dict[str, str]]:
    """ã‚«ãƒ©ãƒ ãƒãƒƒãƒ”ãƒ³ã‚°å®šç¾©ã‚’èª­ã¿è¾¼ã¿"""
    storage_client = storage.Client()
    bucket = storage_client.bucket(LANDING_BUCKET)

    mapping_blob = bucket.blob(f"{COLUMNS_PATH}/{table_name}.csv")
    if not mapping_blob.exists():
        print(f"âš ï¸  ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {table_name}.csv")
        return {}

    csv_data = mapping_blob.download_as_bytes()
    df = pd.read_csv(io.BytesIO(csv_data))

    mapping = {}
    for _, row in df.iterrows():
        mapping[row['jp_name']] = {
            'en_name': row['en_name'],
            'type': row['type']
        }
    return mapping

def convert_date_format(value: Any, date_type: str, column_name: str = '') -> str:
    """æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å¤‰æ›"""
    if pd.isna(value) or value == '' or value is None:
        return ''

    # ç„¡åŠ¹ãªæ—¥ä»˜å€¤ã‚’ç©ºæ–‡å­—åˆ—ã«å¤‰æ›
    value_str = str(value)
    if value_str in ['0000/00/00', '0000-00-00', '00/00/0000', '0', 'NaT']:
        return ''

    # èª¤ã£ãŸå½¢å¼ã®æ—¥ä»˜ã‚’ä¿®æ­£ (ä¾‹: "0223/03/25" â†’ "2023/03/25")
    import re
    match = re.match(r'^0(\d{3})/(\d{2})/(\d{2})$', value_str)
    if match:
        value_str = f"2{match.group(1)}/{match.group(2)}/{match.group(3)}"

    # æ•°å€¤ã®å ´åˆã®å‡¦ç†
    if isinstance(value, (int, float)):
        # Excelã®ã‚·ãƒªã‚¢ãƒ«æ—¥ä»˜
        if value > 0 and value < 100000:
            try:
                excel_base = pd.Timestamp('1899-12-30')
                dt = excel_base + pd.Timedelta(days=int(value))
                if date_type == 'DATETIME':
                    return dt.strftime('%Y-%m-%d %H:%M:%S')
                else:
                    return dt.strftime('%Y-%m-%d')
            except:
                pass

        # Unixã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆãƒŠãƒç§’ï¼‰
        elif value > 1e15:
            try:
                dt = pd.to_datetime(value, unit='ns')
                if date_type == 'DATETIME':
                    return dt.strftime('%Y-%m-%d %H:%M:%S')
                else:
                    return dt.strftime('%Y-%m-%d')
            except:
                pass


    # ã€Œå¹´æœˆã€ç‰¹æ®Šå‡¦ç†ï¼ˆä¾‹: "2025å¹´9æœˆ" â†’ "2025-09-01"ï¼‰
    if 'å¹´' in value_str and 'æœˆ' in value_str:
        import re
        try:
            match = re.match(r'(\d{4})å¹´(\d{1,2})æœˆ', value_str)
            if match:
                year = match.group(1)
                month = match.group(2).zfill(2)
                return f"{year}-{month}-01"
        except:
            pass

    # DATEå‹ã®å‡¦ç†
    if date_type == 'DATE':
        # YYYY/MMå½¢å¼ã®å ´åˆã€1æ—¥ã‚’è¿½åŠ 
        import re
        if re.match(r'^\d{4}/\d{1,2}$', value_str):
            try:
                dt = pd.to_datetime(value_str + '/01', format='%Y/%m/%d')
                return dt.strftime('%Y-%m-%d')
            except:
                pass

        try:
            dt = pd.to_datetime(value_str)
            return dt.strftime('%Y-%m-%d')
        except:
            print(f"âš ï¸  æ—¥ä»˜å¤‰æ›ã‚¨ãƒ©ãƒ¼: {value_str}")
            return value_str

    # DATETIMEå‹ã®å‡¦ç†
    elif date_type == 'DATETIME':
        try:
            dt = pd.to_datetime(value_str)
            return dt.strftime('%Y-%m-%d %H:%M:%S')
        except:
            print(f"âš ï¸  æ—¥æ™‚å¤‰æ›ã‚¨ãƒ©ãƒ¼: {value_str}")
            return value_str

    return value_str

def apply_data_type_conversion(df: pd.DataFrame, column_mapping: Dict) -> pd.DataFrame:
    """ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›ã‚’é©ç”¨"""
    df = df.copy()

    for col in df.columns:
        if col not in column_mapping:
            continue

        data_type = column_mapping[col]['type']

        # DATE/DATETIMEå‹
        if data_type in ['DATE', 'DATETIME']:
            if pd.api.types.is_datetime64_any_dtype(df[col]):
                if data_type == 'DATE':
                    df[col] = pd.to_datetime(df[col]).dt.strftime('%Y-%m-%d')
                else:
                    df[col] = pd.to_datetime(df[col]).dt.strftime('%Y-%m-%d %H:%M:%S')
            else:
                df[col] = df[col].apply(lambda x: convert_date_format(x, data_type, col))

        # INT64å‹
        elif data_type == 'INT64':
            df[col] = pd.to_numeric(df[col], errors='coerce')
            df[col] = df[col].astype('Int64')

        # NUMERICå‹
        elif data_type == 'NUMERIC':
            df[col] = pd.to_numeric(df[col], errors='coerce')

        # STRINGå‹
        elif data_type == 'STRING':
            df[col] = df[col].fillna('')
            df[col] = df[col].astype(str)
            df[col] = df[col].replace('nan', '')

    return df

def rename_columns(df: pd.DataFrame, column_mapping: Dict) -> pd.DataFrame:
    """ã‚«ãƒ©ãƒ åã‚’æ—¥æœ¬èªã‹ã‚‰è‹±èªã«å¤‰æ›"""
    rename_dict = {}

    for jp_col in df.columns:
        if jp_col in column_mapping:
            en_col = column_mapping[jp_col]['en_name']
            rename_dict[jp_col] = en_col
        else:
            print(f"âš ï¸  ãƒãƒƒãƒ”ãƒ³ã‚°æœªå®šç¾©ã®ã‚«ãƒ©ãƒ : {jp_col}")
            rename_dict[jp_col] = jp_col

    return df.rename(columns=rename_dict)

def load_monetary_scale_config(storage_client: storage.Client) -> pd.DataFrame:
    """é‡‘é¡å˜ä½å¤‰æ›è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
    try:
        bucket = storage_client.bucket(LANDING_BUCKET)
        blob = bucket.blob(MONETARY_SCALE_FILE)

        if not blob.exists():
            print(f"âš ï¸  é‡‘é¡å¤‰æ›è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {MONETARY_SCALE_FILE}")
            return pd.DataFrame()

        csv_data = blob.download_as_bytes()
        df = pd.read_csv(io.BytesIO(csv_data))
        return df
    except Exception as e:
        print(f"âš ï¸  é‡‘é¡å¤‰æ›è¨­å®šã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return pd.DataFrame()

def apply_monetary_scale_conversion(
    df: pd.DataFrame,
    table_name: str,
    storage_client: storage.Client
) -> pd.DataFrame:
    """
    é‡‘é¡å˜ä½å¤‰æ›ã‚’é©ç”¨

    Args:
        df: å¤‰æ›å¯¾è±¡ã®DataFrameï¼ˆè‹±èªã‚«ãƒ©ãƒ åã«å¤‰æ›æ¸ˆã¿ï¼‰
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
        storage_client: Storage Client

    Returns:
        å¤‰æ›å¾Œã®DataFrame
    """
    try:
        # é‡‘é¡å¤‰æ›è¨­å®šã‚’èª­ã¿è¾¼ã¿
        config_df = load_monetary_scale_config(storage_client)

        if config_df.empty:
            return df

        # å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¨­å®šã‚’å–å¾—
        target_config = config_df[config_df['file_name'] == table_name]

        if target_config.empty:
            print(f"   é‡‘é¡å¤‰æ›è¨­å®šãªã—: {table_name}")
            return df

        df = df.copy()

        for _, config in target_config.iterrows():
            condition_col = config['condition_column_name']
            condition_values = eval(config['condition_column_value'])  # ãƒªã‚¹ãƒˆæ–‡å­—åˆ—ã‚’è©•ä¾¡
            object_columns = eval(config['object_column_name'])  # ãƒªã‚¹ãƒˆæ–‡å­—åˆ—ã‚’è©•ä¾¡
            convert_value = float(config['convert_value'])

            # æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹è¡Œã‚’ãƒ•ã‚£ãƒ«ã‚¿
            if condition_col not in df.columns:
                print(f"âš ï¸  æ¡ä»¶ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {condition_col}")
                continue

            mask = df[condition_col].isin(condition_values)

            # å¯¾è±¡ã‚«ãƒ©ãƒ ã‚’å¤‰æ›
            for col in object_columns:
                if col in df.columns:
                    # æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹è¡Œã®ã¿å¤‰æ›
                    df.loc[mask, col] = pd.to_numeric(df.loc[mask, col], errors='coerce') * convert_value
                    print(f"   ğŸ’° {col} ã‚’{convert_value}å€ã«å¤‰æ›ï¼ˆæ¡ä»¶: {condition_col} in {condition_values}ï¼‰")
                else:
                    print(f"âš ï¸  å¤‰æ›å¯¾è±¡ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {col}")

        return df

    except Exception as e:
        print(f"âš ï¸  é‡‘é¡å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return df


def load_zero_date_config(storage_client: storage.Client) -> pd.DataFrame:
    """GCSã‹ã‚‰ã‚¼ãƒ­æ—¥ä»˜å¤‰æ›è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
    try:
        bucket = storage_client.bucket(LANDING_BUCKET)
        blob = bucket.blob(ZERO_DATE_FILE)

        if not blob.exists():
            print(f"âš ï¸  ã‚¼ãƒ­æ—¥ä»˜å¤‰æ›è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: gs://{LANDING_BUCKET}/{ZERO_DATE_FILE}")
            return pd.DataFrame()

        csv_data = blob.download_as_bytes()
        df = pd.read_csv(io.BytesIO(csv_data))
        return df
    except Exception as e:
        print(f"âš ï¸  ã‚¼ãƒ­æ—¥ä»˜å¤‰æ›è¨­å®šã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return pd.DataFrame()


def apply_zero_date_to_null_conversion(
    df: pd.DataFrame,
    table_name: str,
    storage_client: storage.Client
) -> pd.DataFrame:
    """
    ã‚¼ãƒ­æ—¥ä»˜ï¼ˆ0000/00/00ï¼‰ã‚’nullã«å¤‰æ›

    Args:
        df: å¤‰æ›å¯¾è±¡ã®DataFrameï¼ˆè‹±èªã‚«ãƒ©ãƒ åã«å¤‰æ›æ¸ˆã¿ï¼‰
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
        storage_client: Storage Client

    Returns:
        å¤‰æ›å¾Œã®DataFrame
    """
    try:
        # ã‚¼ãƒ­æ—¥ä»˜å¤‰æ›è¨­å®šã‚’èª­ã¿è¾¼ã¿
        config_df = load_zero_date_config(storage_client)

        if config_df.empty:
            return df

        # å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¨­å®šã‚’å–å¾—
        target_config = config_df[config_df['file_name'] == table_name]

        if target_config.empty:
            return df

        df = df.copy()

        # ã‚¼ãƒ­æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ§˜ã€…ãªå½¢å¼ã«å¯¾å¿œï¼‰
        zero_date_patterns = [
            '0000/00/00',
            '0000-00-00',
            '0000/0/0',
            '0000-0-0',
        ]

        for _, config in target_config.iterrows():
            column_name = config['condition_column_name']

            if column_name not in df.columns:
                print(f"âš ï¸  å¯¾è±¡ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {column_name}")
                continue

            # å¤‰æ›å‰ã®nullä»¥å¤–ã®ä»¶æ•°ã‚’è¨˜éŒ²
            non_null_before = df[column_name].notna().sum()

            # ã‚¼ãƒ­æ—¥ä»˜ã‚’nullã«å¤‰æ›
            for pattern in zero_date_patterns:
                mask = df[column_name].astype(str).str.strip() == pattern
                if mask.any():
                    df.loc[mask, column_name] = None

            # å¤‰æ›å¾Œã®nullä»¥å¤–ã®ä»¶æ•°
            non_null_after = df[column_name].notna().sum()
            converted_count = non_null_before - non_null_after

            if converted_count > 0:
                print(f"   ğŸ”„ {column_name}: {converted_count}ä»¶ã®ã‚¼ãƒ­æ—¥ä»˜ã‚’nullã«å¤‰æ›")

        return df

    except Exception as e:
        print(f"âš ï¸  ã‚¼ãƒ­æ—¥ä»˜å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return df


def transform_excel_to_csv(
    storage_client: storage.Client,
    table_name: str,
    yyyymm: str
) -> bool:
    """Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§CSVã«å¤‰æ›"""
    try:
        print(f"\nğŸ“„ å‡¦ç†ä¸­: {table_name}")

        bucket = storage_client.bucket(LANDING_BUCKET)

        # google-drive/raw/ ã‹ã‚‰èª­ã¿è¾¼ã¿
        raw_path = f"google-drive/raw/{yyyymm}/{table_name}.xlsx"
        raw_blob = bucket.blob(raw_path)

        if not raw_blob.exists():
            print(f"âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: gs://{LANDING_BUCKET}/{raw_path}")
            return False

        # ã‚«ãƒ©ãƒ ãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿
        column_mapping = load_column_mapping(table_name)
        if not column_mapping:
            print(f"âŒ ã‚«ãƒ©ãƒ ãƒãƒƒãƒ”ãƒ³ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {table_name}")
            return False

        # Excelãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        excel_bytes = raw_blob.download_as_bytes()

        # profit_plan_termã®å ´åˆã¯ã€Œæ±äº¬æ”¯åº—ç›®æ¨™103æœŸã€ã‚·ãƒ¼ãƒˆã®ã¿ã‚’èª­ã¿è¾¼ã‚€
        if table_name == "profit_plan_term":
            df = pd.read_excel(io.BytesIO(excel_bytes), sheet_name='æ±äº¬æ”¯åº—ç›®æ¨™103æœŸ')
            print(f"   ã‚·ãƒ¼ãƒˆæŒ‡å®š: æ±äº¬æ”¯åº—ç›®æ¨™103æœŸ")
        else:
            df = pd.read_excel(io.BytesIO(excel_bytes))

        # ã‚«ãƒ©ãƒ åã®æ”¹è¡Œã‚’é™¤å»
        df.columns = [col.replace('\n', '') if isinstance(col, str) else col for col in df.columns]

        print(f"   ãƒ‡ãƒ¼ã‚¿: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")

        # ============================================================
        # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³: ã‚«ãƒ©ãƒ ä¸æ•´åˆãƒ»ãƒ¬ã‚³ãƒ¼ãƒ‰0ä»¶ãƒã‚§ãƒƒã‚¯
        # ============================================================
        if VALIDATION_ENABLED:
            expected_columns = list(column_mapping.keys())
            validation_result = validate_columns_and_rows(
                df=df,
                table_name=table_name,
                expected_columns=expected_columns,
                source_file=raw_path
            )
            log_validation_result(validation_result)

            # ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹å ´åˆã¯è­¦å‘Šã‚’å‡ºã™ãŒå‡¦ç†ã¯ç¶šè¡Œ
            if validation_result.get("status") == "ERROR":
                for error in validation_result.get("errors", []):
                    print(f"   âš ï¸  ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼: {error.get('message')}")
            else:
                print(f"   âœ… ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³OK: ã‚«ãƒ©ãƒ ãƒ»ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ãƒã‚§ãƒƒã‚¯ passed")

        # æ—¥æœ¬èªã‚«ãƒ©ãƒ åã‚’è‹±èªã«å¤‰æ›ï¼ˆå‹å¤‰æ›å‰ï¼‰
        jp_column_mapping = {jp: info for jp, info in column_mapping.items()}

        # æ—¥ä»˜åˆ—ã®äº‹å‰å‡¦ç†
        for jp_col, info in jp_column_mapping.items():
            if jp_col in df.columns and info['type'] in ['DATE', 'DATETIME']:
                if pd.api.types.is_datetime64_any_dtype(df[jp_col]):
                    if info['type'] == 'DATE':
                        df[jp_col] = df[jp_col].dt.strftime('%Y-%m-%d')
                    else:
                        df[jp_col] = df[jp_col].dt.strftime('%Y-%m-%d %H:%M:%S')

        # ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›
        df = apply_data_type_conversion(df, jp_column_mapping)

        # ã‚«ãƒ©ãƒ åå¤‰æ›
        df = rename_columns(df, jp_column_mapping)

        # é‡‘é¡å˜ä½å¤‰æ›ï¼ˆã‚«ãƒ©ãƒ åå¤‰æ›å¾Œã«å®Ÿè¡Œï¼‰
        df = apply_monetary_scale_conversion(df, table_name, storage_client)

        # ã‚¼ãƒ­æ—¥ä»˜ã‚’nullã«å¤‰æ›ï¼ˆé‡‘é¡å¤‰æ›å¾Œã«å®Ÿè¡Œï¼‰
        df = apply_zero_date_to_null_conversion(df, table_name, storage_client)

        # CSVå‡ºåŠ›
        csv_buffer = io.BytesIO()
        df.to_csv(csv_buffer, index=False, encoding='utf-8')
        csv_buffer.seek(0)

        # google-drive/proceed/ ã«ä¿å­˜
        proceed_path = f"google-drive/proceed/{yyyymm}/{table_name}.csv"
        proceed_blob = bucket.blob(proceed_path)
        proceed_blob.upload_from_file(csv_buffer, content_type='text/csv')

        print(f"   å‡ºåŠ›: gs://{LANDING_BUCKET}/{proceed_path}")
        print(f"âœ… å¤‰æ›å®Œäº†: {table_name}")

        return True

    except Exception as e:
        print(f"âŒ å¤‰æ›ã‚¨ãƒ©ãƒ¼ ({table_name}): {e}")
        traceback.print_exc()
        return False

# ============================================================
# BigQuery ãƒ­ãƒ¼ãƒ‰å‡¦ç†
# ============================================================

def load_table_name_mapping(storage_client: storage.Client) -> Dict[str, str]:
    """ãƒ†ãƒ¼ãƒ–ãƒ«åãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ—¥æœ¬èªâ†’è‹±èªï¼‰ã‚’èª­ã¿è¾¼ã¿"""
    try:
        bucket = storage_client.bucket(LANDING_BUCKET)
        blob = bucket.blob(MAPPING_FILE)

        if not blob.exists():
            print(f"âš ï¸  ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {MAPPING_FILE}")
            return {}

        csv_data = blob.download_as_bytes()
        df = pd.read_csv(io.BytesIO(csv_data))

        mapping = {}
        for _, row in df.iterrows():
            en_name = row['en_name']
            jp_name = row['jp_name'].replace('.xlsx', '')
            mapping[en_name] = jp_name

        return mapping
    except Exception as e:
        print(f"âš ï¸  ãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {}

def load_column_descriptions(storage_client: storage.Client, table_name: str) -> Dict[str, str]:
    """ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚’èª­ã¿è¾¼ã¿"""
    try:
        bucket = storage_client.bucket(LANDING_BUCKET)
        blob = bucket.blob(f"{COLUMNS_PATH}/{table_name}.csv")

        if not blob.exists():
            print(f"âš ï¸  ã‚«ãƒ©ãƒ å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {table_name}.csv")
            return {}

        csv_data = blob.download_as_bytes()
        df = pd.read_csv(io.BytesIO(csv_data))

        descriptions = {}
        for _, row in df.iterrows():
            en_name = row['en_name']
            description = row['description']
            descriptions[en_name] = description

        return descriptions
    except Exception as e:
        print(f"âš ï¸  ã‚«ãƒ©ãƒ èª¬æ˜èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {}

def update_table_and_column_descriptions(
    bq_client: bigquery.Client,
    storage_client: storage.Client,
    table_name: str
) -> bool:
    """ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚’æ›´æ–°"""
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"

    try:
        table = bq_client.get_table(table_id)

        # ãƒ†ãƒ¼ãƒ–ãƒ«åãƒãƒƒãƒ”ãƒ³ã‚°ã‚’èª­ã¿è¾¼ã¿
        table_mapping = load_table_name_mapping(storage_client)
        if table_name in table_mapping:
            table.description = table_mapping[table_name]
            print(f"   ğŸ“ ãƒ†ãƒ¼ãƒ–ãƒ«èª¬æ˜ã‚’è¨­å®š: {table_mapping[table_name]}")

        # ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚’èª­ã¿è¾¼ã¿
        column_descriptions = load_column_descriptions(storage_client, table_name)

        # æ—¢å­˜ã®ã‚¹ã‚­ãƒ¼ãƒã‚’å–å¾—ã—ã€èª¬æ˜ã‚’è¿½åŠ 
        new_schema = []
        for field in table.schema:
            description = column_descriptions.get(field.name, field.description)
            new_field = bigquery.SchemaField(
                name=field.name,
                field_type=field.field_type,
                mode=field.mode,
                description=description,
                fields=field.fields
            )
            new_schema.append(new_field)

        table.schema = new_schema

        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ›´æ–°
        table = bq_client.update_table(table, ["description", "schema"])
        print(f"   âœ… {len(column_descriptions)}å€‹ã®ã‚«ãƒ©ãƒ èª¬æ˜ã‚’è¨­å®š")

        return True

    except Exception as e:
        print(f"   âš ï¸  èª¬æ˜ã®æ›´æ–°ã«å¤±æ•—: {e}")
        return False

def delete_partition_data(
    bq_client: bigquery.Client,
    table_name: str,
    yyyymm: str = None  # æœªä½¿ç”¨ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰
) -> bool:
    """2024å¹´9æœˆä»¥é™ã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’å…¨ã¦å‰Šé™¤ï¼ˆå†ªç­‰æ€§ä¿è¨¼ï¼‰"""
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
    partition_field = TABLE_CONFIG[table_name]["partition_field"]

    # æœŸé¦–: 2024å¹´9æœˆ1æ—¥ä»¥é™ã‚’å…¨ã¦å‰Šé™¤
    start_date = "2024-09-01"

    # slip_date ã‚„ final_billing_sales_date ãªã©ã€æœˆã®1æ—¥ä»¥å¤–ã®æ—¥ä»˜ãŒå…¥ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯
    # æ—¥ä»˜æ¯”è¼ƒã§å¯¾å¿œ
    tables_with_non_first_day_dates = [
        "ledger_income",
        "ledger_loss",
        "construction_progress_days_final_date"
    ]

    if table_name in tables_with_non_first_day_dates:
        delete_query = f"""
        DELETE FROM `{table_id}`
        WHERE {partition_field} >= '{start_date}'
        """
    else:
        delete_query = f"""
        DELETE FROM `{table_id}`
        WHERE {partition_field} >= '{start_date}'
        """

    try:
        print(f"   ğŸ—‘ï¸  æ—¢å­˜ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ä¸­: {start_date}ä»¥é™")
        query_job = bq_client.query(delete_query)
        query_job.result()

        if query_job.num_dml_affected_rows:
            print(f"      å‰Šé™¤: {query_job.num_dml_affected_rows} è¡Œ")
        else:
            print(f"      å‰Šé™¤å¯¾è±¡ãªã—")

        return True

    except Exception as e:
        print(f"   âš ï¸  å‰Šé™¤å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—: {e}")
        return True

def load_csv_to_bigquery(
    bq_client: bigquery.Client,
    table_name: str,
    yyyymm: str
) -> bool:
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’BigQueryã«ãƒ­ãƒ¼ãƒ‰"""
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
    gcs_uri = f"gs://{LANDING_BUCKET}/google-drive/proceed/{yyyymm}/{table_name}.csv"

    try:
        job_config = bigquery.LoadJobConfig(
            source_format=bigquery.SourceFormat.CSV,
            skip_leading_rows=1,
            autodetect=False,
            write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
            schema_update_options=[
                bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION,
            ],
            allow_quoted_newlines=True,
            allow_jagged_rows=False,
            ignore_unknown_values=False,
            max_bad_records=0,
        )

        load_job = bq_client.load_table_from_uri(
            gcs_uri,
            table_id,
            job_config=job_config
        )

        print(f"   â³ ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {table_name} (Job ID: {load_job.job_id})")

        load_job.result(timeout=300)

        destination_table = bq_client.get_table(table_id)
        print(f"   âœ… ãƒ­ãƒ¼ãƒ‰å®Œäº†: {load_job.output_rows} è¡Œã‚’è¿½åŠ ")
        print(f"      ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {destination_table.num_rows:,} è¡Œ")

        return True

    except GoogleCloudError as e:
        print(f"   âŒ ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        if hasattr(e, 'errors') and e.errors:
            for error in e.errors:
                print(f"      è©³ç´°: {error}")
        return False
    except Exception as e:
        print(f"   âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        return False

# ============================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# ============================================================

# æœŸé¦–ï¼ˆãƒ‡ãƒ¼ã‚¿é–‹å§‹æ—¥ï¼‰
FISCAL_START_YYYYMM = "202409"  # 2024å¹´9æœˆ

def get_available_months_from_gcs(storage_client: storage.Client) -> list:
    """GCSã®google-drive/proceed/ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªå¹´æœˆãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆ2024/9ä»¥é™ï¼‰"""
    bucket = storage_client.bucket(LANDING_BUCKET)
    blobs = bucket.list_blobs(prefix="google-drive/proceed/")

    months = set()
    for blob in blobs:
        # google-drive/proceed/202409/xxx.csv ã®ã‚ˆã†ãªå½¢å¼ã‹ã‚‰yyyymmã‚’æŠ½å‡º
        parts = blob.name.split("/")
        if len(parts) >= 3 and parts[2].isdigit() and len(parts[2]) == 6:
            yyyymm = parts[2]
            # 2024/9ä»¥é™ã®ã¿å¯¾è±¡
            if yyyymm >= FISCAL_START_YYYYMM:
                months.add(yyyymm)

    return sorted(list(months))

# ============================================================
# Flask ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
# ============================================================

app = Flask(__name__)

@app.route("/transform", methods=["POST"])
def transform_endpoint():
    """
    Excel â†’ CSV å¤‰æ›ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

    ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¾‹:
    {
        "yyyymm": "202509",
        "tables": ["sales_target_and_achievements", "billing_balance"]
    }

    ç©ºã®å ´åˆã¯å…¨ãƒ†ãƒ¼ãƒ–ãƒ«å‡¦ç†
    """
    try:
        payload = request.get_json(force=True, silent=True) or {}
        yyyymm = payload.get("yyyymm")
        tables = payload.get("tables", list(TABLE_CONFIG.keys()))

        if not yyyymm:
            return jsonify({"error": "yyyymm is required"}), 400

        print("=" * 60)
        print(f"raw/ â†’ proceed/ å¤‰æ›å‡¦ç†")
        print(f"å¯¾è±¡å¹´æœˆ: {yyyymm}")
        print("=" * 60)

        storage_client = storage.Client()

        success_count = 0
        error_count = 0
        results = []

        for table_name in tables:
            if transform_excel_to_csv(storage_client, table_name, yyyymm):
                success_count += 1
                results.append({"table": table_name, "status": "success"})
            else:
                error_count += 1
                results.append({"table": table_name, "status": "error"})

        print("=" * 60)
        print(f"å‡¦ç†å®Œäº†: æˆåŠŸ {success_count} / ã‚¨ãƒ©ãƒ¼ {error_count}")
        print("=" * 60)

        return jsonify({
            "status": "completed",
            "yyyymm": yyyymm,
            "success": success_count,
            "error": error_count,
            "results": results
        }), 200

    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route("/load", methods=["POST"])
def load_endpoint():
    """
    CSV â†’ BigQuery ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

    ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¾‹:
    {
        "yyyymm": "202509",  # çœç•¥æ™‚ã¯2024/9ä»¥é™ã®å…¨å¹´æœˆã‚’å‡¦ç†
        "tables": ["sales_target_and_achievements"],
        "replace": true
    }

    æ³¨æ„: å†ªç­‰æ€§ã‚’ä¿è¨¼ã™ã‚‹ãŸã‚ã€2024/9ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã¯å…¨ã¦å‰Šé™¤ã•ã‚Œã¦ã‹ã‚‰è¿½åŠ ã•ã‚Œã¾ã™ã€‚
    """
    try:
        payload = request.get_json(force=True, silent=True) or {}
        yyyymm = payload.get("yyyymm")  # çœç•¥å¯èƒ½
        tables = payload.get("tables", list(TABLE_CONFIG.keys()))

        bq_client = bigquery.Client(project=PROJECT_ID)
        storage_client = storage.Client()

        # å¯¾è±¡å¹´æœˆãƒªã‚¹ãƒˆã‚’æ±ºå®š
        if yyyymm:
            # ç‰¹å®šæœˆãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã§ã‚‚ã€2024/9ä»¥é™ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
            target_months = get_available_months_from_gcs(storage_client)
            print(f"æŒ‡å®šæœˆ: {yyyymm}ï¼ˆãŸã ã—2024/9ä»¥é™ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ï¼‰")
        else:
            # çœç•¥æ™‚ã¯2024/9ä»¥é™ã®å…¨å¹´æœˆ
            target_months = get_available_months_from_gcs(storage_client)

        print("=" * 60)
        print(f"proceed/ â†’ BigQuery ãƒ­ãƒ¼ãƒ‰å‡¦ç†")
        print(f"å¯¾è±¡å¹´æœˆ: {', '.join(target_months)}")
        print(f"ãƒ¢ãƒ¼ãƒ‰: REPLACEï¼ˆ2024/9ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã‚’å…¨ã¦å‰Šé™¤ã—ã¦å†ãƒ­ãƒ¼ãƒ‰ï¼‰")
        print("=" * 60)

        success_count = 0
        error_count = 0
        results = []

        for table_name in tables:
            print(f"\nğŸ“Š å‡¦ç†ä¸­: {table_name}")

            # 2024/9ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã‚’å…¨ã¦å‰Šé™¤ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ã”ã¨ã«1å›ã ã‘ï¼‰
            delete_partition_data(bq_client, table_name)

            # å…¨å¹´æœˆã®CSVã‚’ãƒ­ãƒ¼ãƒ‰
            table_success = True
            for month in target_months:
                if not load_csv_to_bigquery(bq_client, table_name, month):
                    table_success = False

            if table_success:
                # ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚’æ›´æ–°
                update_table_and_column_descriptions(bq_client, storage_client, table_name)

                # ============================================================
                # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³: é‡è¤‡ãƒã‚§ãƒƒã‚¯
                # ============================================================
                if VALIDATION_ENABLED:
                    dup_result = validate_duplicates_in_bq(bq_client, table_name)
                    log_validation_result(dup_result)

                    if dup_result.get("status") == "ERROR":
                        for error in dup_result.get("errors", []):
                            print(f"   âš ï¸  é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {error.get('message')}")
                    elif dup_result.get("status") == "SKIPPED":
                        print(f"   â­ï¸  é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚¹ã‚­ãƒƒãƒ—: ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼æœªå®šç¾©")
                    else:
                        print(f"   âœ… ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³OK: é‡è¤‡ãƒã‚§ãƒƒã‚¯ passed")

                success_count += 1
                results.append({"table": table_name, "status": "success"})
            else:
                error_count += 1
                results.append({"table": table_name, "status": "error"})

        print("\n" + "=" * 60)
        print(f"å‡¦ç†å®Œäº†: æˆåŠŸ {success_count} / ã‚¨ãƒ©ãƒ¼ {error_count}")
        print("=" * 60)

        return jsonify({
            "status": "completed",
            "target_months": target_months,
            "success": success_count,
            "error": error_count,
            "results": results
        }), 200

    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route("/pubsub", methods=["POST"])
def pubsub_endpoint():
    """
    Pub/Sub ãƒˆãƒªã‚¬ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    drive-to-gcså®Œäº†å¾Œã«è‡ªå‹•å®Ÿè¡Œã•ã‚Œã‚‹

    ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä¾‹:
    {
        "message": {
            "data": "eyJ5eXl5bW0iOiAiMjAyNTA5In0="  # base64: {"yyyymm": "202509"}
        }
    }

    æ³¨æ„: å†ªç­‰æ€§ã‚’ä¿è¨¼ã™ã‚‹ãŸã‚ã€2024/9ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã¯å…¨ã¦å‰Šé™¤ã•ã‚Œã¦ã‹ã‚‰è¿½åŠ ã•ã‚Œã¾ã™ã€‚
    """
    try:
        envelope = request.get_json(force=True, silent=True) or {}
        msg = envelope.get("message", {})
        data_b64 = msg.get("data")

        if not data_b64:
            return ("Bad Request: no message.data", 400)

        payload = json.loads(base64.b64decode(data_b64).decode("utf-8"))
        yyyymm = payload.get("yyyymm")
        tables = payload.get("tables", list(TABLE_CONFIG.keys()))

        if not yyyymm:
            return jsonify({"error": "yyyymm is required"}), 400

        print(f"[INFO] Pub/Sub triggered: yyyymm={yyyymm}")

        # Transformå®Ÿè¡Œï¼ˆæŒ‡å®šæœˆã®ã¿ï¼‰
        print("=" * 60)
        print(f"raw/ â†’ proceed/ å¤‰æ›å‡¦ç†")
        print(f"å¯¾è±¡å¹´æœˆ: {yyyymm}")
        print("=" * 60)

        storage_client = storage.Client()
        transform_success = 0
        transform_error = 0

        for table_name in tables:
            if transform_excel_to_csv(storage_client, table_name, yyyymm):
                transform_success += 1
            else:
                transform_error += 1

        print(f"å¤‰æ›å®Œäº†: æˆåŠŸ {transform_success} / ã‚¨ãƒ©ãƒ¼ {transform_error}")

        # Loadå®Ÿè¡Œï¼ˆ2024/9ä»¥é™ã®å…¨å¹´æœˆï¼‰
        target_months = get_available_months_from_gcs(storage_client)

        print("=" * 60)
        print(f"proceed/ â†’ BigQuery ãƒ­ãƒ¼ãƒ‰å‡¦ç†")
        print(f"å¯¾è±¡å¹´æœˆ: {', '.join(target_months)}")
        print(f"ãƒ¢ãƒ¼ãƒ‰: REPLACEï¼ˆ2024/9ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã‚’å…¨ã¦å‰Šé™¤ã—ã¦å†ãƒ­ãƒ¼ãƒ‰ï¼‰")
        print("=" * 60)

        bq_client = bigquery.Client(project=PROJECT_ID)
        load_success = 0
        load_error = 0

        for table_name in tables:
            print(f"\nğŸ“Š å‡¦ç†ä¸­: {table_name}")

            # 2024/9ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã‚’å…¨ã¦å‰Šé™¤ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ã”ã¨ã«1å›ã ã‘ï¼‰
            delete_partition_data(bq_client, table_name)

            # å…¨å¹´æœˆã®CSVã‚’ãƒ­ãƒ¼ãƒ‰
            table_success = True
            for month in target_months:
                if not load_csv_to_bigquery(bq_client, table_name, month):
                    table_success = False

            if table_success:
                update_table_and_column_descriptions(bq_client, storage_client, table_name)

                # ============================================================
                # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³: é‡è¤‡ãƒã‚§ãƒƒã‚¯
                # ============================================================
                if VALIDATION_ENABLED:
                    dup_result = validate_duplicates_in_bq(bq_client, table_name)
                    log_validation_result(dup_result)

                    if dup_result.get("status") == "ERROR":
                        for error in dup_result.get("errors", []):
                            print(f"   âš ï¸  é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {error.get('message')}")
                    elif dup_result.get("status") == "SKIPPED":
                        print(f"   â­ï¸  é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚¹ã‚­ãƒƒãƒ—: ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼æœªå®šç¾©")
                    else:
                        print(f"   âœ… ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³OK: é‡è¤‡ãƒã‚§ãƒƒã‚¯ passed")

                load_success += 1
            else:
                load_error += 1

        print(f"ãƒ­ãƒ¼ãƒ‰å®Œäº†: æˆåŠŸ {load_success} / ã‚¨ãƒ©ãƒ¼ {load_error}")

        return ("", 204)

    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route("/", methods=["GET"])
def health():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return "gcs-to-bq service is running", 200

if __name__ == "__main__":
    port = int(os.getenv("PORT", "8080"))
    app.run(host="0.0.0.0", port=port, debug=False)
