#!/usr/bin/env python3
"""
gcs-to-bq Cloud Run Service
GCSä¸Šã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’CSVã«å¤‰æ›ã—ã€BigQueryã«ãƒ­ãƒ¼ãƒ‰
"""

import os
import io
import json
import base64
import traceback
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, Optional, Any
from flask import Flask, request, jsonify
from google.cloud import storage
from google.cloud import bigquery
from google.cloud.exceptions import GoogleCloudError

# ç’°å¢ƒå¤‰æ•°
PROJECT_ID = os.environ.get("GCP_PROJECT", "data-platform-prod-475201")
DATASET_ID = "corporate_data"
LANDING_BUCKET = os.environ.get("LANDING_BUCKET", "data-platform-landing-prod")
COLUMNS_PATH = "config/columns"
MAPPING_FILE = "config/mapping/excel_mapping.csv"
MONETARY_SCALE_FILE = "config/mapping/monetary_scale_conversion.csv"

# ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©
TABLE_CONFIG = {
    "sales_target_and_achievements": {
        "partition_field": "sales_accounting_period",
        "clustering_fields": ["branch_code"]
    },
    "billing_balance": {
        "partition_field": "sales_month",
        "clustering_fields": ["branch_code"]
    },
    "ledger_income": {
        "partition_field": "slip_date",
        "clustering_fields": ["classification_type"]
    },
    "department_summary": {
        "partition_field": "sales_accounting_period",
        "clustering_fields": ["code"]
    },
    "internal_interest": {
        "partition_field": "year_month",
        "clustering_fields": ["branch"]
    },
    "profit_plan_term": {
        "partition_field": "period",
        "clustering_fields": ["item"]
    },
    "ledger_loss": {
        "partition_field": "slip_date",
        "clustering_fields": ["classification_type"]
    },
    "customer_sales_target_and_achievements": {
        "partition_field": "sales_accounting_period",
        "clustering_fields": ["branch_code", "customer_code"]
    },
    "construction_progress_days_amount": {
        "partition_field": "property_period",
        "clustering_fields": ["branch_code", "property_number"]
    },
    "construction_progress_days_final_date": {
        "partition_field": "final_billing_sales_date",
        "clustering_fields": ["property_number"]
    }
}

# ============================================================
# Excel â†’ CSV å¤‰æ›å‡¦ç†
# ============================================================

def load_column_mapping(table_name: str) -> Dict[str, Dict[str, str]]:
    """ã‚«ãƒ©ãƒ ãƒãƒƒãƒ”ãƒ³ã‚°å®šç¾©ã‚’èª­ã¿è¾¼ã¿"""
    storage_client = storage.Client()
    bucket = storage_client.bucket(LANDING_BUCKET)

    mapping_blob = bucket.blob(f"{COLUMNS_PATH}/{table_name}.csv")
    if not mapping_blob.exists():
        print(f"âš ï¸  ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {table_name}.csv")
        return {}

    csv_data = mapping_blob.download_as_bytes()
    df = pd.read_csv(io.BytesIO(csv_data))

    mapping = {}
    for _, row in df.iterrows():
        mapping[row['jp_name']] = {
            'en_name': row['en_name'],
            'type': row['type']
        }
    return mapping

def convert_date_format(value: Any, date_type: str, column_name: str = '') -> str:
    """æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å¤‰æ›"""
    if pd.isna(value) or value == '' or value is None:
        return ''

    # ç„¡åŠ¹ãªæ—¥ä»˜å€¤ã‚’ç©ºæ–‡å­—åˆ—ã«å¤‰æ›
    value_str = str(value)
    if value_str in ['0000/00/00', '0000-00-00', '00/00/0000', '0', 'NaT']:
        return ''

    # èª¤ã£ãŸå½¢å¼ã®æ—¥ä»˜ã‚’ä¿®æ­£ (ä¾‹: "0223/03/25" â†’ "2023/03/25")
    import re
    match = re.match(r'^0(\d{3})/(\d{2})/(\d{2})$', value_str)
    if match:
        value_str = f"2{match.group(1)}/{match.group(2)}/{match.group(3)}"

    # æ•°å€¤ã®å ´åˆã®å‡¦ç†
    if isinstance(value, (int, float)):
        # Excelã®ã‚·ãƒªã‚¢ãƒ«æ—¥ä»˜
        if value > 0 and value < 100000:
            try:
                excel_base = pd.Timestamp('1899-12-30')
                dt = excel_base + pd.Timedelta(days=int(value))
                if date_type == 'DATETIME':
                    return dt.strftime('%Y-%m-%d %H:%M:%S')
                else:
                    return dt.strftime('%Y-%m-%d')
            except:
                pass

        # Unixã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆãƒŠãƒç§’ï¼‰
        elif value > 1e15:
            try:
                dt = pd.to_datetime(value, unit='ns')
                if date_type == 'DATETIME':
                    return dt.strftime('%Y-%m-%d %H:%M:%S')
                else:
                    return dt.strftime('%Y-%m-%d')
            except:
                pass


    # ã€Œå¹´æœˆã€ç‰¹æ®Šå‡¦ç†ï¼ˆä¾‹: "2025å¹´9æœˆ" â†’ "2025-09-01"ï¼‰
    if 'å¹´' in value_str and 'æœˆ' in value_str:
        import re
        try:
            match = re.match(r'(\d{4})å¹´(\d{1,2})æœˆ', value_str)
            if match:
                year = match.group(1)
                month = match.group(2).zfill(2)
                return f"{year}-{month}-01"
        except:
            pass

    # DATEå‹ã®å‡¦ç†
    if date_type == 'DATE':
        # YYYY/MMå½¢å¼ã®å ´åˆã€1æ—¥ã‚’è¿½åŠ 
        import re
        if re.match(r'^\d{4}/\d{1,2}$', value_str):
            try:
                dt = pd.to_datetime(value_str + '/01', format='%Y/%m/%d')
                return dt.strftime('%Y-%m-%d')
            except:
                pass

        try:
            dt = pd.to_datetime(value_str)
            return dt.strftime('%Y-%m-%d')
        except:
            print(f"âš ï¸  æ—¥ä»˜å¤‰æ›ã‚¨ãƒ©ãƒ¼: {value_str}")
            return value_str

    # DATETIMEå‹ã®å‡¦ç†
    elif date_type == 'DATETIME':
        try:
            dt = pd.to_datetime(value_str)
            return dt.strftime('%Y-%m-%d %H:%M:%S')
        except:
            print(f"âš ï¸  æ—¥æ™‚å¤‰æ›ã‚¨ãƒ©ãƒ¼: {value_str}")
            return value_str

    return value_str

def apply_data_type_conversion(df: pd.DataFrame, column_mapping: Dict) -> pd.DataFrame:
    """ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›ã‚’é©ç”¨"""
    df = df.copy()

    for col in df.columns:
        if col not in column_mapping:
            continue

        data_type = column_mapping[col]['type']

        # DATE/DATETIMEå‹
        if data_type in ['DATE', 'DATETIME']:
            if pd.api.types.is_datetime64_any_dtype(df[col]):
                if data_type == 'DATE':
                    df[col] = pd.to_datetime(df[col]).dt.strftime('%Y-%m-%d')
                else:
                    df[col] = pd.to_datetime(df[col]).dt.strftime('%Y-%m-%d %H:%M:%S')
            else:
                df[col] = df[col].apply(lambda x: convert_date_format(x, data_type, col))

        # INT64å‹
        elif data_type == 'INT64':
            df[col] = pd.to_numeric(df[col], errors='coerce')
            df[col] = df[col].astype('Int64')

        # NUMERICå‹
        elif data_type == 'NUMERIC':
            df[col] = pd.to_numeric(df[col], errors='coerce')

        # STRINGå‹
        elif data_type == 'STRING':
            df[col] = df[col].fillna('')
            df[col] = df[col].astype(str)
            df[col] = df[col].replace('nan', '')

    return df

def rename_columns(df: pd.DataFrame, column_mapping: Dict) -> pd.DataFrame:
    """ã‚«ãƒ©ãƒ åã‚’æ—¥æœ¬èªã‹ã‚‰è‹±èªã«å¤‰æ›"""
    rename_dict = {}

    for jp_col in df.columns:
        if jp_col in column_mapping:
            en_col = column_mapping[jp_col]['en_name']
            rename_dict[jp_col] = en_col
        else:
            print(f"âš ï¸  ãƒãƒƒãƒ”ãƒ³ã‚°æœªå®šç¾©ã®ã‚«ãƒ©ãƒ : {jp_col}")
            rename_dict[jp_col] = jp_col

    return df.rename(columns=rename_dict)

def load_monetary_scale_config(storage_client: storage.Client) -> pd.DataFrame:
    """é‡‘é¡å˜ä½å¤‰æ›è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
    try:
        bucket = storage_client.bucket(LANDING_BUCKET)
        blob = bucket.blob(MONETARY_SCALE_FILE)

        if not blob.exists():
            print(f"âš ï¸  é‡‘é¡å¤‰æ›è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {MONETARY_SCALE_FILE}")
            return pd.DataFrame()

        csv_data = blob.download_as_bytes()
        df = pd.read_csv(io.BytesIO(csv_data))
        return df
    except Exception as e:
        print(f"âš ï¸  é‡‘é¡å¤‰æ›è¨­å®šã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return pd.DataFrame()

def apply_monetary_scale_conversion(
    df: pd.DataFrame,
    table_name: str,
    storage_client: storage.Client
) -> pd.DataFrame:
    """
    é‡‘é¡å˜ä½å¤‰æ›ã‚’é©ç”¨

    Args:
        df: å¤‰æ›å¯¾è±¡ã®DataFrameï¼ˆè‹±èªã‚«ãƒ©ãƒ åã«å¤‰æ›æ¸ˆã¿ï¼‰
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
        storage_client: Storage Client

    Returns:
        å¤‰æ›å¾Œã®DataFrame
    """
    try:
        # é‡‘é¡å¤‰æ›è¨­å®šã‚’èª­ã¿è¾¼ã¿
        config_df = load_monetary_scale_config(storage_client)

        if config_df.empty:
            return df

        # å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¨­å®šã‚’å–å¾—
        target_config = config_df[config_df['file_name'] == table_name]

        if target_config.empty:
            print(f"   é‡‘é¡å¤‰æ›è¨­å®šãªã—: {table_name}")
            return df

        df = df.copy()

        for _, config in target_config.iterrows():
            condition_col = config['condition_column_name']
            condition_values = eval(config['condition_column_value'])  # ãƒªã‚¹ãƒˆæ–‡å­—åˆ—ã‚’è©•ä¾¡
            object_columns = eval(config['object_column_name'])  # ãƒªã‚¹ãƒˆæ–‡å­—åˆ—ã‚’è©•ä¾¡
            convert_value = float(config['convert_value'])

            # æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹è¡Œã‚’ãƒ•ã‚£ãƒ«ã‚¿
            if condition_col not in df.columns:
                print(f"âš ï¸  æ¡ä»¶ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {condition_col}")
                continue

            mask = df[condition_col].isin(condition_values)

            # å¯¾è±¡ã‚«ãƒ©ãƒ ã‚’å¤‰æ›
            for col in object_columns:
                if col in df.columns:
                    # æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹è¡Œã®ã¿å¤‰æ›
                    df.loc[mask, col] = pd.to_numeric(df.loc[mask, col], errors='coerce') * convert_value
                    print(f"   ğŸ’° {col} ã‚’{convert_value}å€ã«å¤‰æ›ï¼ˆæ¡ä»¶: {condition_col} in {condition_values}ï¼‰")
                else:
                    print(f"âš ï¸  å¤‰æ›å¯¾è±¡ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {col}")

        return df

    except Exception as e:
        print(f"âš ï¸  é‡‘é¡å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return df

def transform_excel_to_csv(
    storage_client: storage.Client,
    table_name: str,
    yyyymm: str
) -> bool:
    """Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§CSVã«å¤‰æ›"""
    try:
        print(f"\nğŸ“„ å‡¦ç†ä¸­: {table_name}")

        bucket = storage_client.bucket(LANDING_BUCKET)

        # raw/ ã‹ã‚‰èª­ã¿è¾¼ã¿
        raw_path = f"raw/{yyyymm}/{table_name}.xlsx"
        raw_blob = bucket.blob(raw_path)

        if not raw_blob.exists():
            print(f"âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: gs://{LANDING_BUCKET}/{raw_path}")
            return False

        # ã‚«ãƒ©ãƒ ãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿
        column_mapping = load_column_mapping(table_name)
        if not column_mapping:
            print(f"âŒ ã‚«ãƒ©ãƒ ãƒãƒƒãƒ”ãƒ³ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {table_name}")
            return False

        # Excelãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        excel_bytes = raw_blob.download_as_bytes()

        # profit_plan_termã®å ´åˆã¯ã€Œæ±äº¬æ”¯åº—ç›®æ¨™103æœŸã€ã‚·ãƒ¼ãƒˆã®ã¿ã‚’èª­ã¿è¾¼ã‚€
        if table_name == "profit_plan_term":
            df = pd.read_excel(io.BytesIO(excel_bytes), sheet_name='æ±äº¬æ”¯åº—ç›®æ¨™103æœŸ')
            print(f"   ã‚·ãƒ¼ãƒˆæŒ‡å®š: æ±äº¬æ”¯åº—ç›®æ¨™103æœŸ")
        else:
            df = pd.read_excel(io.BytesIO(excel_bytes))

        # ã‚«ãƒ©ãƒ åã®æ”¹è¡Œã‚’é™¤å»
        df.columns = [col.replace('\n', '') if isinstance(col, str) else col for col in df.columns]

        print(f"   ãƒ‡ãƒ¼ã‚¿: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")

        # æ—¥æœ¬èªã‚«ãƒ©ãƒ åã‚’è‹±èªã«å¤‰æ›ï¼ˆå‹å¤‰æ›å‰ï¼‰
        jp_column_mapping = {jp: info for jp, info in column_mapping.items()}

        # æ—¥ä»˜åˆ—ã®äº‹å‰å‡¦ç†
        for jp_col, info in jp_column_mapping.items():
            if jp_col in df.columns and info['type'] in ['DATE', 'DATETIME']:
                if pd.api.types.is_datetime64_any_dtype(df[jp_col]):
                    if info['type'] == 'DATE':
                        df[jp_col] = df[jp_col].dt.strftime('%Y-%m-%d')
                    else:
                        df[jp_col] = df[jp_col].dt.strftime('%Y-%m-%d %H:%M:%S')

        # ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›
        df = apply_data_type_conversion(df, jp_column_mapping)

        # ã‚«ãƒ©ãƒ åå¤‰æ›
        df = rename_columns(df, jp_column_mapping)

        # é‡‘é¡å˜ä½å¤‰æ›ï¼ˆã‚«ãƒ©ãƒ åå¤‰æ›å¾Œã«å®Ÿè¡Œï¼‰
        df = apply_monetary_scale_conversion(df, table_name, storage_client)

        # CSVå‡ºåŠ›
        csv_buffer = io.BytesIO()
        df.to_csv(csv_buffer, index=False, encoding='utf-8')
        csv_buffer.seek(0)

        # proceed/ ã«ä¿å­˜
        proceed_path = f"proceed/{yyyymm}/{table_name}.csv"
        proceed_blob = bucket.blob(proceed_path)
        proceed_blob.upload_from_file(csv_buffer, content_type='text/csv')

        print(f"   å‡ºåŠ›: gs://{LANDING_BUCKET}/{proceed_path}")
        print(f"âœ… å¤‰æ›å®Œäº†: {table_name}")

        return True

    except Exception as e:
        print(f"âŒ å¤‰æ›ã‚¨ãƒ©ãƒ¼ ({table_name}): {e}")
        traceback.print_exc()
        return False

# ============================================================
# BigQuery ãƒ­ãƒ¼ãƒ‰å‡¦ç†
# ============================================================

def load_table_name_mapping(storage_client: storage.Client) -> Dict[str, str]:
    """ãƒ†ãƒ¼ãƒ–ãƒ«åãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ—¥æœ¬èªâ†’è‹±èªï¼‰ã‚’èª­ã¿è¾¼ã¿"""
    try:
        bucket = storage_client.bucket(LANDING_BUCKET)
        blob = bucket.blob(MAPPING_FILE)

        if not blob.exists():
            print(f"âš ï¸  ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {MAPPING_FILE}")
            return {}

        csv_data = blob.download_as_bytes()
        df = pd.read_csv(io.BytesIO(csv_data))

        mapping = {}
        for _, row in df.iterrows():
            en_name = row['en_name']
            jp_name = row['jp_name'].replace('.xlsx', '')
            mapping[en_name] = jp_name

        return mapping
    except Exception as e:
        print(f"âš ï¸  ãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {}

def load_column_descriptions(storage_client: storage.Client, table_name: str) -> Dict[str, str]:
    """ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚’èª­ã¿è¾¼ã¿"""
    try:
        bucket = storage_client.bucket(LANDING_BUCKET)
        blob = bucket.blob(f"{COLUMNS_PATH}/{table_name}.csv")

        if not blob.exists():
            print(f"âš ï¸  ã‚«ãƒ©ãƒ å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {table_name}.csv")
            return {}

        csv_data = blob.download_as_bytes()
        df = pd.read_csv(io.BytesIO(csv_data))

        descriptions = {}
        for _, row in df.iterrows():
            en_name = row['en_name']
            description = row['description']
            descriptions[en_name] = description

        return descriptions
    except Exception as e:
        print(f"âš ï¸  ã‚«ãƒ©ãƒ èª¬æ˜èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {}

def update_table_and_column_descriptions(
    bq_client: bigquery.Client,
    storage_client: storage.Client,
    table_name: str
) -> bool:
    """ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚’æ›´æ–°"""
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"

    try:
        table = bq_client.get_table(table_id)

        # ãƒ†ãƒ¼ãƒ–ãƒ«åãƒãƒƒãƒ”ãƒ³ã‚°ã‚’èª­ã¿è¾¼ã¿
        table_mapping = load_table_name_mapping(storage_client)
        if table_name in table_mapping:
            table.description = table_mapping[table_name]
            print(f"   ğŸ“ ãƒ†ãƒ¼ãƒ–ãƒ«èª¬æ˜ã‚’è¨­å®š: {table_mapping[table_name]}")

        # ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚’èª­ã¿è¾¼ã¿
        column_descriptions = load_column_descriptions(storage_client, table_name)

        # æ—¢å­˜ã®ã‚¹ã‚­ãƒ¼ãƒã‚’å–å¾—ã—ã€èª¬æ˜ã‚’è¿½åŠ 
        new_schema = []
        for field in table.schema:
            description = column_descriptions.get(field.name, field.description)
            new_field = bigquery.SchemaField(
                name=field.name,
                field_type=field.field_type,
                mode=field.mode,
                description=description,
                fields=field.fields
            )
            new_schema.append(new_field)

        table.schema = new_schema

        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ›´æ–°
        table = bq_client.update_table(table, ["description", "schema"])
        print(f"   âœ… {len(column_descriptions)}å€‹ã®ã‚«ãƒ©ãƒ èª¬æ˜ã‚’è¨­å®š")

        return True

    except Exception as e:
        print(f"   âš ï¸  èª¬æ˜ã®æ›´æ–°ã«å¤±æ•—: {e}")
        return False

def delete_partition_data(
    bq_client: bigquery.Client,
    table_name: str,
    yyyymm: str
) -> bool:
    """æŒ‡å®šæœˆã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤"""
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
    partition_field = TABLE_CONFIG[table_name]["partition_field"]

    year = yyyymm[:4]
    month = yyyymm[4:6]

    if table_name in ["ledger_income", "ledger_loss"]:
        delete_query = f"""
        DELETE FROM `{table_id}`
        WHERE DATE({partition_field}) = '{year}-{month}-01'
        """
    else:
        delete_query = f"""
        DELETE FROM `{table_id}`
        WHERE {partition_field} = '{year}-{month}-01'
        """

    try:
        print(f"   ğŸ—‘ï¸  æ—¢å­˜ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ä¸­: {year}-{month}")
        query_job = bq_client.query(delete_query)
        query_job.result()

        if query_job.num_dml_affected_rows:
            print(f"      å‰Šé™¤: {query_job.num_dml_affected_rows} è¡Œ")
        else:
            print(f"      å‰Šé™¤å¯¾è±¡ãªã—")

        return True

    except Exception as e:
        print(f"   âš ï¸  å‰Šé™¤å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—: {e}")
        return True

def load_csv_to_bigquery(
    bq_client: bigquery.Client,
    table_name: str,
    yyyymm: str
) -> bool:
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’BigQueryã«ãƒ­ãƒ¼ãƒ‰"""
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
    gcs_uri = f"gs://{LANDING_BUCKET}/proceed/{yyyymm}/{table_name}.csv"

    try:
        job_config = bigquery.LoadJobConfig(
            source_format=bigquery.SourceFormat.CSV,
            skip_leading_rows=1,
            autodetect=False,
            write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
            schema_update_options=[
                bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION,
            ],
            allow_quoted_newlines=True,
            allow_jagged_rows=False,
            ignore_unknown_values=False,
            max_bad_records=0,
        )

        load_job = bq_client.load_table_from_uri(
            gcs_uri,
            table_id,
            job_config=job_config
        )

        print(f"   â³ ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {table_name} (Job ID: {load_job.job_id})")

        load_job.result(timeout=300)

        destination_table = bq_client.get_table(table_id)
        print(f"   âœ… ãƒ­ãƒ¼ãƒ‰å®Œäº†: {load_job.output_rows} è¡Œã‚’è¿½åŠ ")
        print(f"      ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {destination_table.num_rows:,} è¡Œ")

        return True

    except GoogleCloudError as e:
        print(f"   âŒ ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        if hasattr(e, 'errors') and e.errors:
            for error in e.errors:
                print(f"      è©³ç´°: {error}")
        return False
    except Exception as e:
        print(f"   âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        return False

# ============================================================
# Flask ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
# ============================================================

app = Flask(__name__)

@app.route("/transform", methods=["POST"])
def transform_endpoint():
    """
    Excel â†’ CSV å¤‰æ›ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

    ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¾‹:
    {
        "yyyymm": "202509",
        "tables": ["sales_target_and_achievements", "billing_balance"]
    }

    ç©ºã®å ´åˆã¯å…¨ãƒ†ãƒ¼ãƒ–ãƒ«å‡¦ç†
    """
    try:
        payload = request.get_json(force=True, silent=True) or {}
        yyyymm = payload.get("yyyymm")
        tables = payload.get("tables", list(TABLE_CONFIG.keys()))

        if not yyyymm:
            return jsonify({"error": "yyyymm is required"}), 400

        print("=" * 60)
        print(f"raw/ â†’ proceed/ å¤‰æ›å‡¦ç†")
        print(f"å¯¾è±¡å¹´æœˆ: {yyyymm}")
        print("=" * 60)

        storage_client = storage.Client()

        success_count = 0
        error_count = 0
        results = []

        for table_name in tables:
            if transform_excel_to_csv(storage_client, table_name, yyyymm):
                success_count += 1
                results.append({"table": table_name, "status": "success"})
            else:
                error_count += 1
                results.append({"table": table_name, "status": "error"})

        print("=" * 60)
        print(f"å‡¦ç†å®Œäº†: æˆåŠŸ {success_count} / ã‚¨ãƒ©ãƒ¼ {error_count}")
        print("=" * 60)

        return jsonify({
            "status": "completed",
            "yyyymm": yyyymm,
            "success": success_count,
            "error": error_count,
            "results": results
        }), 200

    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route("/load", methods=["POST"])
def load_endpoint():
    """
    CSV â†’ BigQuery ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

    ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¾‹:
    {
        "yyyymm": "202509",
        "tables": ["sales_target_and_achievements"],
        "replace": true
    }
    """
    try:
        payload = request.get_json(force=True, silent=True) or {}
        yyyymm = payload.get("yyyymm")
        tables = payload.get("tables", list(TABLE_CONFIG.keys()))
        replace_existing = payload.get("replace", False)

        if not yyyymm:
            return jsonify({"error": "yyyymm is required"}), 400

        print("=" * 60)
        print(f"proceed/ â†’ BigQuery ãƒ­ãƒ¼ãƒ‰å‡¦ç†")
        print(f"å¯¾è±¡å¹´æœˆ: {yyyymm}")
        print(f"ãƒ¢ãƒ¼ãƒ‰: {'REPLACE' if replace_existing else 'APPEND'}")
        print("=" * 60)

        bq_client = bigquery.Client(project=PROJECT_ID)
        storage_client = storage.Client()

        success_count = 0
        error_count = 0
        results = []

        for table_name in tables:
            print(f"\nğŸ“Š å‡¦ç†ä¸­: {table_name}")

            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®å‰Šé™¤
            if replace_existing:
                delete_partition_data(bq_client, table_name, yyyymm)

            # BigQueryã¸ãƒ­ãƒ¼ãƒ‰
            if load_csv_to_bigquery(bq_client, table_name, yyyymm):
                # ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚’æ›´æ–°
                update_table_and_column_descriptions(bq_client, storage_client, table_name)
                success_count += 1
                results.append({"table": table_name, "status": "success"})
            else:
                error_count += 1
                results.append({"table": table_name, "status": "error"})

        print("\n" + "=" * 60)
        print(f"å‡¦ç†å®Œäº†: æˆåŠŸ {success_count} / ã‚¨ãƒ©ãƒ¼ {error_count}")
        print("=" * 60)

        return jsonify({
            "status": "completed",
            "yyyymm": yyyymm,
            "success": success_count,
            "error": error_count,
            "results": results
        }), 200

    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route("/pubsub", methods=["POST"])
def pubsub_endpoint():
    """
    Pub/Sub ãƒˆãƒªã‚¬ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    drive-to-gcså®Œäº†å¾Œã«è‡ªå‹•å®Ÿè¡Œã•ã‚Œã‚‹

    ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä¾‹:
    {
        "message": {
            "data": "eyJ5eXl5bW0iOiAiMjAyNTA5In0="  # base64: {"yyyymm": "202509"}
        }
    }
    """
    try:
        envelope = request.get_json(force=True, silent=True) or {}
        msg = envelope.get("message", {})
        data_b64 = msg.get("data")

        if not data_b64:
            return ("Bad Request: no message.data", 400)

        payload = json.loads(base64.b64decode(data_b64).decode("utf-8"))
        yyyymm = payload.get("yyyymm")
        tables = payload.get("tables", list(TABLE_CONFIG.keys()))
        replace_existing = payload.get("replace", True)

        if not yyyymm:
            return jsonify({"error": "yyyymm is required"}), 400

        print(f"[INFO] Pub/Sub triggered: yyyymm={yyyymm}")

        # Transformå®Ÿè¡Œ
        print("=" * 60)
        print(f"raw/ â†’ proceed/ å¤‰æ›å‡¦ç†")
        print(f"å¯¾è±¡å¹´æœˆ: {yyyymm}")
        print("=" * 60)

        storage_client = storage.Client()
        transform_success = 0
        transform_error = 0

        for table_name in tables:
            if transform_excel_to_csv(storage_client, table_name, yyyymm):
                transform_success += 1
            else:
                transform_error += 1

        print(f"å¤‰æ›å®Œäº†: æˆåŠŸ {transform_success} / ã‚¨ãƒ©ãƒ¼ {transform_error}")

        # Loadå®Ÿè¡Œ
        print("=" * 60)
        print(f"proceed/ â†’ BigQuery ãƒ­ãƒ¼ãƒ‰å‡¦ç†")
        print(f"å¯¾è±¡å¹´æœˆ: {yyyymm}")
        print("=" * 60)

        bq_client = bigquery.Client(project=PROJECT_ID)
        load_success = 0
        load_error = 0

        for table_name in tables:
            print(f"\nğŸ“Š å‡¦ç†ä¸­: {table_name}")

            if replace_existing:
                delete_partition_data(bq_client, table_name, yyyymm)

            if load_csv_to_bigquery(bq_client, table_name, yyyymm):
                update_table_and_column_descriptions(bq_client, storage_client, table_name)
                load_success += 1
            else:
                load_error += 1

        print(f"ãƒ­ãƒ¼ãƒ‰å®Œäº†: æˆåŠŸ {load_success} / ã‚¨ãƒ©ãƒ¼ {load_error}")

        return ("", 204)

    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route("/", methods=["GET"])
def health():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return "gcs-to-bq service is running", 200

if __name__ == "__main__":
    port = int(os.getenv("PORT", "8080"))
    app.run(host="0.0.0.0", port=port, debug=False)
