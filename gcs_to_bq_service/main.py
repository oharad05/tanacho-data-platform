#!/usr/bin/env python3
"""
gcs-to-bq Cloud Run Service
GCSä¸Šã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’CSVã«å¤‰æ›ã—ã€BigQueryã«ãƒ­ãƒ¼ãƒ‰

ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½:
- ã‚«ãƒ©ãƒ ä¸æ•´åˆãƒã‚§ãƒƒã‚¯
- ãƒ¬ã‚³ãƒ¼ãƒ‰0ä»¶ãƒã‚§ãƒƒã‚¯
- é‡è¤‡ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯

çµæœã¯Google Cloud Loggingã«å‡ºåŠ›ã•ã‚Œã€å¾Œã‹ã‚‰Slackç­‰ã«é€£æºå¯èƒ½ã€‚
"""

import os
import io
import json
import traceback
import logging
import pandas as pd
import numpy as np
from datetime import datetime, date
from typing import Dict, Optional, Any, List


class DateTimeEncoder(json.JSONEncoder):
    """date/datetime ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ–‡å­—åˆ—ã«å¤‰æ›ã™ã‚‹JSONã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼"""
    def default(self, obj):
        if isinstance(obj, datetime):
            return obj.isoformat()
        if isinstance(obj, date):
            return obj.isoformat()
        return super().default(obj)
from flask import Flask, request, jsonify
from google.cloud import storage
from google.cloud import bigquery
from google.cloud.exceptions import GoogleCloudError

# ============================================================
# çµ±ä¸€ãƒ­ã‚°è¨­å®š
# ============================================================

# ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æœ‰åŠ¹åŒ–ãƒ•ãƒ©ã‚°
VALIDATION_ENABLED = os.environ.get("VALIDATION_ENABLED", "true").lower() == "true"

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è­˜åˆ¥ç”¨ã®è¨­å®š
PIPELINE_ID = "data-pipeline"
STEP_NAME = "gcs-to-bq"

# çµ±ä¸€ãƒ­ã‚°ç”¨ã®logger
pipeline_logger = logging.getLogger("pipeline-logger")
if not pipeline_logger.handlers:
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter('%(message)s'))
    pipeline_logger.addHandler(handler)
    pipeline_logger.setLevel(logging.INFO)


def get_execution_id() -> str:
    """
    å®Ÿè¡ŒIDã‚’å–å¾—
    ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã€ãªã‘ã‚Œã°ãƒªã‚¯ã‚¨ã‚¹ãƒˆã”ã¨ã«ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ç”Ÿæˆ
    """
    return os.environ.get("EXECUTION_ID", datetime.utcnow().strftime("%Y%m%d_%H%M%S"))


def log_pipeline_event(
    action: str,
    status: str = "INFO",
    message: str = "",
    table_name: Optional[str] = None,
    details: Optional[Dict[str, Any]] = None,
    execution_id: Optional[str] = None
) -> None:
    """
    çµ±ä¸€å½¢å¼ã§ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ­ã‚°ã‚’å‡ºåŠ›

    Args:
        action: å®Ÿè¡Œä¸­ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆä¾‹: "load", "transform", "validation"ï¼‰
        status: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼ˆ"INFO", "OK", "ERROR", "WARNING"ï¼‰
        message: ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        table_name: å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        details: è©³ç´°æƒ…å ±ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        execution_id: å®Ÿè¡ŒIDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€æŒ‡å®šãªã‘ã‚Œã°è‡ªå‹•ç”Ÿæˆï¼‰
    """
    severity = "ERROR" if status == "ERROR" else "WARNING" if status == "WARNING" else "INFO"
    exec_id = execution_id or get_execution_id()

    log_entry = {
        "severity": severity,
        "message": message,
        "labels": {
            "pipeline_id": PIPELINE_ID,
            "execution_id": exec_id,
            "step": STEP_NAME,
            "action": action,
            "status": status
        },
        "jsonPayload": {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "pipeline_id": PIPELINE_ID,
            "execution_id": exec_id,
            "step": STEP_NAME,
            "action": action,
            "status": status,
            "message": message
        }
    }

    if table_name:
        log_entry["labels"]["table_name"] = table_name
        log_entry["jsonPayload"]["table_name"] = table_name

    if details:
        log_entry["jsonPayload"]["details"] = details

    if severity == "ERROR":
        pipeline_logger.error(json.dumps(log_entry, ensure_ascii=False, cls=DateTimeEncoder))
    elif severity == "WARNING":
        pipeline_logger.warning(json.dumps(log_entry, ensure_ascii=False, cls=DateTimeEncoder))
    else:
        pipeline_logger.info(json.dumps(log_entry, ensure_ascii=False, cls=DateTimeEncoder))


# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
validation_logger = pipeline_logger

# ãƒ†ãƒ¼ãƒ–ãƒ«ã”ã¨ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼å®šç¾©ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
UNIQUE_KEYS_CONFIG = {
    "sales_target_and_achievements": ["sales_accounting_period", "branch_code", "department_code", "staff_code"],
    "billing_balance": ["sales_month", "branch_code", "branch_name", "source_folder"],
    "ledger_income": ["slip_date", "slip_number", "line_number"],
    "ledger_loss": ["accounting_month", "slip_number", "line_number"],
    "department_summary": ["sales_accounting_period", "code"],
    "internal_interest": ["year_month", "branch", "category"],
    "profit_plan_term": ["period", "item"],
    "profit_plan_term_nagasaki": ["period", "item"],
    "profit_plan_term_fukuoka": ["period", "item"],
    "stocks": ["year_month", "branch", "category"],
    "ms_allocation_ratio": ["year_month", "branch", "department", "category"],
    "customer_sales_target_and_achievements": ["sales_accounting_period", "branch_code", "customer_code"],
    "construction_progress_days_amount": ["property_period", "branch_code", "staff_code", "property_number", "customer_code", "contract_date"],
    "construction_progress_days_final_date": ["final_billing_sales_date", "property_number", "property_data_classification"],
}

# ç’°å¢ƒå¤‰æ•°
PROJECT_ID = os.environ.get("GCP_PROJECT", "data-platform-prod-475201")
DATASET_ID = "corporate_data"
LANDING_BUCKET = os.environ.get("LANDING_BUCKET", "data-platform-landing-prod")
COLUMNS_PATH = "google-drive/config/columns"
MAPPING_FILE = "google-drive/config/mapping/mapping_files.csv"
MONETARY_SCALE_FILE = "google-drive/config/mapping/monetary_scale_conversion.csv"
ZERO_DATE_FILE = "google-drive/config/mapping/zero_date_to_null.csv"

# ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©
TABLE_CONFIG = {
    "sales_target_and_achievements": {
        "partition_field": "sales_accounting_period",
        "clustering_fields": ["branch_code"]
    },
    "billing_balance": {
        "partition_field": "sales_month",
        "clustering_fields": ["branch_code"]
    },
    "ledger_income": {
        "partition_field": "slip_date",
        "clustering_fields": ["classification_type"]
    },
    "department_summary": {
        "partition_field": "sales_accounting_period",
        "clustering_fields": ["code"]
    },
    "internal_interest": {
        "partition_field": "year_month",
        "clustering_fields": ["branch"]
    },
    "profit_plan_term": {
        "partition_field": "period",
        "clustering_fields": ["item"]
    },
    "profit_plan_term_nagasaki": {
        "partition_field": "period",
        "clustering_fields": ["item"]
    },
    "profit_plan_term_fukuoka": {
        "partition_field": "period",
        "clustering_fields": ["item"]
    },
    "ledger_loss": {
        "partition_field": "slip_date",
        "clustering_fields": ["classification_type"]
    },
    "customer_sales_target_and_achievements": {
        "partition_field": "sales_accounting_period",
        "clustering_fields": ["branch_code", "customer_code"]
    },
    "construction_progress_days_amount": {
        "partition_field": "property_period",
        "clustering_fields": ["branch_code", "property_number"]
    },
    "construction_progress_days_final_date": {
        "partition_field": "final_billing_sales_date",
        "clustering_fields": ["property_number"]
    },
    "stocks": {
        "partition_field": "year_month",
        "clustering_fields": ["branch"]
    },
    "ms_allocation_ratio": {
        "partition_field": "year_month",
        "clustering_fields": ["branch"]
    }
}

# ============================================================
# ç´¯ç©å‹ãƒ†ãƒ¼ãƒ–ãƒ«ã®å®šç¾©
# ============================================================
# 2025-12-27: å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å˜æœˆå‹ã¨ã—ã¦æ‰±ã†ã‚ˆã†ã«å¤‰æ›´
# ç†ç”±: ç´¯ç©å‹å‡¦ç†ã«ã‚ˆã‚Šéå»æœˆã®source_folderãŒæ¶ˆå¤±ã—ã€
#       SQLã®JOINæ¡ä»¶ï¼ˆsource_folder = year_monthï¼‰ãŒæˆç«‹ã—ãªããªã‚‹å•é¡Œã‚’å›é¿
#
# ä»¥ä¸‹ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã¯å˜æœˆå‹ã¨ã—ã¦æ‰±ã†ï¼ˆå„æœˆã®source_folderã‚’ä¿æŒï¼‰:
# - billing_balance
# - profit_plan_term / profit_plan_term_nagasaki / profit_plan_term_fukuoka
# - ms_allocation_ratio
# - construction_progress_days_amount
# - stocks
# - construction_progress_days_final_date
CUMULATIVE_TABLE_CONFIG = {}

# ============================================================
# ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€£æºãƒ†ãƒ¼ãƒ–ãƒ«ã®å®šç¾©
# ============================================================
# ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰é€£æºã•ã‚Œã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«
# ãƒ‘ã‚¹: gs://data-platform-landing-prod/spreadsheet/proceed/
SPREADSHEET_PROCEED_PATH = "spreadsheet/proceed"
SPREADSHEET_COLUMNS_PATH = "spreadsheet/config/columns"
SPREADSHEET_TABLE_PREFIX = "ss_"

SPREADSHEET_TABLE_CONFIG = {
    "gs_sales_profit": {
        "description": "GSå£²ä¸Šåˆ©ç›Š",
        "bq_table_name": "ss_gs_sales_profit",
    },
    "inventory_advance_tokyo": {
        "description": "æ±äº¬åœ¨åº«å‰æ‰•",
        "bq_table_name": "ss_inventory_advance_tokyo",
    },
    "inventory_advance_nagasaki": {
        "description": "é•·å´åœ¨åº«å‰æ‰•",
        "bq_table_name": "ss_inventory_advance_nagasaki",
    },
    "inventory_advance_fukuoka": {
        "description": "ç¦å²¡åœ¨åº«å‰æ‰•",
        "bq_table_name": "ss_inventory_advance_fukuoka",
    },
}

# ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼å®šç¾©ï¼ˆãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ï¼‰
SPREADSHEET_UNIQUE_KEYS_CONFIG = {
    "ss_gs_sales_profit": [],  # ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼æœªå®šç¾©
    "ss_inventory_advance_tokyo": ["posting_month", "branch_name", "sales_office", "category"],
    "ss_inventory_advance_nagasaki": ["posting_month", "branch_name", "sales_office", "category"],
    "ss_inventory_advance_fukuoka": ["posting_month", "branch_name", "sales_office", "category"],
}

# ============================================================
# ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–¢æ•°
# ============================================================

def validate_table_config_completeness(
    storage_client: storage.Client,
    target_months: list
) -> Dict[str, Any]:
    """
    GCSã®CSVãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã¨TABLE_CONFIGã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯

    TABLE_CONFIGã«å«ã¾ã‚Œã¦ã„ãªã„ãƒ†ãƒ¼ãƒ–ãƒ«ãŒã‚ã‚‹å ´åˆã€ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™ã€‚
    ã“ã‚Œã«ã‚ˆã‚Šã€æ–°è¦ãƒ†ãƒ¼ãƒ–ãƒ«è¿½åŠ æ™‚ã®è¨­å®šæ¼ã‚Œã«ã‚ˆã‚‹é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’é˜²æ­¢ã™ã‚‹ã€‚

    Returns:
        Dict with keys:
        - status: "OK" or "ERROR"
        - missing_tables: TABLE_CONFIGã«å«ã¾ã‚Œã¦ã„ãªã„ãƒ†ãƒ¼ãƒ–ãƒ«åã®ãƒªã‚¹ãƒˆ
        - message: çµæœãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    """
    bucket = storage_client.bucket(LANDING_BUCKET)

    # GCSã®proceed/é…ä¸‹ã®å…¨CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
    gcs_tables = set()
    for month in target_months:
        prefix = f"google-drive/proceed/{month}/"
        blobs = bucket.list_blobs(prefix=prefix)
        for blob in blobs:
            if blob.name.endswith(".csv"):
                # google-drive/proceed/202409/sales_target_and_achievements.csv
                # â†’ sales_target_and_achievements
                filename = blob.name.split("/")[-1]
                table_name = filename.replace(".csv", "")
                gcs_tables.add(table_name)

    # TABLE_CONFIGã«å«ã¾ã‚Œã¦ã„ãªã„ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¤œå‡º
    missing_tables = gcs_tables - set(TABLE_CONFIG.keys())

    if missing_tables:
        return {
            "status": "ERROR",
            "validation_type": "table_config_completeness",
            "missing_tables": sorted(list(missing_tables)),
            "message": f"TABLE_CONFIGã«æœªè¨­å®šã®ãƒ†ãƒ¼ãƒ–ãƒ«ãŒã‚ã‚Šã¾ã™: {', '.join(sorted(missing_tables))}ã€‚"
                       f"è¨­å®šã‚’è¿½åŠ ã—ã¦ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
        }

    return {
        "status": "OK",
        "validation_type": "table_config_completeness",
        "missing_tables": [],
        "message": f"å…¨ã¦ã®GCSãƒ†ãƒ¼ãƒ–ãƒ«({len(gcs_tables)}ä»¶)ãŒTABLE_CONFIGã«è¨­å®šæ¸ˆã¿ã§ã™ã€‚"
    }


def log_validation_result(result: Dict[str, Any]) -> None:
    """
    ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœã‚’Cloud Loggingã«å‡ºåŠ›

    æ§‹é€ åŒ–ãƒ­ã‚°ã¨ã—ã¦Cloud Loggingã§æ¤œç´¢ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯èƒ½ã€‚
    ãƒ­ã‚°ã¯ä»¥ä¸‹ã®ãƒ©ãƒ™ãƒ«ã§ãƒ•ã‚£ãƒ«ã‚¿å¯èƒ½:
    - labels.service: "gcs-to-bq"
    - labels.validation_type: "column_check" / "empty_check" / "duplicate_check"
    - labels.status: "OK" / "ERROR"
    """
    log_entry = {
        "severity": "ERROR" if result.get("status") == "ERROR" else "INFO",
        "message": _format_validation_message(result),
        "labels": {
            "service": "gcs-to-bq",
            "table_name": result.get("table_name", "unknown"),
            "validation_type": result.get("validation_type", "unknown"),
            "status": result.get("status", "unknown")
        },
        "jsonPayload": result
    }

    if result.get("status") == "ERROR":
        validation_logger.error(json.dumps(log_entry, ensure_ascii=False, cls=DateTimeEncoder))
    elif result.get("warnings"):
        validation_logger.warning(json.dumps(log_entry, ensure_ascii=False, cls=DateTimeEncoder))
    else:
        validation_logger.info(json.dumps(log_entry, ensure_ascii=False, cls=DateTimeEncoder))


def _format_validation_message(result: Dict[str, Any]) -> str:
    """ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ•´å½¢"""
    status = result.get("status", "UNKNOWN")
    table_name = result.get("table_name", "unknown")
    validation_type = result.get("validation_type", "validation")

    if status == "OK":
        row_count = result.get("row_count", result.get("total_rows", 0))
        return f"[VALIDATION {status}] {table_name}: {validation_type} passed ({row_count} rows)"
    else:
        error_count = len(result.get("errors", []))
        return f"[VALIDATION {status}] {table_name}: {validation_type} failed ({error_count} errors)"


def validate_columns_and_rows(
    df: pd.DataFrame,
    table_name: str,
    expected_columns: List[str],
    source_file: str = None
) -> Dict[str, Any]:
    """
    ã‚«ãƒ©ãƒ ä¸æ•´åˆã¨ãƒ¬ã‚³ãƒ¼ãƒ‰0ä»¶ã‚’ãƒã‚§ãƒƒã‚¯

    Args:
        df: æ¤œè¨¼å¯¾è±¡ã®DataFrameï¼ˆæ—¥æœ¬èªã‚«ãƒ©ãƒ åï¼‰
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
        expected_columns: æœŸå¾…ã•ã‚Œã‚‹ã‚«ãƒ©ãƒ åãƒªã‚¹ãƒˆï¼ˆæ—¥æœ¬èªï¼‰
        source_file: ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å

    Returns:
        æ¤œè¨¼çµæœã®è¾æ›¸
    """
    errors = []
    warnings = []

    # ã‚«ãƒ©ãƒ åã®æ”¹è¡Œã‚’é™¤å»ã—ã¦ã‹ã‚‰æ¯”è¼ƒ
    actual_columns = [str(col).replace('\n', '') for col in df.columns]

    # 1. ã‚«ãƒ©ãƒ ä¸æ•´åˆãƒã‚§ãƒƒã‚¯
    missing_columns = [col for col in expected_columns if col not in actual_columns]
    extra_columns = [col for col in actual_columns if col not in expected_columns]

    if missing_columns:
        errors.append({
            "type": "MISSING_COLUMNS",
            "message": f"æœŸå¾…ã•ã‚Œã‚‹ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {missing_columns}",
            "details": {"missing": missing_columns}
        })

    if extra_columns:
        warnings.append({
            "type": "EXTRA_COLUMNS",
            "message": f"å®šç¾©å¤–ã®ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã™: {extra_columns}",
            "details": {"extra": extra_columns}
        })

    # 2. ãƒ¬ã‚³ãƒ¼ãƒ‰0ä»¶ãƒã‚§ãƒƒã‚¯
    row_count = len(df)
    if row_count == 0:
        errors.append({
            "type": "EMPTY_DATA",
            "message": "ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™"
        })

    result = {
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "service": "gcs-to-bq",
        "validation_type": "column_and_row_check",
        "table_name": table_name,
        "source_file": source_file,
        "status": "ERROR" if errors else "OK",
        "row_count": row_count,
        "column_count": len(actual_columns),
        "expected_column_count": len(expected_columns),
        "errors": errors,
        "warnings": warnings
    }

    return result


def validate_duplicates_in_bq(
    bq_client: bigquery.Client,
    table_name: str
) -> Dict[str, Any]:
    """
    BigQueryãƒ†ãƒ¼ãƒ–ãƒ«ã®é‡è¤‡ã‚’ãƒã‚§ãƒƒã‚¯

    Args:
        bq_client: BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å

    Returns:
        æ¤œè¨¼çµæœã®è¾æ›¸
    """
    errors = []

    # ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼å®šç¾©ã‚’å–å¾—
    unique_keys = UNIQUE_KEYS_CONFIG.get(table_name)
    if not unique_keys:
        return {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "service": "gcs-to-bq",
            "validation_type": "duplicate_check",
            "table_name": table_name,
            "status": "SKIPPED",
            "message": "ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        }

    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
    key_cols = ", ".join(unique_keys)

    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚¯ã‚¨ãƒª
    query = f"""
    SELECT {key_cols}, COUNT(*) as duplicate_count
    FROM `{table_id}`
    GROUP BY {key_cols}
    HAVING COUNT(*) > 1
    LIMIT 10
    """

    try:
        result = bq_client.query(query).result()
        duplicates = [dict(row) for row in result]
        duplicate_count = len(duplicates)

        if duplicate_count > 0:
            errors.append({
                "type": "DUPLICATE_RECORDS",
                "message": f"é‡è¤‡ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒå­˜åœ¨ã—ã¾ã™ï¼ˆã‚µãƒ³ãƒ—ãƒ«: {duplicate_count}ä»¶ï¼‰",
                "details": {
                    "unique_keys": unique_keys,
                    "sample_duplicates": duplicates
                }
            })

        return {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "service": "gcs-to-bq",
            "validation_type": "duplicate_check",
            "table_name": table_name,
            "status": "ERROR" if errors else "OK",
            "unique_keys": unique_keys,
            "duplicate_sample_count": duplicate_count,
            "errors": errors
        }

    except Exception as e:
        return {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "service": "gcs-to-bq",
            "validation_type": "duplicate_check",
            "table_name": table_name,
            "status": "ERROR",
            "errors": [{
                "type": "QUERY_ERROR",
                "message": f"é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}"
            }]
        }


# ============================================================
# Excel â†’ CSV å¤‰æ›å‡¦ç†
# ============================================================

def load_column_mapping(table_name: str) -> Dict[str, Dict[str, str]]:
    """ã‚«ãƒ©ãƒ ãƒãƒƒãƒ”ãƒ³ã‚°å®šç¾©ã‚’èª­ã¿è¾¼ã¿"""
    storage_client = storage.Client()
    bucket = storage_client.bucket(LANDING_BUCKET)

    mapping_blob = bucket.blob(f"{COLUMNS_PATH}/{table_name}.csv")
    if not mapping_blob.exists():
        print(f"âš ï¸  ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {table_name}.csv")
        return {}

    csv_data = mapping_blob.download_as_bytes()
    df = pd.read_csv(io.BytesIO(csv_data))

    mapping = {}
    for _, row in df.iterrows():
        mapping[row['jp_name']] = {
            'en_name': row['en_name'],
            'type': row['type']
        }
    return mapping

def convert_date_format(value: Any, date_type: str, column_name: str = '') -> str:
    """æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å¤‰æ›"""
    if pd.isna(value) or value == '' or value is None:
        return ''

    # ç„¡åŠ¹ãªæ—¥ä»˜å€¤ã‚’ç©ºæ–‡å­—åˆ—ã«å¤‰æ›
    value_str = str(value)
    if value_str in ['0000/00/00', '0000-00-00', '00/00/0000', '0', 'NaT']:
        return ''

    # èª¤ã£ãŸå½¢å¼ã®æ—¥ä»˜ã‚’ä¿®æ­£ (ä¾‹: "0223/03/25" â†’ "2023/03/25")
    import re
    match = re.match(r'^0(\d{3})/(\d{2})/(\d{2})$', value_str)
    if match:
        value_str = f"2{match.group(1)}/{match.group(2)}/{match.group(3)}"

    # æ•°å€¤ã®å ´åˆã®å‡¦ç†
    if isinstance(value, (int, float)):
        # Excelã®ã‚·ãƒªã‚¢ãƒ«æ—¥ä»˜
        if value > 0 and value < 100000:
            try:
                excel_base = pd.Timestamp('1899-12-30')
                dt = excel_base + pd.Timedelta(days=int(value))
                if date_type == 'DATETIME':
                    return dt.strftime('%Y-%m-%d %H:%M:%S')
                else:
                    return dt.strftime('%Y-%m-%d')
            except:
                pass

        # Unixã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆãƒŠãƒç§’ï¼‰
        elif value > 1e15:
            try:
                dt = pd.to_datetime(value, unit='ns')
                if date_type == 'DATETIME':
                    return dt.strftime('%Y-%m-%d %H:%M:%S')
                else:
                    return dt.strftime('%Y-%m-%d')
            except:
                pass


    # ã€Œå¹´æœˆã€ç‰¹æ®Šå‡¦ç†ï¼ˆä¾‹: "2025å¹´9æœˆ" â†’ "2025-09-01"ï¼‰
    if 'å¹´' in value_str and 'æœˆ' in value_str:
        import re
        try:
            match = re.match(r'(\d{4})å¹´(\d{1,2})æœˆ', value_str)
            if match:
                year = match.group(1)
                month = match.group(2).zfill(2)
                return f"{year}-{month}-01"
        except:
            pass

    # DATEå‹ã®å‡¦ç†
    if date_type == 'DATE':
        # YYYY/MMå½¢å¼ã®å ´åˆã€1æ—¥ã‚’è¿½åŠ 
        import re
        if re.match(r'^\d{4}/\d{1,2}$', value_str):
            try:
                dt = pd.to_datetime(value_str + '/01', format='%Y/%m/%d')
                return dt.strftime('%Y-%m-%d')
            except:
                pass

        try:
            dt = pd.to_datetime(value_str)
            return dt.strftime('%Y-%m-%d')
        except:
            print(f"âš ï¸  æ—¥ä»˜å¤‰æ›ã‚¨ãƒ©ãƒ¼: {value_str}")
            return value_str

    # DATETIMEå‹ã®å‡¦ç†
    elif date_type == 'DATETIME':
        try:
            dt = pd.to_datetime(value_str)
            return dt.strftime('%Y-%m-%d %H:%M:%S')
        except:
            print(f"âš ï¸  æ—¥æ™‚å¤‰æ›ã‚¨ãƒ©ãƒ¼: {value_str}")
            return value_str

    return value_str

def apply_data_type_conversion(df: pd.DataFrame, column_mapping: Dict) -> pd.DataFrame:
    """ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›ã‚’é©ç”¨"""
    df = df.copy()

    for col in df.columns:
        if col not in column_mapping:
            continue

        data_type = column_mapping[col]['type']

        # DATE/DATETIMEå‹
        if data_type in ['DATE', 'DATETIME']:
            if pd.api.types.is_datetime64_any_dtype(df[col]):
                if data_type == 'DATE':
                    df[col] = pd.to_datetime(df[col]).dt.strftime('%Y-%m-%d')
                else:
                    df[col] = pd.to_datetime(df[col]).dt.strftime('%Y-%m-%d %H:%M:%S')
            else:
                df[col] = df[col].apply(lambda x: convert_date_format(x, data_type, col))

        # INT64å‹
        elif data_type == 'INT64':
            df[col] = pd.to_numeric(df[col], errors='coerce')
            df[col] = df[col].astype('Int64')

        # NUMERICå‹
        elif data_type == 'NUMERIC':
            df[col] = pd.to_numeric(df[col], errors='coerce')

        # STRINGå‹
        elif data_type == 'STRING':
            df[col] = df[col].fillna('')
            df[col] = df[col].astype(str)
            df[col] = df[col].replace('nan', '')

    return df

def rename_columns(df: pd.DataFrame, column_mapping: Dict) -> pd.DataFrame:
    """ã‚«ãƒ©ãƒ åã‚’æ—¥æœ¬èªã‹ã‚‰è‹±èªã«å¤‰æ›"""
    rename_dict = {}

    for jp_col in df.columns:
        if jp_col in column_mapping:
            en_col = column_mapping[jp_col]['en_name']
            rename_dict[jp_col] = en_col
        else:
            print(f"âš ï¸  ãƒãƒƒãƒ”ãƒ³ã‚°æœªå®šç¾©ã®ã‚«ãƒ©ãƒ : {jp_col}")
            rename_dict[jp_col] = jp_col

    return df.rename(columns=rename_dict)

def load_monetary_scale_config(storage_client: storage.Client) -> pd.DataFrame:
    """é‡‘é¡å˜ä½å¤‰æ›è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
    try:
        bucket = storage_client.bucket(LANDING_BUCKET)
        blob = bucket.blob(MONETARY_SCALE_FILE)

        if not blob.exists():
            print(f"âš ï¸  é‡‘é¡å¤‰æ›è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {MONETARY_SCALE_FILE}")
            return pd.DataFrame()

        csv_data = blob.download_as_bytes()
        df = pd.read_csv(io.BytesIO(csv_data))
        return df
    except Exception as e:
        print(f"âš ï¸  é‡‘é¡å¤‰æ›è¨­å®šã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return pd.DataFrame()

def apply_monetary_scale_conversion(
    df: pd.DataFrame,
    table_name: str,
    storage_client: storage.Client
) -> pd.DataFrame:
    """
    é‡‘é¡å˜ä½å¤‰æ›ã‚’é©ç”¨

    Args:
        df: å¤‰æ›å¯¾è±¡ã®DataFrameï¼ˆè‹±èªã‚«ãƒ©ãƒ åã«å¤‰æ›æ¸ˆã¿ï¼‰
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
        storage_client: Storage Client

    Returns:
        å¤‰æ›å¾Œã®DataFrame
    """
    try:
        # é‡‘é¡å¤‰æ›è¨­å®šã‚’èª­ã¿è¾¼ã¿
        config_df = load_monetary_scale_config(storage_client)

        if config_df.empty:
            return df

        # å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¨­å®šã‚’å–å¾—
        target_config = config_df[config_df['file_name'] == table_name]

        if target_config.empty:
            print(f"   é‡‘é¡å¤‰æ›è¨­å®šãªã—: {table_name}")
            return df

        df = df.copy()

        for _, config in target_config.iterrows():
            condition_col = config['condition_column_name']
            condition_values = eval(config['condition_column_value'])  # ãƒªã‚¹ãƒˆæ–‡å­—åˆ—ã‚’è©•ä¾¡
            object_columns = eval(config['object_column_name'])  # ãƒªã‚¹ãƒˆæ–‡å­—åˆ—ã‚’è©•ä¾¡
            convert_value = float(config['convert_value'])

            # æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹è¡Œã‚’ãƒ•ã‚£ãƒ«ã‚¿
            if condition_col not in df.columns:
                print(f"âš ï¸  æ¡ä»¶ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {condition_col}")
                continue

            mask = df[condition_col].isin(condition_values)

            # å¯¾è±¡ã‚«ãƒ©ãƒ ã‚’å¤‰æ›
            for col in object_columns:
                if col in df.columns:
                    # æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹è¡Œã®ã¿å¤‰æ›
                    df.loc[mask, col] = pd.to_numeric(df.loc[mask, col], errors='coerce') * convert_value
                    print(f"   ğŸ’° {col} ã‚’{convert_value}å€ã«å¤‰æ›ï¼ˆæ¡ä»¶: {condition_col} in {condition_values}ï¼‰")
                else:
                    print(f"âš ï¸  å¤‰æ›å¯¾è±¡ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {col}")

        return df

    except Exception as e:
        print(f"âš ï¸  é‡‘é¡å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return df


def load_zero_date_config(storage_client: storage.Client) -> pd.DataFrame:
    """GCSã‹ã‚‰ã‚¼ãƒ­æ—¥ä»˜å¤‰æ›è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
    try:
        bucket = storage_client.bucket(LANDING_BUCKET)
        blob = bucket.blob(ZERO_DATE_FILE)

        if not blob.exists():
            print(f"âš ï¸  ã‚¼ãƒ­æ—¥ä»˜å¤‰æ›è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: gs://{LANDING_BUCKET}/{ZERO_DATE_FILE}")
            return pd.DataFrame()

        csv_data = blob.download_as_bytes()
        df = pd.read_csv(io.BytesIO(csv_data))
        return df
    except Exception as e:
        print(f"âš ï¸  ã‚¼ãƒ­æ—¥ä»˜å¤‰æ›è¨­å®šã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return pd.DataFrame()


def apply_zero_date_to_null_conversion(
    df: pd.DataFrame,
    table_name: str,
    storage_client: storage.Client
) -> pd.DataFrame:
    """
    ã‚¼ãƒ­æ—¥ä»˜ï¼ˆ0000/00/00ï¼‰ã‚’nullã«å¤‰æ›

    Args:
        df: å¤‰æ›å¯¾è±¡ã®DataFrameï¼ˆè‹±èªã‚«ãƒ©ãƒ åã«å¤‰æ›æ¸ˆã¿ï¼‰
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
        storage_client: Storage Client

    Returns:
        å¤‰æ›å¾Œã®DataFrame
    """
    try:
        # ã‚¼ãƒ­æ—¥ä»˜å¤‰æ›è¨­å®šã‚’èª­ã¿è¾¼ã¿
        config_df = load_zero_date_config(storage_client)

        if config_df.empty:
            return df

        # å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¨­å®šã‚’å–å¾—
        target_config = config_df[config_df['file_name'] == table_name]

        if target_config.empty:
            return df

        df = df.copy()

        # ã‚¼ãƒ­æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ§˜ã€…ãªå½¢å¼ã«å¯¾å¿œï¼‰
        zero_date_patterns = [
            '0000/00/00',
            '0000-00-00',
            '0000/0/0',
            '0000-0-0',
        ]

        for _, config in target_config.iterrows():
            column_name = config['condition_column_name']

            if column_name not in df.columns:
                print(f"âš ï¸  å¯¾è±¡ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {column_name}")
                continue

            # å¤‰æ›å‰ã®nullä»¥å¤–ã®ä»¶æ•°ã‚’è¨˜éŒ²
            non_null_before = df[column_name].notna().sum()

            # ã‚¼ãƒ­æ—¥ä»˜ã‚’nullã«å¤‰æ›
            for pattern in zero_date_patterns:
                mask = df[column_name].astype(str).str.strip() == pattern
                if mask.any():
                    df.loc[mask, column_name] = None

            # å¤‰æ›å¾Œã®nullä»¥å¤–ã®ä»¶æ•°
            non_null_after = df[column_name].notna().sum()
            converted_count = non_null_before - non_null_after

            if converted_count > 0:
                print(f"   ğŸ”„ {column_name}: {converted_count}ä»¶ã®ã‚¼ãƒ­æ—¥ä»˜ã‚’nullã«å¤‰æ›")

        return df

    except Exception as e:
        print(f"âš ï¸  ã‚¼ãƒ­æ—¥ä»˜å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return df


def transform_excel_to_csv(
    storage_client: storage.Client,
    table_name: str,
    yyyymm: str
) -> bool:
    """Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§CSVã«å¤‰æ›"""
    try:
        print(f"\nğŸ“„ å‡¦ç†ä¸­: {table_name}")

        bucket = storage_client.bucket(LANDING_BUCKET)

        # google-drive/raw/ ã‹ã‚‰èª­ã¿è¾¼ã¿
        raw_path = f"google-drive/raw/{yyyymm}/{table_name}.xlsx"
        raw_blob = bucket.blob(raw_path)

        if not raw_blob.exists():
            print(f"âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: gs://{LANDING_BUCKET}/{raw_path}")
            return False

        # ã‚«ãƒ©ãƒ ãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿
        column_mapping = load_column_mapping(table_name)
        if not column_mapping:
            print(f"âŒ ã‚«ãƒ©ãƒ ãƒãƒƒãƒ”ãƒ³ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {table_name}")
            return False

        # Excelãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        excel_bytes = raw_blob.download_as_bytes()

        # profit_plan_termã®å ´åˆã¯ã€Œæ±äº¬æ”¯åº—ç›®æ¨™103æœŸã€ã‚·ãƒ¼ãƒˆã®ã¿ã‚’èª­ã¿è¾¼ã‚€
        if table_name == "profit_plan_term":
            df = pd.read_excel(io.BytesIO(excel_bytes), sheet_name='æ±äº¬æ”¯åº—ç›®æ¨™103æœŸ')
            print(f"   ã‚·ãƒ¼ãƒˆæŒ‡å®š: æ±äº¬æ”¯åº—ç›®æ¨™103æœŸ")
        else:
            df = pd.read_excel(io.BytesIO(excel_bytes))

        # ã‚«ãƒ©ãƒ åã®æ”¹è¡Œã‚’é™¤å»
        df.columns = [col.replace('\n', '') if isinstance(col, str) else col for col in df.columns]

        print(f"   ãƒ‡ãƒ¼ã‚¿: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")

        # ============================================================
        # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³: ã‚«ãƒ©ãƒ ä¸æ•´åˆãƒ»ãƒ¬ã‚³ãƒ¼ãƒ‰0ä»¶ãƒã‚§ãƒƒã‚¯
        # ============================================================
        if VALIDATION_ENABLED:
            expected_columns = list(column_mapping.keys())
            validation_result = validate_columns_and_rows(
                df=df,
                table_name=table_name,
                expected_columns=expected_columns,
                source_file=raw_path
            )
            log_validation_result(validation_result)

            # ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹å ´åˆã¯è­¦å‘Šã‚’å‡ºã™ãŒå‡¦ç†ã¯ç¶šè¡Œ
            if validation_result.get("status") == "ERROR":
                for error in validation_result.get("errors", []):
                    print(f"   âš ï¸  ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼: {error.get('message')}")
            else:
                print(f"   âœ… ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³OK: ã‚«ãƒ©ãƒ ãƒ»ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ãƒã‚§ãƒƒã‚¯ passed")

        # æ—¥æœ¬èªã‚«ãƒ©ãƒ åã‚’è‹±èªã«å¤‰æ›ï¼ˆå‹å¤‰æ›å‰ï¼‰
        jp_column_mapping = {jp: info for jp, info in column_mapping.items()}

        # æ—¥ä»˜åˆ—ã®äº‹å‰å‡¦ç†
        for jp_col, info in jp_column_mapping.items():
            if jp_col in df.columns and info['type'] in ['DATE', 'DATETIME']:
                if pd.api.types.is_datetime64_any_dtype(df[jp_col]):
                    if info['type'] == 'DATE':
                        df[jp_col] = df[jp_col].dt.strftime('%Y-%m-%d')
                    else:
                        df[jp_col] = df[jp_col].dt.strftime('%Y-%m-%d %H:%M:%S')

        # ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›
        df = apply_data_type_conversion(df, jp_column_mapping)

        # ã‚«ãƒ©ãƒ åå¤‰æ›
        df = rename_columns(df, jp_column_mapping)

        # é‡‘é¡å˜ä½å¤‰æ›ï¼ˆã‚«ãƒ©ãƒ åå¤‰æ›å¾Œã«å®Ÿè¡Œï¼‰
        df = apply_monetary_scale_conversion(df, table_name, storage_client)

        # ã‚¼ãƒ­æ—¥ä»˜ã‚’nullã«å¤‰æ›ï¼ˆé‡‘é¡å¤‰æ›å¾Œã«å®Ÿè¡Œï¼‰
        df = apply_zero_date_to_null_conversion(df, table_name, storage_client)

        # source_folderã‚«ãƒ©ãƒ ã‚’è¿½åŠ ï¼ˆã©ã®ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰å–å¾—ã—ãŸã‹ã‚’è­˜åˆ¥ï¼‰
        df["source_folder"] = int(yyyymm)
        print(f"   â• source_folder={yyyymm} ã‚’è¿½åŠ ")

        # CSVå‡ºåŠ›
        csv_buffer = io.BytesIO()
        df.to_csv(csv_buffer, index=False, encoding='utf-8')
        csv_buffer.seek(0)

        # google-drive/proceed/ ã«ä¿å­˜
        proceed_path = f"google-drive/proceed/{yyyymm}/{table_name}.csv"
        proceed_blob = bucket.blob(proceed_path)
        proceed_blob.upload_from_file(csv_buffer, content_type='text/csv')

        print(f"   å‡ºåŠ›: gs://{LANDING_BUCKET}/{proceed_path}")
        print(f"âœ… å¤‰æ›å®Œäº†: {table_name}")

        return True

    except Exception as e:
        print(f"âŒ å¤‰æ›ã‚¨ãƒ©ãƒ¼ ({table_name}): {e}")
        traceback.print_exc()
        return False

# ============================================================
# BigQuery ãƒ­ãƒ¼ãƒ‰å‡¦ç†
# ============================================================

def load_table_name_mapping(storage_client: storage.Client) -> Dict[str, str]:
    """ãƒ†ãƒ¼ãƒ–ãƒ«åãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ—¥æœ¬èªâ†’è‹±èªï¼‰ã‚’èª­ã¿è¾¼ã¿"""
    try:
        bucket = storage_client.bucket(LANDING_BUCKET)
        blob = bucket.blob(MAPPING_FILE)

        if not blob.exists():
            print(f"âš ï¸  ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {MAPPING_FILE}")
            return {}

        csv_data = blob.download_as_bytes()
        df = pd.read_csv(io.BytesIO(csv_data))

        mapping = {}
        for _, row in df.iterrows():
            en_name = row['en_name']
            jp_name = row['jp_name'].replace('.xlsx', '')
            mapping[en_name] = jp_name

        return mapping
    except Exception as e:
        print(f"âš ï¸  ãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {}

def load_column_descriptions(storage_client: storage.Client, table_name: str) -> Dict[str, str]:
    """ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚’èª­ã¿è¾¼ã¿"""
    try:
        bucket = storage_client.bucket(LANDING_BUCKET)
        blob = bucket.blob(f"{COLUMNS_PATH}/{table_name}.csv")

        if not blob.exists():
            print(f"âš ï¸  ã‚«ãƒ©ãƒ å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {table_name}.csv")
            return {}

        csv_data = blob.download_as_bytes()
        df = pd.read_csv(io.BytesIO(csv_data))

        descriptions = {}
        for _, row in df.iterrows():
            en_name = row['en_name']
            description = row['description']
            descriptions[en_name] = description

        return descriptions
    except Exception as e:
        print(f"âš ï¸  ã‚«ãƒ©ãƒ èª¬æ˜èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {}

def update_table_and_column_descriptions(
    bq_client: bigquery.Client,
    storage_client: storage.Client,
    table_name: str
) -> bool:
    """ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚’æ›´æ–°"""
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"

    try:
        table = bq_client.get_table(table_id)

        # ãƒ†ãƒ¼ãƒ–ãƒ«åãƒãƒƒãƒ”ãƒ³ã‚°ã‚’èª­ã¿è¾¼ã¿
        table_mapping = load_table_name_mapping(storage_client)
        if table_name in table_mapping:
            table.description = table_mapping[table_name]
            print(f"   ğŸ“ ãƒ†ãƒ¼ãƒ–ãƒ«èª¬æ˜ã‚’è¨­å®š: {table_mapping[table_name]}")

        # ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚’èª­ã¿è¾¼ã¿
        column_descriptions = load_column_descriptions(storage_client, table_name)

        # æ—¢å­˜ã®ã‚¹ã‚­ãƒ¼ãƒã‚’å–å¾—ã—ã€èª¬æ˜ã‚’è¿½åŠ 
        new_schema = []
        for field in table.schema:
            description = column_descriptions.get(field.name, field.description)
            new_field = bigquery.SchemaField(
                name=field.name,
                field_type=field.field_type,
                mode=field.mode,
                description=description,
                fields=field.fields
            )
            new_schema.append(new_field)

        table.schema = new_schema

        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ›´æ–°
        table = bq_client.update_table(table, ["description", "schema"])
        print(f"   âœ… {len(column_descriptions)}å€‹ã®ã‚«ãƒ©ãƒ èª¬æ˜ã‚’è¨­å®š")

        return True

    except Exception as e:
        print(f"   âš ï¸  èª¬æ˜ã®æ›´æ–°ã«å¤±æ•—: {e}")
        return False

def delete_partition_data(
    bq_client: bigquery.Client,
    table_name: str,
    yyyymm: str = None  # æœªä½¿ç”¨ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰
) -> bool:
    """2024å¹´1æœˆä»¥é™ã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’å…¨ã¦å‰Šé™¤ï¼ˆå†ªç­‰æ€§ä¿è¨¼ï¼‰"""
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
    partition_field = TABLE_CONFIG[table_name]["partition_field"]

    # 2024å¹´1æœˆ1æ—¥ä»¥é™ã‚’å…¨ã¦å‰Šé™¤
    # â€»stocksãƒ†ãƒ¼ãƒ–ãƒ«ã¯source_folderã®1ãƒ¶æœˆå‰ã®year_monthã‚’æŒã¤ãŸã‚ã€
    #   2024/9é–‹å§‹ã ã¨2024/8ã®ãƒ‡ãƒ¼ã‚¿ãŒå‰Šé™¤ã•ã‚Œãªã„å•é¡Œã‚’å›é¿
    start_date = "2024-01-01"

    # slip_date ã‚„ final_billing_sales_date ãªã©ã€æœˆã®1æ—¥ä»¥å¤–ã®æ—¥ä»˜ãŒå…¥ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯
    # æ—¥ä»˜æ¯”è¼ƒã§å¯¾å¿œ
    tables_with_non_first_day_dates = [
        "ledger_income",
        "ledger_loss",
        "construction_progress_days_final_date"
    ]

    if table_name in tables_with_non_first_day_dates:
        delete_query = f"""
        DELETE FROM `{table_id}`
        WHERE {partition_field} >= '{start_date}'
        """
    else:
        delete_query = f"""
        DELETE FROM `{table_id}`
        WHERE {partition_field} >= '{start_date}'
        """

    try:
        print(f"   ğŸ—‘ï¸  æ—¢å­˜ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ä¸­: {start_date}ä»¥é™ï¼ˆpartition_field: {partition_field}ï¼‰")
        query_job = bq_client.query(delete_query)
        query_job.result()

        if query_job.num_dml_affected_rows:
            print(f"      å‰Šé™¤: {query_job.num_dml_affected_rows} è¡Œ")
        else:
            print(f"      å‰Šé™¤å¯¾è±¡ãªã—")

        return True

    except Exception as e:
        print(f"   âš ï¸  å‰Šé™¤å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—: {e}")
        return True

def load_csv_to_bigquery(
    bq_client: bigquery.Client,
    table_name: str,
    yyyymm: str,
    execution_id: str = None
) -> bool:
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’BigQueryã«ãƒ­ãƒ¼ãƒ‰"""
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
    gcs_uri = f"gs://{LANDING_BUCKET}/google-drive/proceed/{yyyymm}/{table_name}.csv"
    exec_id = execution_id or get_execution_id()

    try:
        job_config = bigquery.LoadJobConfig(
            source_format=bigquery.SourceFormat.CSV,
            skip_leading_rows=1,
            autodetect=False,
            write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
            schema_update_options=[
                bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION,
            ],
            allow_quoted_newlines=True,
            allow_jagged_rows=False,
            ignore_unknown_values=False,
            max_bad_records=0,
        )

        load_job = bq_client.load_table_from_uri(
            gcs_uri,
            table_id,
            job_config=job_config
        )

        print(f"   â³ ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {table_name} (Job ID: {load_job.job_id})")

        load_job.result(timeout=300)

        destination_table = bq_client.get_table(table_id)
        print(f"   âœ… ãƒ­ãƒ¼ãƒ‰å®Œäº†: {load_job.output_rows} è¡Œã‚’è¿½åŠ ")
        print(f"      ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {destination_table.num_rows:,} è¡Œ")

        # çµ±ä¸€ãƒ­ã‚°å‡ºåŠ›
        log_pipeline_event(
            action="load_table",
            status="OK",
            message=f"ãƒ†ãƒ¼ãƒ–ãƒ« {table_name} ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†",
            table_name=table_name,
            details={
                "yyyymm": yyyymm,
                "rows_added": load_job.output_rows,
                "total_rows": destination_table.num_rows,
                "gcs_uri": gcs_uri
            },
            execution_id=exec_id
        )

        return True

    except GoogleCloudError as e:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚¨ãƒ©ãƒ¼ã§ã¯ãªãè­¦å‘Šï¼‰
        error_str = str(e)
        if "Not found" in error_str or "notFound" in error_str:
            print(f"   âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {gcs_uri}")
            log_pipeline_event(
                action="load_table",
                status="SKIPPED",
                message=f"ãƒ†ãƒ¼ãƒ–ãƒ« {table_name} ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—",
                table_name=table_name,
                details={
                    "yyyymm": yyyymm,
                    "reason": "FILE_NOT_FOUND",
                    "gcs_uri": gcs_uri
                },
                execution_id=exec_id
            )
            return None  # ã‚¹ã‚­ãƒƒãƒ—ã‚’ç¤ºã™

        print(f"   âŒ ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        if hasattr(e, 'errors') and e.errors:
            for error in e.errors:
                print(f"      è©³ç´°: {error}")

        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°å‡ºåŠ›
        log_pipeline_event(
            action="load_table",
            status="ERROR",
            message=f"ãƒ†ãƒ¼ãƒ–ãƒ« {table_name} ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—",
            table_name=table_name,
            details={
                "yyyymm": yyyymm,
                "error": str(e),
                "gcs_uri": gcs_uri
            },
            execution_id=exec_id
        )
        return False
    except Exception as e:
        print(f"   âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")

        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°å‡ºåŠ›
        log_pipeline_event(
            action="load_table",
            status="ERROR",
            message=f"ãƒ†ãƒ¼ãƒ–ãƒ« {table_name} ã®ãƒ­ãƒ¼ãƒ‰ã§äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼",
            table_name=table_name,
            details={
                "yyyymm": yyyymm,
                "error": str(e),
                "gcs_uri": gcs_uri
            },
            execution_id=exec_id
        )
        return False


def process_cumulative_table(
    bq_client: bigquery.Client,
    storage_client: storage.Client,
    table_name: str,
    target_months: list,
    execution_id: str = None
) -> bool:
    """
    ç´¯ç©å‹ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å‡¦ç†

    å…¨æœˆã®CSVã‚’èª­ã¿è¾¼ã¿ã€source_folderã‚«ãƒ©ãƒ ã‚’è¿½åŠ ã—ã¦çµåˆã€‚
    ã‚­ãƒ¼æ¯ã«max(source_folder)ã®ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆã—ã¦é‡è¤‡ã‚’è§£æ¶ˆã€‚

    Args:
        bq_client: BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        storage_client: GCSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
        target_months: å¯¾è±¡å¹´æœˆãƒªã‚¹ãƒˆ
        execution_id: å®Ÿè¡ŒIDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

    Returns:
        æˆåŠŸæ™‚True
    """
    exec_id = execution_id or get_execution_id()
    print(f"\nğŸ“Š å‡¦ç†ä¸­ï¼ˆç´¯ç©å‹ï¼‰: {table_name}")

    config = CUMULATIVE_TABLE_CONFIG[table_name]
    unique_keys = config["unique_keys"]
    bucket = storage_client.bucket(LANDING_BUCKET)

    # å…¨æœˆã®CSVã‚’èª­ã¿è¾¼ã¿ã€source_folderã‚«ãƒ©ãƒ ã‚’è¿½åŠ 
    all_dfs = []
    for yyyymm in target_months:
        blob = bucket.blob(f"google-drive/proceed/{yyyymm}/{table_name}.csv")
        if blob.exists():
            csv_content = blob.download_as_string().decode("utf-8")
            df = pd.read_csv(io.StringIO(csv_content))
            df["source_folder"] = int(yyyymm)
            all_dfs.append(df)
            print(f"   ğŸ“ {yyyymm}: {len(df)}è¡Œ")

    if not all_dfs:
        print(f"   âš ï¸  CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return False

    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
    combined_df = pd.concat(all_dfs, ignore_index=True)
    print(f"   ğŸ“Š çµåˆå¾Œ: {len(combined_df)}è¡Œ")

    # ã‚­ãƒ¼æ¯ã«max(source_folder)ã§ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆæœ€æ–°ãƒ•ã‚©ãƒ«ãƒ€ã‚’å„ªå…ˆï¼‰
    idx = combined_df.groupby(unique_keys)["source_folder"].transform("max") == combined_df["source_folder"]
    deduped_df = combined_df[idx].drop_duplicates(subset=unique_keys, keep="last").reset_index(drop=True)
    print(f"   âœ¨ é‡è¤‡é™¤å»å¾Œ: {len(deduped_df)}è¡Œ")

    # ä¸€æ™‚CSVã«ä¿å­˜
    import tempfile
    with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
        temp_csv = f.name
        deduped_df.to_csv(f, index=False)

    # GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    temp_blob = bucket.blob(f"google-drive/temp/{table_name}_cumulative.csv")
    temp_blob.upload_from_filename(temp_csv)
    gcs_uri = f"gs://{LANDING_BUCKET}/google-drive/temp/{table_name}_cumulative.csv"

    # BigQueryã«ãƒ­ãƒ¼ãƒ‰
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"

    try:
        # ç´¯ç©å‹ãƒ†ãƒ¼ãƒ–ãƒ«ã¯å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ï¼ˆCSVã«å…¨å±¥æ­´ãŒå«ã¾ã‚Œã‚‹ãŸã‚ï¼‰
        delete_query = f"""
        DELETE FROM `{table_id}`
        WHERE TRUE
        """
        query_job = bq_client.query(delete_query)
        query_job.result()
        deleted = query_job.num_dml_affected_rows or 0
        print(f"   ğŸ—‘ï¸  æ—¢å­˜ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ï¼ˆå…¨ä»¶ï¼‰: {deleted}è¡Œ")

        # ã‚¹ã‚­ãƒ¼ãƒã‚’å–å¾—ã—ã¦source_folderã‚«ãƒ©ãƒ ã‚’è¿½åŠ ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
        table = bq_client.get_table(table_id)
        existing_schema = list(table.schema)
        has_source_folder = any(f.name == "source_folder" for f in existing_schema)

        if not has_source_folder:
            new_schema = existing_schema + [bigquery.SchemaField("source_folder", "INTEGER")]
            table.schema = new_schema
            bq_client.update_table(table, ["schema"])
            print(f"   â• source_folderã‚«ãƒ©ãƒ ã‚’è¿½åŠ ")

        # ãƒ­ãƒ¼ãƒ‰
        job_config = bigquery.LoadJobConfig(
            source_format=bigquery.SourceFormat.CSV,
            skip_leading_rows=1,
            write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
            allow_quoted_newlines=True,
        )
        load_job = bq_client.load_table_from_uri(gcs_uri, table_id, job_config=job_config)
        load_job.result(timeout=300)
        print(f"   âœ… ãƒ­ãƒ¼ãƒ‰å®Œäº†: {load_job.output_rows}è¡Œ")

        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        temp_blob.delete()
        import os
        os.remove(temp_csv)

        # ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚’æ›´æ–°
        update_table_and_column_descriptions(bq_client, storage_client, table_name)

        # çµ±ä¸€ãƒ­ã‚°å‡ºåŠ›
        log_pipeline_event(
            action="load_cumulative_table",
            status="OK",
            message=f"ç´¯ç©å‹ãƒ†ãƒ¼ãƒ–ãƒ« {table_name} ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†",
            table_name=table_name,
            details={
                "target_months": target_months,
                "rows_loaded": load_job.output_rows,
                "unique_keys": unique_keys
            },
            execution_id=exec_id
        )

        return True

    except Exception as e:
        print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()

        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°å‡ºåŠ›
        log_pipeline_event(
            action="load_cumulative_table",
            status="ERROR",
            message=f"ç´¯ç©å‹ãƒ†ãƒ¼ãƒ–ãƒ« {table_name} ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—",
            table_name=table_name,
            details={
                "target_months": target_months,
                "error": str(e)
            },
            execution_id=exec_id
        )
        return False


# ============================================================
# ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ â†’ BigQuery ãƒ­ãƒ¼ãƒ‰å‡¦ç†
# ============================================================

def load_spreadsheet_column_schema(
    storage_client: storage.Client,
    table_name: str
) -> List[bigquery.SchemaField]:
    """
    ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ã‚«ãƒ©ãƒ å®šç¾©ã‹ã‚‰BigQueryã‚¹ã‚­ãƒ¼ãƒã‚’ç”Ÿæˆ

    Args:
        storage_client: GCSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«åï¼ˆss_ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãªã—ï¼‰

    Returns:
        BigQueryã‚¹ã‚­ãƒ¼ãƒãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒªã‚¹ãƒˆ
    """
    try:
        bucket = storage_client.bucket(LANDING_BUCKET)
        blob = bucket.blob(f"{SPREADSHEET_COLUMNS_PATH}/{table_name}.csv")

        if not blob.exists():
            print(f"âš ï¸  ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚«ãƒ©ãƒ å®šç¾©ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {table_name}.csv")
            return []

        csv_data = blob.download_as_bytes()
        df = pd.read_csv(io.BytesIO(csv_data))

        # data_type â†’ BigQueryå‹ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        type_mapping = {
            "STRING": "STRING",
            "INTEGER": "INTEGER",
            "INT64": "INTEGER",
            "FLOAT": "FLOAT",
            "NUMERIC": "NUMERIC",
            "DATE": "DATE",
            "DATETIME": "DATETIME",
            "TIMESTAMP": "TIMESTAMP",
            "BOOLEAN": "BOOLEAN",
            "BOOL": "BOOLEAN",
        }

        schema = []
        for _, row in df.iterrows():
            en_name = row['en_name']
            data_type = row.get('data_type', 'STRING')
            bq_type = type_mapping.get(data_type.upper(), 'STRING')
            jp_name = row.get('jp_name', en_name)

            schema.append(bigquery.SchemaField(
                name=en_name,
                field_type=bq_type,
                mode="NULLABLE",
                description=jp_name
            ))

        return schema

    except Exception as e:
        print(f"âš ï¸  ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚¹ã‚­ãƒ¼ãƒèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return []


def load_spreadsheet_to_bigquery(
    bq_client: bigquery.Client,
    storage_client: storage.Client,
    table_name: str,
    execution_id: str = None
) -> bool:
    """
    ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆCSVã‚’BigQueryã«ãƒ­ãƒ¼ãƒ‰ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿æ´—ã„æ›¿ãˆï¼‰

    Args:
        bq_client: BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        storage_client: GCSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«åï¼ˆss_ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãªã—ï¼‰
        execution_id: å®Ÿè¡ŒID

    Returns:
        æˆåŠŸæ™‚True
    """
    exec_id = execution_id or get_execution_id()
    config = SPREADSHEET_TABLE_CONFIG.get(table_name)

    if not config:
        print(f"âš ï¸  æœªå®šç¾©ã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«: {table_name}")
        return False

    bq_table_name = config["bq_table_name"]
    description = config["description"]
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{bq_table_name}"
    gcs_uri = f"gs://{LANDING_BUCKET}/{SPREADSHEET_PROCEED_PATH}/{table_name}.csv"

    print(f"\nğŸ“Š ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆå‡¦ç†ä¸­: {table_name} â†’ {bq_table_name}")

    try:
        # GCSãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        bucket = storage_client.bucket(LANDING_BUCKET)
        blob = bucket.blob(f"{SPREADSHEET_PROCEED_PATH}/{table_name}.csv")

        if not blob.exists():
            print(f"   âš ï¸  CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {gcs_uri}")
            log_pipeline_event(
                action="load_spreadsheet",
                status="WARNING",
                message=f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆCSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
                table_name=bq_table_name,
                details={"gcs_uri": gcs_uri},
                execution_id=exec_id
            )
            return False

        # CSVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        csv_content = blob.download_as_string().decode("utf-8")
        df = pd.read_csv(io.StringIO(csv_content))
        row_count = len(df)

        print(f"   ğŸ“ ãƒ‡ãƒ¼ã‚¿: {row_count}è¡Œ Ã— {len(df.columns)}åˆ—")

        # ã‚«ãƒ©ãƒ ãƒ»ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if VALIDATION_ENABLED:
            # ã‚¹ã‚­ãƒ¼ãƒã‹ã‚‰ã‚«ãƒ©ãƒ åãƒªã‚¹ãƒˆã‚’å–å¾—
            schema = load_spreadsheet_column_schema(storage_client, table_name)
            expected_columns = [field.name for field in schema]

            if expected_columns:
                validation_result = {
                    "timestamp": datetime.utcnow().isoformat() + "Z",
                    "service": "gcs-to-bq",
                    "validation_type": "spreadsheet_column_check",
                    "table_name": bq_table_name,
                    "source_file": gcs_uri,
                    "status": "OK",
                    "row_count": row_count,
                    "column_count": len(df.columns),
                    "expected_column_count": len(expected_columns),
                    "errors": [],
                    "warnings": []
                }

                actual_columns = list(df.columns)
                missing_columns = [col for col in expected_columns if col not in actual_columns]
                extra_columns = [col for col in actual_columns if col not in expected_columns]

                if missing_columns:
                    validation_result["errors"].append({
                        "type": "MISSING_COLUMNS",
                        "message": f"æœŸå¾…ã•ã‚Œã‚‹ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {missing_columns}",
                        "details": {"missing": missing_columns}
                    })
                    validation_result["status"] = "ERROR"

                if extra_columns:
                    validation_result["warnings"].append({
                        "type": "EXTRA_COLUMNS",
                        "message": f"å®šç¾©å¤–ã®ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã™: {extra_columns}",
                        "details": {"extra": extra_columns}
                    })

                if row_count == 0:
                    validation_result["errors"].append({
                        "type": "EMPTY_DATA",
                        "message": "ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™"
                    })
                    validation_result["status"] = "ERROR"

                log_validation_result(validation_result)

                if validation_result.get("status") == "ERROR":
                    for error in validation_result.get("errors", []):
                        print(f"   âš ï¸  ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼: {error.get('message')}")
                else:
                    print(f"   âœ… ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³OK: ã‚«ãƒ©ãƒ ãƒ»ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ãƒã‚§ãƒƒã‚¯ passed")

        # ã‚¹ã‚­ãƒ¼ãƒã‚’å–å¾—
        schema = load_spreadsheet_column_schema(storage_client, table_name)

        # BigQueryã‚¸ãƒ§ãƒ–è¨­å®š
        job_config = bigquery.LoadJobConfig(
            source_format=bigquery.SourceFormat.CSV,
            skip_leading_rows=1,
            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,  # å…¨ãƒ‡ãƒ¼ã‚¿æ´—ã„æ›¿ãˆ
            allow_quoted_newlines=True,
            allow_jagged_rows=False,
            ignore_unknown_values=False,
            max_bad_records=0,
        )

        # ã‚¹ã‚­ãƒ¼ãƒãŒã‚ã‚‹å ´åˆã¯è¨­å®š
        if schema:
            job_config.schema = schema
        else:
            job_config.autodetect = True

        # BigQueryã«ãƒ­ãƒ¼ãƒ‰
        load_job = bq_client.load_table_from_uri(
            gcs_uri,
            table_id,
            job_config=job_config
        )

        print(f"   â³ ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {bq_table_name} (Job ID: {load_job.job_id})")

        load_job.result(timeout=300)

        destination_table = bq_client.get_table(table_id)
        print(f"   âœ… ãƒ­ãƒ¼ãƒ‰å®Œäº†: {load_job.output_rows} è¡Œ")

        # ãƒ†ãƒ¼ãƒ–ãƒ«ã®èª¬æ˜ã‚’è¨­å®š
        destination_table.description = description
        bq_client.update_table(destination_table, ["description"])
        print(f"   ğŸ“ ãƒ†ãƒ¼ãƒ–ãƒ«èª¬æ˜ã‚’è¨­å®š: {description}")

        # çµ±ä¸€ãƒ­ã‚°å‡ºåŠ›
        log_pipeline_event(
            action="load_spreadsheet",
            status="OK",
            message=f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ« {bq_table_name} ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†",
            table_name=bq_table_name,
            details={
                "source_table": table_name,
                "rows_loaded": load_job.output_rows,
                "total_rows": destination_table.num_rows,
                "gcs_uri": gcs_uri
            },
            execution_id=exec_id
        )

        return True

    except GoogleCloudError as e:
        print(f"   âŒ ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        if hasattr(e, 'errors') and e.errors:
            for error in e.errors:
                print(f"      è©³ç´°: {error}")

        log_pipeline_event(
            action="load_spreadsheet",
            status="ERROR",
            message=f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ« {bq_table_name} ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—",
            table_name=bq_table_name,
            details={
                "source_table": table_name,
                "error": str(e),
                "gcs_uri": gcs_uri
            },
            execution_id=exec_id
        )
        return False

    except Exception as e:
        print(f"   âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()

        log_pipeline_event(
            action="load_spreadsheet",
            status="ERROR",
            message=f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ« {bq_table_name} ã®ãƒ­ãƒ¼ãƒ‰ã§äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼",
            table_name=bq_table_name,
            details={
                "source_table": table_name,
                "error": str(e),
                "gcs_uri": gcs_uri
            },
            execution_id=exec_id
        )
        return False


def validate_spreadsheet_duplicates_in_bq(
    bq_client: bigquery.Client,
    bq_table_name: str
) -> Dict[str, Any]:
    """
    ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã®é‡è¤‡ã‚’ãƒã‚§ãƒƒã‚¯

    Args:
        bq_client: BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        bq_table_name: BigQueryãƒ†ãƒ¼ãƒ–ãƒ«åï¼ˆss_ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ãï¼‰

    Returns:
        æ¤œè¨¼çµæœã®è¾æ›¸
    """
    errors = []

    # ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼å®šç¾©ã‚’å–å¾—
    unique_keys = SPREADSHEET_UNIQUE_KEYS_CONFIG.get(bq_table_name, [])
    if not unique_keys:
        return {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "service": "gcs-to-bq",
            "validation_type": "spreadsheet_duplicate_check",
            "table_name": bq_table_name,
            "status": "SKIPPED",
            "message": "ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        }

    table_id = f"{PROJECT_ID}.{DATASET_ID}.{bq_table_name}"
    key_cols = ", ".join(unique_keys)

    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚¯ã‚¨ãƒª
    query = f"""
    SELECT {key_cols}, COUNT(*) as duplicate_count
    FROM `{table_id}`
    GROUP BY {key_cols}
    HAVING COUNT(*) > 1
    LIMIT 10
    """

    try:
        result = bq_client.query(query).result()
        duplicates = [dict(row) for row in result]
        duplicate_count = len(duplicates)

        if duplicate_count > 0:
            errors.append({
                "type": "DUPLICATE_RECORDS",
                "message": f"é‡è¤‡ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒå­˜åœ¨ã—ã¾ã™ï¼ˆã‚µãƒ³ãƒ—ãƒ«: {duplicate_count}ä»¶ï¼‰",
                "details": {
                    "unique_keys": unique_keys,
                    "sample_duplicates": duplicates
                }
            })

        return {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "service": "gcs-to-bq",
            "validation_type": "spreadsheet_duplicate_check",
            "table_name": bq_table_name,
            "status": "ERROR" if errors else "OK",
            "unique_keys": unique_keys,
            "duplicate_sample_count": duplicate_count,
            "errors": errors
        }

    except Exception as e:
        return {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "service": "gcs-to-bq",
            "validation_type": "spreadsheet_duplicate_check",
            "table_name": bq_table_name,
            "status": "ERROR",
            "errors": [{
                "type": "QUERY_ERROR",
                "message": f"é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}"
            }]
        }


def process_spreadsheet_tables(
    bq_client: bigquery.Client,
    storage_client: storage.Client,
    tables: List[str] = None,
    execution_id: str = None
) -> Dict[str, Any]:
    """
    ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä¸€æ‹¬å‡¦ç†

    Args:
        bq_client: BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        storage_client: GCSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        tables: å‡¦ç†å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«ãƒªã‚¹ãƒˆï¼ˆçœç•¥æ™‚ã¯å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰
        execution_id: å®Ÿè¡ŒID

    Returns:
        å‡¦ç†çµæœã®è¾æ›¸
    """
    exec_id = execution_id or get_execution_id()
    target_tables = tables or list(SPREADSHEET_TABLE_CONFIG.keys())

    print("\n" + "=" * 60)
    print(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ â†’ BigQuery ãƒ­ãƒ¼ãƒ‰å‡¦ç†")
    print(f"å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«: {', '.join(target_tables)}")
    print("=" * 60)

    # å‡¦ç†é–‹å§‹ãƒ­ã‚°
    log_pipeline_event(
        action="spreadsheet_load_start",
        status="INFO",
        message=f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ­ãƒ¼ãƒ‰å‡¦ç†ã‚’é–‹å§‹",
        details={
            "tables": target_tables,
            "table_count": len(target_tables)
        },
        execution_id=exec_id
    )

    success_count = 0
    error_count = 0
    results = []

    for table_name in target_tables:
        config = SPREADSHEET_TABLE_CONFIG.get(table_name)
        if not config:
            print(f"âš ï¸  æœªå®šç¾©ã®ãƒ†ãƒ¼ãƒ–ãƒ«: {table_name}")
            error_count += 1
            results.append({"table": table_name, "status": "error", "reason": "undefined"})
            continue

        bq_table_name = config["bq_table_name"]

        # ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
        if load_spreadsheet_to_bigquery(bq_client, storage_client, table_name, exec_id):
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            if VALIDATION_ENABLED:
                dup_result = validate_spreadsheet_duplicates_in_bq(bq_client, bq_table_name)
                log_validation_result(dup_result)

                if dup_result.get("status") == "ERROR":
                    for error in dup_result.get("errors", []):
                        print(f"   âš ï¸  é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {error.get('message')}")
                elif dup_result.get("status") == "SKIPPED":
                    print(f"   â­ï¸  é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚¹ã‚­ãƒƒãƒ—: ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼æœªå®šç¾©")
                else:
                    print(f"   âœ… ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³OK: é‡è¤‡ãƒã‚§ãƒƒã‚¯ passed")

            success_count += 1
            results.append({"table": table_name, "bq_table": bq_table_name, "status": "success"})
        else:
            error_count += 1
            results.append({"table": table_name, "bq_table": bq_table_name, "status": "error"})

    print("\n" + "=" * 60)
    print(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆå‡¦ç†å®Œäº†: æˆåŠŸ {success_count} / ã‚¨ãƒ©ãƒ¼ {error_count}")
    print("=" * 60)

    # å‡¦ç†å®Œäº†ãƒ­ã‚°
    final_status = "OK" if error_count == 0 else "WARNING"
    log_pipeline_event(
        action="spreadsheet_load_complete",
        status=final_status,
        message=f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ­ãƒ¼ãƒ‰å‡¦ç†ãŒå®Œäº†",
        details={
            "success_count": success_count,
            "error_count": error_count,
            "results": results
        },
        execution_id=exec_id
    )

    return {
        "success_count": success_count,
        "error_count": error_count,
        "results": results
    }


# ============================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# ============================================================

# æœŸé¦–ï¼ˆãƒ‡ãƒ¼ã‚¿é–‹å§‹æ—¥ï¼‰
FISCAL_START_YYYYMM = "202409"  # 2024å¹´9æœˆ

def get_available_months_from_gcs(storage_client: storage.Client) -> list:
    """GCSã®google-drive/proceed/ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªå¹´æœˆãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆ2024/9ä»¥é™ï¼‰"""
    bucket = storage_client.bucket(LANDING_BUCKET)
    blobs = bucket.list_blobs(prefix="google-drive/proceed/")

    months = set()
    for blob in blobs:
        # google-drive/proceed/202409/xxx.csv ã®ã‚ˆã†ãªå½¢å¼ã‹ã‚‰yyyymmã‚’æŠ½å‡º
        parts = blob.name.split("/")
        if len(parts) >= 3 and parts[2].isdigit() and len(parts[2]) == 6:
            yyyymm = parts[2]
            # 2024/9ä»¥é™ã®ã¿å¯¾è±¡
            if yyyymm >= FISCAL_START_YYYYMM:
                months.add(yyyymm)

    return sorted(list(months))

# ============================================================
# Flask ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
# ============================================================

app = Flask(__name__)

@app.route("/transform", methods=["POST"])
def transform_endpoint():
    """
    Excel â†’ CSV å¤‰æ›ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

    ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¾‹:
    {
        "yyyymm": "202509",
        "tables": ["sales_target_and_achievements", "billing_balance"]
    }

    ç©ºã®å ´åˆã¯å…¨ãƒ†ãƒ¼ãƒ–ãƒ«å‡¦ç†
    """
    try:
        payload = request.get_json(force=True, silent=True) or {}
        yyyymm = payload.get("yyyymm")
        tables = payload.get("tables", list(TABLE_CONFIG.keys()))

        if not yyyymm:
            return jsonify({"error": "yyyymm is required"}), 400

        print("=" * 60)
        print(f"raw/ â†’ proceed/ å¤‰æ›å‡¦ç†")
        print(f"å¯¾è±¡å¹´æœˆ: {yyyymm}")
        print("=" * 60)

        storage_client = storage.Client()

        success_count = 0
        error_count = 0
        results = []

        for table_name in tables:
            if transform_excel_to_csv(storage_client, table_name, yyyymm):
                success_count += 1
                results.append({"table": table_name, "status": "success"})
            else:
                error_count += 1
                results.append({"table": table_name, "status": "error"})

        print("=" * 60)
        print(f"å‡¦ç†å®Œäº†: æˆåŠŸ {success_count} / ã‚¨ãƒ©ãƒ¼ {error_count}")
        print("=" * 60)

        return jsonify({
            "status": "completed",
            "yyyymm": yyyymm,
            "success": success_count,
            "error": error_count,
            "results": results
        }), 200

    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route("/load", methods=["POST"])
def load_endpoint():
    """
    CSV â†’ BigQuery ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

    ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¾‹:
    {
        "yyyymm": "202509",  # çœç•¥æ™‚ã¯2024/9ä»¥é™ã®å…¨å¹´æœˆã‚’å‡¦ç†
        "tables": ["sales_target_and_achievements"],
        "replace": true
    }

    æ³¨æ„: å†ªç­‰æ€§ã‚’ä¿è¨¼ã™ã‚‹ãŸã‚ã€2024/9ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã¯å…¨ã¦å‰Šé™¤ã•ã‚Œã¦ã‹ã‚‰è¿½åŠ ã•ã‚Œã¾ã™ã€‚
    """
    exec_id = get_execution_id()

    try:
        payload = request.get_json(force=True, silent=True) or {}
        yyyymm = payload.get("yyyymm")  # çœç•¥å¯èƒ½
        tables = payload.get("tables", list(TABLE_CONFIG.keys()))

        bq_client = bigquery.Client(project=PROJECT_ID)
        storage_client = storage.Client()

        # å¯¾è±¡å¹´æœˆãƒªã‚¹ãƒˆã‚’æ±ºå®š
        if yyyymm:
            # ç‰¹å®šæœˆãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã§ã‚‚ã€2024/9ä»¥é™ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
            target_months = get_available_months_from_gcs(storage_client)
            print(f"æŒ‡å®šæœˆ: {yyyymm}ï¼ˆãŸã ã—2024/9ä»¥é™ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ï¼‰")
        else:
            # çœç•¥æ™‚ã¯2024/9ä»¥é™ã®å…¨å¹´æœˆ
            target_months = get_available_months_from_gcs(storage_client)

        # ============================================================
        # TABLE_CONFIGæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆè¨­å®šæ¼ã‚Œé˜²æ­¢ï¼‰
        # ============================================================
        config_check = validate_table_config_completeness(storage_client, target_months)
        if config_check["status"] == "ERROR":
            print("=" * 60)
            print("âŒ TABLE_CONFIGæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼")
            print("=" * 60)
            print(f"æœªè¨­å®šã®ãƒ†ãƒ¼ãƒ–ãƒ«: {', '.join(config_check['missing_tables'])}")
            print("")
            print("å¯¾å‡¦æ–¹æ³•:")
            print("  1. gcs_to_bq_service/main.py ã® TABLE_CONFIG ã«ä¸Šè¨˜ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’è¿½åŠ ")
            print("  2. ã‚µãƒ¼ãƒ“ã‚¹ã‚’å†ãƒ‡ãƒ—ãƒ­ã‚¤")
            print("  3. å†å®Ÿè¡Œ")
            print("=" * 60)

            log_pipeline_event(
                action="load_config_check",
                status="ERROR",
                message=config_check["message"],
                details={"missing_tables": config_check["missing_tables"]},
                execution_id=exec_id
            )

            return jsonify({
                "status": "error",
                "error": config_check["message"],
                "missing_tables": config_check["missing_tables"]
            }), 400

        print("=" * 60)
        print(f"proceed/ â†’ BigQuery ãƒ­ãƒ¼ãƒ‰å‡¦ç†")
        print(f"å¯¾è±¡å¹´æœˆ: {', '.join(target_months)}")
        print(f"ãƒ¢ãƒ¼ãƒ‰: REPLACEï¼ˆ2024/1ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã‚’å…¨ã¦å‰Šé™¤ã—ã¦å†ãƒ­ãƒ¼ãƒ‰ï¼‰")
        print("=" * 60)

        # å‡¦ç†é–‹å§‹ãƒ­ã‚°
        log_pipeline_event(
            action="load_start",
            status="INFO",
            message=f"GCS â†’ BigQueryãƒ­ãƒ¼ãƒ‰å‡¦ç†ã‚’é–‹å§‹",
            details={
                "target_months": target_months,
                "tables": tables,
                "table_count": len(tables)
            },
            execution_id=exec_id
        )

        success_count = 0
        error_count = 0
        results = []

        for table_name in tables:
            # ç´¯ç©å‹ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã©ã†ã‹ã§å‡¦ç†ã‚’åˆ†å²
            if table_name in CUMULATIVE_TABLE_CONFIG:
                # ç´¯ç©å‹ãƒ†ãƒ¼ãƒ–ãƒ«: å°‚ç”¨å‡¦ç†ï¼ˆsource_folderè¿½åŠ ã€é‡è¤‡é™¤å»ï¼‰
                table_success = process_cumulative_table(
                    bq_client, storage_client, table_name, target_months, exec_id
                )
            else:
                # å˜æœˆå‹ãƒ†ãƒ¼ãƒ–ãƒ«: å¾“æ¥ã®å‡¦ç†
                print(f"\nğŸ“Š å‡¦ç†ä¸­ï¼ˆå˜æœˆå‹ï¼‰: {table_name}")

                # 2024/9ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã‚’å…¨ã¦å‰Šé™¤ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ã”ã¨ã«1å›ã ã‘ï¼‰
                delete_partition_data(bq_client, table_name)

                # å…¨å¹´æœˆã®CSVã‚’ãƒ­ãƒ¼ãƒ‰
                table_success = True
                for month in target_months:
                    result = load_csv_to_bigquery(bq_client, table_name, month, exec_id)
                    if result is False:  # Noneï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰ã¯æˆåŠŸã¨ã—ã¦æ‰±ã†
                        table_success = False

                if table_success:
                    # ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã‚«ãƒ©ãƒ ã®èª¬æ˜ã‚’æ›´æ–°
                    update_table_and_column_descriptions(bq_client, storage_client, table_name)

            if table_success:
                # ============================================================
                # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³: é‡è¤‡ãƒã‚§ãƒƒã‚¯
                # ============================================================
                if VALIDATION_ENABLED:
                    dup_result = validate_duplicates_in_bq(bq_client, table_name)
                    log_validation_result(dup_result)

                    if dup_result.get("status") == "ERROR":
                        for error in dup_result.get("errors", []):
                            print(f"   âš ï¸  é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {error.get('message')}")
                    elif dup_result.get("status") == "SKIPPED":
                        print(f"   â­ï¸  é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚¹ã‚­ãƒƒãƒ—: ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼æœªå®šç¾©")
                    else:
                        print(f"   âœ… ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³OK: é‡è¤‡ãƒã‚§ãƒƒã‚¯ passed")

                success_count += 1
                results.append({"table": table_name, "status": "success"})
            else:
                error_count += 1
                results.append({"table": table_name, "status": "error"})

        print("\n" + "=" * 60)
        print(f"Driveå‡¦ç†å®Œäº†: æˆåŠŸ {success_count} / ã‚¨ãƒ©ãƒ¼ {error_count}")
        print("=" * 60)

        # ============================================================
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å‡¦ç†
        # ============================================================
        spreadsheet_result = process_spreadsheet_tables(
            bq_client, storage_client, execution_id=exec_id
        )

        # å…¨ä½“ã®çµæœã‚’é›†è¨ˆ
        total_success = success_count + spreadsheet_result["success_count"]
        total_error = error_count + spreadsheet_result["error_count"]

        # å‡¦ç†å®Œäº†ãƒ­ã‚°
        final_status = "OK" if total_error == 0 else "WARNING"
        log_pipeline_event(
            action="load_complete",
            status=final_status,
            message=f"GCS â†’ BigQueryãƒ­ãƒ¼ãƒ‰å‡¦ç†ãŒå®Œäº†",
            details={
                "target_months": target_months,
                "drive_success_count": success_count,
                "drive_error_count": error_count,
                "spreadsheet_success_count": spreadsheet_result["success_count"],
                "spreadsheet_error_count": spreadsheet_result["error_count"],
                "total_success_count": total_success,
                "total_error_count": total_error,
                "drive_results": results,
                "spreadsheet_results": spreadsheet_result["results"]
            },
            execution_id=exec_id
        )

        return jsonify({
            "status": "completed",
            "target_months": target_months,
            "drive": {
                "success": success_count,
                "error": error_count,
                "results": results
            },
            "spreadsheet": spreadsheet_result,
            "total_success": total_success,
            "total_error": total_error
        }), 200

    except Exception as e:
        traceback.print_exc()

        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
        log_pipeline_event(
            action="load_complete",
            status="ERROR",
            message=f"GCS â†’ BigQueryãƒ­ãƒ¼ãƒ‰å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ",
            details={"error": str(e)},
            execution_id=exec_id
        )

        return jsonify({"error": str(e)}), 500

@app.route("/", methods=["GET"])
def health():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return "gcs-to-bq service is running", 200

if __name__ == "__main__":
    port = int(os.getenv("PORT", "8080"))
    app.run(host="0.0.0.0", port=port, debug=False)
